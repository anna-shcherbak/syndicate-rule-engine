{"ecc-azure-056":{"article":"Ensure that all Secrets in the Azure Key Vault have an expiration time set.","impact":"Expired secrets can be misused or exposed during their life cycle, which can lead to potential threats to data integrity and confidentiality.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Key vaults\n2. For each Key vault, click on Secrets\n3. Under the Settings section, make sure Enabled is set to Yes\n4. Set an appropriate EXPIRATION DATE on all secrets.","multiregional":true,"service":"Key Vault"},"ecc-azure-016":{"article":"Enable 'Azure Defender for SQL' on critical SQL Servers.","impact":"Insufficient monitoring of your SQL servers can increase the amount of time necessary to detect and respond to potential database vulnerabilities, as well as to abnormal activities that may indicate a threat to your databases.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each server instance\n3. Click Azure Defender for SQL\n4. Set Azure Defender for SQL to On","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-353":{"article":"Enabling automatic OS image upgrades on your scale set helps ease update management by safely and automatically upgrading the OS disk for all instances in the scale set.","impact":"OS image autoupgrading is the aministrative efficient approach of the VMSS state management","report_fields":["id"],"remediation":"From Azure CLI run:\n  az vmss update --name {myScaleSet} --resource-group {myResourceGroup} --set UpgradePolicy.AutomaticOSUpgradePolicy.EnableAutomaticOSUpgrade=true\nNote: Health probe or HealthExtension must be configured first for all instances in a scale set","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-057":{"article":"A Key Vault contains object keys, secrets and certificates. Accidental unavailability of a key vault can cause immediate data loss or loss of security functions (authentication, validation, verification, non-repudiation, etc.) supported by the key vault objects. It is recommended the key vault be made recoverable by enabling the 'Do Not Purge' and 'Soft Delete' functions. This is in order to prevent loss of encrypted data including storage accounts, SQL databases, and/or dependent services provided by key vault objects (Keys, Secrets, Certificates), etc., as may happen in the case of accidental deletion by a user or from disruptive activity by a malicious user.","impact":"The absence of recovery function in Key Vault may cause irreversible loss of keys, secrets, or certificates stored in a key vault.","report_fields":["id"],"remediation":"To enable 'Do Not Purge' and 'Soft Delete' for a Key Vault:\nUsing Azure CLI 2.0:  \n   az resource update --id /subscriptions/xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/<resourceGroupName>/providers/Microsoft.KeyVault /vaults/<keyVaultName> --set properties.enablePurgeProtection=true properties.enableSoftDelete=true","multiregional":true,"service":"Key Vault"},"ecc-azure-108":{"article":"Some Microsoft services interacting with storage accounts operate from networks that can't be granted access through network rules. To help this type of service work as intended, allow a set of trusted Microsoft services to bypass the network rules. These services will then use strong authentication to access the storage account. If the 'Allow trusted Microsoft services' exception is enabled, such services as Azure Backup, Azure Site Recovery, Azure DevTest Labs, Azure Event Grid, Azure Event Hubs, Azure Networking, Azure Monitor and Azure SQL Data Warehouse (when registered in the subscription) are granted access to the storage account.","impact":"Exposed and vulnerable services can access your storage account without proper authentification and authorization.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts\n2. For each storage account, click the settings menu called Firewalls and Virtual networks.\n3. Ensure that you have elected to allow access from Selected networks.\n4. Check Allow trusted Microsoft services to access this storage account.\n5. Click Save to apply your changes.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-038":{"article":"Enable AuditEvent logging for key vault instances to ensure interactions with key vaults are logged and available.","impact":"Insufficient monitoring of how, when, and who accesses KeyVaults affects the privacy of your data. Also, the absence of event logs with a centralized storage point can lead to improper threats investigation.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Key vaults\n2. For each Key vault\n3. Go to Diagnostic settings\n4. Click on Edit Settings\n5. Enable Archive to a storage account\n6. Check AuditEvent\n7. Change the retention days to be 180, 0 (for indefinite) or as appropriate","multiregional":true,"service":"Key Vault"},"ecc-azure-310":{"article":"Azure Defender for open-source relational databases detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases. Learn more about the capabilities of Azure Defender for open-source relational databases at https://aka.ms/AzDforOpenSourceDBsDocu. Important: Enabling this plan will result in charges for protecting your open-source relational databases. Learn about the pricing on Security Center's pricing page: https://aka.ms/pricing-security-center","impact":"Insufficient monitoring of your OpenSource Relational Databases can lead to a lack of security assessments and threat detection activities.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Security Center \n2. Select the Pricing & settings blade \n3. Click on the subscription name \n4. Select the Azure Defender plans blade\n5. On the line in the table, for OpenSource Relational Databases, select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-270":{"article":"Periodically, newer versions are released for Python software either due to security flaws or to include additional functionality. \nUsing the latest Python version for Function apps is recommended in order to take advantage of security fixes, if any, and/or new functionalities of the latest version. \nCurrently, this policy only applies to Linux web apps.","impact":"An outdated version of Python libraries may contain well-known vulnerabilities such as backdoors or code bugs, which are fixed in the latest version.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Azure App Service \n2. Under the Configuration blade, select the latest Python version","multiregional":true,"service":"App Service"},"ecc-azure-367":{"article":"On September 14, 2021, Microsoft released fixes for three Elevation of Privilege (EoP) vulnerabilities and one unauthenticated Remote Code Execution (RCE) vulnerability in the Open Management Infrastructure (OMI) framework:  CVE-2021-38645, CVE-2021-38649, CVE-2021-38648, and CVE-2021-38647, respectively.  Open Management Infrastructure (OMI) is an open-source Web-Based Enterprise Management (WBEM) implementation for managing Linux and UNIX systems. Several Azure Virtual Machine (VM) management extensions use this framework to orchestrate configuration management and log collection on Linux VMs.","impact":"Log Analytics Agent v1.13.39 or less bundled with vulnerable OMI version. It can be point of Local Elevation Privileges on affected VM instances.\nLog Analytics Agent v.1.13.40 considered as safe and requires additional manual check.","report_fields":["id"],"remediation":"Follow this article to detect and remediate OMI vulnerability:\nhttps://techcommunity.microsoft.com/t5/azure-observability-blog/detecting-and-updating-agents-using-the-omi-vulnerability/ba-p/2795462","multiregional":true,"service":"Virtual Machines"},"ecc-azure-314":{"article":"The debug_print_plan setting enables printing the execution plan for each executed query. These messages are emitted at the LOG message level. Unless directed otherwise by your organization's logging policy, it is recommended this setting be disabled by setting it to off.","impact":"Enabling any of the DEBUG printing variables may cause the logging of sensitive information that would otherwise be omitted based on the configuration of the other logging settings.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'debug_print_plan' to 'off'","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-039":{"article":"Create an activity log alert for the Create Policy Assignment event.","impact":"Monitoring for 'Create Policy Assignment' events gives insight into changes done in 'azure policy - assignments' and can reduce the time it takes to detect unsolicited changes.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Policy Assignment under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Verify Selection preview shows All Policy assignment (policyAssignments) and your selected subscription name\n10. Click Done\n11. Under Condition, click Add Condition\n12. Select the Create policy assignment signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-044":{"article":"Create an activity log alert for the Create or Update Security Solution event.","impact":"Monitoring for 'Create' or 'Update Security Solution' events gives insight into changes to the active security solutions and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Security Solutions under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Click Done\n10. Verify Selection preview shows Security Solutions and your selected subscription name\n11. Under Condition click Add Condition\n12. Select the Create or Update Security Solutions signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter the Alert rule name and Description\n16. Select appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-042":{"article":"Create an Activity Log Alert for the 'Create' or 'Update Network Security Group' event.","impact":"Monitoring for 'Create' or 'Update Network Security Group' events gives insight into network access changes and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Network Security Groups under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Verify Selection preview shows All Network Security Groups and your selected subscription name\n10. Click Done\n11. Under Condition, click Add Condition\n12. Select the Create or Update Network Security Group signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-068":{"article":"Create an activity log alert for the Delete Network Security Group Rule event.","impact":"Monitoring for Delete Network Security Group Rule events gives insight into network access changes and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Network Security Group Rules under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Click Done\n10. Verify Selection preview shows Network Security Group Rules and your selected subscription name\n11. Under Condition, click Add Condition\n12. Select the Delete Network Security Group Rule signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description  appropriate resource group to save the alert to\n16. Check Enable alert rule upon creation checkbox\n17. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-137":{"article":"Azure Storage always stores multiple copies of your data so that it is protected from planned and unplanned events,\nincluding transient hardware failures, network or power outages, and massive natural disasters. \nRedundancy ensures that your storage account meets its availability and durability targets even in the face of failures.","impact":"Replicating a storage account to other regions affects data recovery \nif the region where the storage is located suffers from technical failures \ncaused by unplanned outages or failure of the entire data center.","report_fields":["id"],"remediation":"To enable Storage Account Replication follow this article:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-163":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination.\nThe Private Link platform handles the connectivity between the consumer and services over the Azure backbone network. \nBy mapping private endpoints to your Event Grid domain instead of the entire service, \nyou'll also be protected against data leakage risks. Learn more at https://aka.ms/privateendpoints.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Event Grid domains.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Event Grid domains and select the instance you want to remediate\n2. Under Settings, select Private endpoint connections\n3. Click Add and configure the private endpoint","multiregional":true,"service":"Event Grid"},"ecc-azure-219":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Batch Account and select Monitoring \n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Batch"},"ecc-azure-299":{"article":"Health check increases your application's availability by removing unhealthy instances from the load balancer. If your instance remains unhealthy, it will be restarted.","impact":"Lack of monitoring of Health can make it harder to find a problem when it occurs","report_fields":["id"],"remediation":"Using Azure Console:\nGo to your Function App and find Health Check under Monitoring in the left-side navigation menu.\nThen make sure to enable it.\nReferences:\nhttps://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check","multiregional":true,"service":"App Service"},"ecc-azure-043":{"article":"Create an activity log alert for the Delete Network Security Group event.","impact":"Monitoring for 'Delete Network Security Group' events gives insight into network access changes and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Network Security Groups under Filter by resource type\n7. Select All for Filter by location\n8. Click the subscription resource from the entries populated under Resource\n9. Click Done\n10. Verify Selection preview shows Network Security Groups and your selected subscription name\n11. Under Condition, click Add Condition\n12. Select the Delete Network Security Group signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-021":{"article":"Enable Vulnerability Assessment (VA) Periodic recurring scans for critical SQL servers and corresponding SQL databases.","impact":"Timely scheduled security checks can help identify security issues more quickly.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each server instance\n3. Click Security Center\n4. In the Vulnerability Assessment Settings section, set Storage Account unless already set\n5. Toggle Periodic recurring scans to ON.\n6. Click Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-117":{"article":"NOTE: This is a legacy recommendation. Managed Disks are encrypted by default and recommended for all new VM implementations. VHDs (Virtual Hard Disks) are stored in BLOB storage and are the old style disks that were attached to Virtual Machines, and the BLOB VHD was then leased to the VM. By default, storage accounts are not encrypted, and Azure Defender(Security Centre) would then recommend that the OS disks should be encrypted. Storage accounts can be encrypted as a whole using PMK or CMK and this should be turned on for storage accounts containing VHDs.","impact":"Storage account BLOB container is not encrypted by default and all VHDs stored in the storage account container are vulnerable.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Navigate to the storage account that you wish to encrypt\n2. Select the encryption option\n3. Select the key type that you wish to use","multiregional":true,"service":"Virtual Machines"},"ecc-azure-161":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination.\nThe private link platform handles the connectivity between the consumer and services over the Azure backbone network. \nBy mapping private endpoints to your app configuration instances instead of the entire service, \nyou'll also be protected against data leakage risks.\nLearn more at https://aka.ms/appconfig/private-endpoint.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your App Configuration service.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to App Configuration and select the instance you want to remediate. \n2. Under Settings, select Private endpoint connections\n3. Click Add and configure the private endpoint.","multiregional":true,"service":"App Configuration"},"ecc-azure-214":{"article":"Azure Defender for Resource Manager automatically monitors the resource management operations in your organization. Azure Defender detects threats and alerts you about suspicious activity. Learn more about the capabilities of Azure Defender for Resource Manager at https://aka.ms/defender-for-resource-manager. Enabling this Azure Defender plan results in charges. Learn about the pricing details per region on Security Center's pricing page at https://aka.ms/pricing-security-center.","impact":"Insufficient monitoring of your Azure Resource Manager operations can lead to increased response time to suspicious activities performed through REST APIs, Azure CLI, Portal interface, or other programmatic clients.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Security Center \n2. Select the Pricing & settings blade \n3. Click on the subscription name \n4. Select the Azure Defender plans blade\n5. On the line in the table, for Resource Manager, select On under Plan. \n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-205":{"article":"Use customer-managed keys to manage the encryption at rest of the contents of your registries. By default, the data is encrypted at rest with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management. Learn more at https://aka.ms/acr/CMK.","impact":"Unencrypted Container Registry instances with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"To encrypt Container Registry with CMK follow this article:\nhttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-customer-managed-keys","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-331":{"article":"detailedErrorLoggingEnabled should be set to 'true'","impact":"Lack of detailed error messages logging may cause difficulties in the incident response process. These logs provide the information about HTTP errors.","report_fields":["id"],"remediation":"From Azure Console\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click on each App\n4. Under the Settings section, click on Log\n5. Set detailedErrorMessages to appropriate value","multiregional":true,"service":"App Service"},"ecc-azure-024":{"article":"Enable SSL connection on PostgreSQLServers.","impact":"Unencrypted traffic between the PostgreSQL server and client applications can lead to man-in-the-middle attacks that easily disrupt communication and compromise the data transmission channel.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Connection security\n4. In the SSL settings.\n5. Click Enabled to enforce SSL connection","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-224":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; when a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Logic Apps and select appropriate service\n2. Under Diagnostic settings create new diagnostic settings.\n3. Select one of the options to store the diagnostics logs\n4. Click Save","multiregional":true,"service":"Azure Logic Apps"},"ecc-azure-123":{"article":"This policy identifies network security group  rules that allow inbound traffic to the Microsoft-DS port (445) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to port 445 can increase opportunities for malicious activities\nsuch as man-in-the-middle attacks (MITM), Denial of Service (DoS) attacks, or the Windows Null Session Exploit.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '445' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-176":{"article":"DDoS protection standard should be enabled for all virtual networks with a subnet that is part of an application gateway with a public IP.","impact":"Insufficient monitoring of DoS attack attempts and suspicious activity can increase response time to detect and react to threats.","report_fields":["id"],"remediation":"From Azure Console: \n1. Select a virtual network to enable the DDoS protection service standard on \n2. Select the Standard \n3. Click Save","multiregional":true,"service":"Virtual Network"},"ecc-azure-122":{"article":"Network security groups should be periodically evaluated for port misconfigurations. Where certain ports and protocols may be exposed to the Internet, they should be evaluated for necessity and restricted wherever they are not explicitly required and narrowly configured.","impact":"Port 80 is a frequent target for attacks. Allowing unrestricted HTTP access can increase opportunities for malicious activities such as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Portal: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '80' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-133":{"article":"Check a virtual machines on tags existence","impact":"Each tag consists of a name and a value pair, which makes resource administration easier.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Virtual Machines \n2. Select Tags on Settings plane:  \n   - Add Tag name   \n   - Add Value name \n3. Click Apply","multiregional":true,"service":"Virtual Machines"},"ecc-azure-027":{"article":"Enable log_connections on PostgreSQL Servers.","impact":"Enabled 'log_connections' affects the logging of connection attempts to the server.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for log_connections\n5. Click ON and Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-127":{"article":"This policy identifies network security group rules that allow inbound traffic to the Oracle DB port (1521) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted Oracle Database access can increase opportunities for malicious activities \nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss. \nAlso, unrestricted Oracle Database access allows remote attackers to execute\narbitrary database commands by performing a remote registration of a database instance or service name that already exists,\nthen conducting a man-in-the-middle (MITM) attack to hijack database connections.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '1521' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-146":{"article":"Disable public network access to your key vault so that it's not accessible over the public internet. \nThis can reduce data leakage risks. Learn more at https://aka.ms/akvprivatelink","impact":"Publicly accessible Key Vaults are vulnerable to leakage and disclosure of sensitive data\nsuch as keys, certificates, access tokens, etc.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Azure Key Vaults\n2. Select Networking and then select the Firewalls and virtual networks tab. \n3. Under Allow access from, select Selected networks. \n4. Add existing virtual networks to firewalls and virtual network rules:\n   - On the blade that opens, select the subscription, virtual networks, and subnets\n     that you want to allow access to this key vault.    \n   - Under IP Networks, add IPv4 address ranges by typing IPv4 address ranges in CIDR notation or individual IP addresses. \nNote: For a full list of the current Key Vault Trusted service section:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/overview-vnet-service-endpoints#trusted-services\n5. Select Save","multiregional":true,"service":"Key Vault"},"ecc-azure-236":{"article":"Cross-Origin Resource Sharing (CORS) should not allow all domains to access your API app. Allow only required domains to interact with your API app.","impact":"Weak Cross-origin resource sharing mechanism which allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served can cause increasing of risk to be attacked with CSRF attack.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service\n2. Select CORS\n3. Set '*' in Allowed Origins\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-232":{"article":"This policy audits any Windows/Linux virtual machines (VMs) if the Log Analytics agent is not installed which Security Center uses to monitor for security vulnerabilities and threats","impact":"Missing Log Analytics agent can lead to insufficient integration with Log Analytics Workspace and SIEM, such as Azure Sentinel, if the virtual machines scale sets is in the incident management scope","report_fields":["id"],"remediation":"To install Log Analytic Agent follow the link https://docs.microsoft.com/en-us/azure/azure-monitor/agents/log-analytics-agent","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-015":{"article":"SQL Server Audit Retention should be configured to be greater than 90 days.","impact":"The retention period settings affect how long the SQL database logs are kept. We recommend that you set the retention period in accordance with your company's or required compliance policy.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each server instance\n3. Click Auditing\n4. Select Storage Details\n5. Set the Retention (days) setting greater than 90 days\n6. Select OK\n7. Select Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-156":{"article":"Disable the public network access property to improve security \nand ensure your Azure Database for MariaDB can only be accessed from a private endpoint.\nThis configuration strictly disables access from any public address space outside of Azure IP range \nand denies all logins that match IP or virtual network-based firewall rules.","impact":"Publicly accessible MariaDB instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to MariaDB and click on the server you want to remediate \n2. Under the Settings blade, select Connection security \n3. Change Deny Public Network Access to Yes","multiregional":true,"service":"Azure Database for MariaDB"},"ecc-azure-321":{"article":"The log_statement setting specifies the types of SQL statements that are logged. Valid\nvalues are:\n\u2022 none (off)\n\u2022 ddl\n\u2022 mod\n\u2022 all (all statements)\nIt is recommended this be set to ddl unless otherwise directed by your organization's logging policy.","impact":"If the 'log_statement' parameter not set to the correct value, undesireble SQL statements would be logged.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'log_statement' to approptiate value (none, ddl, mod, all)\n5. Save changes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-344":{"article":"Enable Advanced Threat Detection on your non-Basic tier Azure database for MySQL servers to detect anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases.","impact":"Insufficient monitoring of your MySQL servers can increase the amount of time necessary to detect and respond to potential database vulnerabilities, as well as to abnormal activities that may indicate a threat to your databases.","report_fields":["id"],"remediation":"Follow this article to enable Microsoft Defender for MySQL Instances:\nhttps://docs.microsoft.com/en-us/azure/defender-for-cloud/defender-for-databases-usage","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-119":{"article":"Security groups provide stateful filtering of ingress network traffic to Azure resources.\nIt is recommended that no security group allow unrestricted ingress access.","impact":"When port 22 (SSH) is exposed to public access, it increases opportunities for malicious activities\nsuch as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks, \nwhich raises the risk of resource compromising.","report_fields":["id"],"remediation":"From Azure Portal: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges 'Any' \n   - Protocol 'Any' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-234":{"article":"The Guest Configuration extension requires a system assigned managed identity.\nAzure virtual machines in the scope of this policy will be non-compliant when they have the Guest Configuration extension installed but do not have a system assigned managed identity.\nLearn more at https://aka.ms/gcpol","impact":"Guest Configuration extension requires system-assigned managed identity enabled on virtual machines","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to \"Virtual machines\" and select one to configure\n2. Under the 'Settings' select 'Identity'\n3. Under 'System assigned' select 'On'\n4. Click 'Save'","multiregional":true,"service":"Virtual Machines"},"ecc-azure-053":{"article":"Ensure that OS disks (boot volumes) and data disks (non-boot volumes) are encrypted with CMK.","impact":"Unencrypted OS and data disks can be exposed to unauthorized reading and deleted without recovery by an attacker.","report_fields":["id"],"remediation":"From Azure Portal:  \nNote Disks must be detached from VMs to have encryption changed.\n1. Go to Virtual machines\n2. For each virtual machine, go to Settings\n3. Click Disks\n4. Click X to detach the disk from the VM\n5. Now search for Disks and locate the unattached disk\n6. Click the disk then select Encryption\n7. Change your encryption type, then select your encryption set\n8. Click Save\n9. Go back to the VM and re-attach the disk","multiregional":true,"service":"Azure Disk Storage"},"ecc-azure-126":{"article":"This policy identifies network security group rules that allow inbound traffic to the NetBIOS-SSN port (139) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted NetBIOS access can increase opportunities for malicious activities\nsuch as man-in-the-middle attacks (MITM), Denial of Service (DoS) attacks, or BadTunnel exploits.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '139' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-111":{"article":"Disable access from Azure services to PostgreSQL Database Server.\nIf access from Azure services is enabled, the server's firewall will accept connections from all Azure resources, including the resources that are not in your subscription.\nThis is usually not a desired configuration. Instead, setup firewall rules to allow access from specific network ranges or VNET rules to allow access from specific virtual networks.","impact":"Exposed and vulnerable service can access PostgreSQL server without proper authentification and authorization.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Connection security\n4. In Firewall rules, ensure Allow access to Azure services is set to OFF.\n5. Click Save to apply the changed rule.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-304":{"article":"Application Gateway allows to set network protocols Http and Https. It is highly recommended to use Https protocol for secure connections.","impact":"Using http protocol can make Application Gateway connections vulnerable","report_fields":["id"],"remediation":"az network application-gateway create --name --resource-group --http-settings-protocol Https\nReferences:\n1. https://docs.microsoft.com/en-us/cli/azure/network/application-gateway?view=azure-cli-latest","multiregional":true,"service":"Application Gateway"},"ecc-azure-162":{"article":"Azure Virtual Network deployment provides enhanced security and isolation for your Azure Cache for Redis as well as subnets, \naccess control policies, and other features to further restrict access. \nWhen an Azure Cache for Redis instance is configured with a virtual network, it is not publicly addressable \nand can only be accessed from virtual machines and applications within the virtual network.","impact":"When your Azure Cache for Redis instance is not configured with a virtual network, it can become an easy target for an attacker.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFor more information - https://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-how-to-premium-vnet","multiregional":true,"service":"Azure Cache for Redis"},"ecc-azure-348":{"article":"The local_infile parameter dictates whether files located on the MySQL client's computer can be loaded or selected via LOAD DATA INFILE or SELECT local_file.","impact":"Disabling local_infile reduces an attacker's ability to read sensitive files off the affected server via an SQL injection vulnerability.","report_fields":["id"],"remediation":"1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for MySQL server\n3. For each database, click Server parameters\n4. Search for local_infile\n5. Select OFF and Save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-220":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Data Lake Analytics Account and select Monitoring\n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Data Lake Analytics"},"ecc-azure-284":{"article":"Encrypting OS and data disks using customer-managed keys provides more control and greater flexibility in key management. This is a common requirement in many regulatory and industry compliance standards.","impact":"By default, data in a storage account is encrypted using Microsoft Managed Keys at rest.\nTo avoid data leaks on the side of the cloud provider and access more granular control over encryption infrastructure, you should use your own encryption key.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nTo enable Azure Kubernetes Service encryption with customer managed keys, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-069":{"article":"Periodically, newer versions are released for Java software either due to security flaws or to include additional functionality.\nUsing the latest Java version for web apps is recommended in order to take advantage of security fixes, if any, and/or new functionalities of the newer version.","impact":"Using old versions of TLS allows an attacker to execute well-known attacks.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click on each App\n4. Under the Settings section, click Application settings\n5. Under General settings, set Java version to the latest version available\n6. Set Java minor version to the latest version available\n7. Set Java web container to the latest version of web container available","multiregional":true,"service":"App Service"},"ecc-azure-014":{"article":"Enable Transparent Data Encryption on every SQL server.","impact":"Unencrypted SQL instances are vulnerable to data exposure.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL databases\n2. For each DB instance\n3. Click on Transparent data encryption\n4. Set Data encryption to On","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-048":{"article":"Disable an RDP access on network security groups from the Internet.","impact":"Publicly exposed RDP access can increase opportunities for malicious activities such as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks raising the risk of resource compromising.","report_fields":["id"],"remediation":"Disable a direct RDP access to your Azure Virtual Machines from the Internet. After the direct RDP access from the Internet is disabled, you have other options you can use to access these virtual machines for remote management - Point-to-site VPN - Site-to-site VPN - ExpressRoute","multiregional":true,"service":"Network security groups"},"ecc-azure-341":{"article":"Using a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution, data leakage and more.\nSet your Front Door Web Application Firewall (WAF) to prevent executing such mechanism using the rule definition below.\nAzure WAF has updated Default Rule Set (DRS) versions 1.0 and 1.1 with rule 944240 \u201cRemote Command Execution\u201d under Managed Rules to help in detecting and mitigating this vulnerability. This rule is already enabled by default in block mode for all existing WAF Default Rule Set configurations.\nLearn more around CVE-2021-44228","impact":"Using a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution and data leakage.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Front Door and CDN profiles\n2. Select appropriate Front Door service\n2. Under the Settings tab select Web application firewall\n3. Under WAF Policy select assigned policy\n4. From WAF Policy page go to the Managed Rules\n5. Look for 944240 rule id, select it and click Enable","multiregional":true,"service":"Azure Front Door"},"ecc-azure-110":{"article":"The Storage Table storage is a service that stores structure NoSQL data in the cloud, providing a key/attribute store with a schemaless design. Storage Logging happens server-side and allows details for both successful and failed requests to be recorded in the storage account. These logs allow users to see the details of read, write, and delete operations against the tables. Storage Logging log entries contain the following information about individual requests - timing information such as start time, end-to-end latency and server latency, authentication details , concurrency information, and the sizes of the request and response messages.","impact":"Disabled logging for Storage Account Table Service increases opportunities for malicious activities that cannot be detected or responded to. It is impossible to figure out what malicious actions were executed - who accessed the tables, where they were accessed from, and what operations were performed.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts.\n2. Select the specific Storage Account.\n3. Click the Diagnostics settings (classic) blade from the Monitoring (classic) section.\n4. Set Status to On, if set to Off.\n5. Select Table properties.\n6. Select the Read, Write and Delete options under the Logging section to enable Storage Logging for Table service.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-328":{"article":"Use customer-managed keys to manage the encryption at rest of your Azure Data Factory. By default, customer data is encrypted with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management.","impact":"Unencrypted Azure data factories with CMK are vulnerable to data exposure","report_fields":["id"],"remediation":"Note: A customer-managed key can only be configured on an empty data Factory. The data factory can't contain any resources such as linked services, pipelines and data flows. It is recommended to enable customer-managed key right after factory creation.\n\nTo encrypt Azure Data Factory with customer-managed-key follow this article (CLI):\nhttps://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key","multiregional":true,"service":"Azure Data Factory"},"ecc-azure-160":{"article":"Protect your subnet from potential threats by restricting access to it with a Network Security Group (NSG). \nNSGs contain a list of Access Control List (ACL) rules that allow or deny network traffic to your subnet.","impact":"NSGs contain a list of Access Control List (ACL) rules that allow or deny network traffic to your subnet. \nUnrestricted access to your subnet can disclose some vulnerabilities of the system.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Virtual Networks -> Subnets \n2. Select the existing Subnet and add Network Security Group to the blade \n3. Click Save","multiregional":true,"service":"Virtual Network"},"ecc-azure-128":{"article":"This policy identifies network security group rules that allow inbound traffic to the POP3 port (110) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to port 110 can increase opportunities for malicious activities\nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '110' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-165":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination. \nThe Private Link platform handles the connectivity between the consumer and services over the Azure backbone network. \nBy mapping private endpoints to Azure Machine Learning workspaces, data leakage risks are reduced. \nLearn more about private links at: https://docs.microsoft.com/azure/machine-learning/how-to-configure-private-link.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Machine Learning workspaces.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Machine Learning workspaces and select the one you want to remediate\n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint","multiregional":true,"service":"Azure Machine Learning"},"ecc-azure-096":{"article":"Turning on Azure Defender enables threat detection for Azure SQL database servers, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Lack of monitoring of your SQL Databases can lead to insufficient detection of suspicious activity and response to database incidents.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Azure SQL Databases Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-150":{"article":"Protect your virtual machines from potential threats by restricting access to them with network security groups (NSG). \nLearn more about controlling traffic with NSGs at https://aka.ms/nsg-doc","impact":"Virtual machines which publicly exposed while network security groups not configured on a network interface level are vulnerable to suspicious activities through management ports or services,\nallowing an attacker to take remote control of an instance.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups\n2. Select the existing NSG or create a new one\n3. On the Settings plane, configure Inbound rules and Outbounds rules \n4. Assign network security groups to Network Interfaces with primary ip configuration of your machine or Subnets\nwhere virtual machine resides:\n   - On the Settings plane, select Network interfaces -> Add   \n   - On the Settings plane, select Subnets -> Add","multiregional":true,"service":"Virtual Network"},"ecc-azure-020":{"article":"Enable Vulnerability Assessment (VA) service scans for critical SQL servers and corresponding SQL databases.","impact":"Misconfiguration and well-known SQL vulnerabilities can cause serious problems for the database service without proper security scans.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. Select a server instance\n3. Click Security Center\n4. Select Configure next to Enabled at subscription-level\n5. In the Vulnerability Assessment Settings section, click Storage Account\n6. Choose Storage Account (existing or Create New). Click Ok\n7. Click Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-215":{"article":"Security Center uses the Microsoft Dependency agent to collect network traffic data from your Azure virtual machines to enable advanced network protection features\nsuch as traffic visualization on the network map, network hardening recommendations and specific network threats.","impact":"Disabled or unconfigured network traffic logging and auditing on VMs can lead to increased response time to network threats\nand insufficient identification of security breaches.","report_fields":["id"],"remediation":"To install Azure Monitor Dependency Agent for Linux VM follow this article:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-linux","multiregional":true,"service":"Virtual Machines"},"ecc-azure-159":{"article":"Protect your storage accounts from potential threats using virtual network rules as a preferred method instead of IP-based filtering.\nDisabling IP-based filtering prevents public IPs from accessing your storage accounts.","impact":"Unauthorized network access to Storage Account can lead to broken access controls and sensitive data exposure.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to the storage account you want to secure.\n2. Select the settings menu called Networking.\n3. Check that you've selected to allow access from Selected networks.\n4. To grant access to a virtual network with a new network rule:\n   - Under Virtual networks, select Add existing virtual network,\n   - Select Virtual networks and Subnets options, and then select Add.  \nTo create a new virtual network and grant it access:\n   - Select Add new virtual network. \n   - Provide the information necessary to create a new virtual network, and then select Create.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-364":{"article":"Check an activity log alert on tags existence","impact":"Each tag consists of a name and a value pair, which makes resource administration easier.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Monitor (Alerts) \n2. Select Tags on Settings plane:  \n   - Add Tag name   \n   - Add Value name \n3. Click Apply","multiregional":true,"service":"Azure Monitor"},"ecc-azure-030":{"article":"Enable connection_throttling on PostgreSQL Servers.","impact":"Enabled 'connection_throttling' affects the verbosity of logged messages which, in turn, generates query and error logs with respect to concurrent connections.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for connection_throttling\n5. Click ON and Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-239":{"article":"Client certificates allow for the app to request a certificate for incoming requests. \nOnly clients that have a valid certificate will be able to reach the app.","impact":"Weak client certificate validation breaks the authentication of your API app within App Service, \nresulting in unrestricted access for an attacker if the client certificate has been forged.","report_fields":["id"],"remediation":"From Azure Console: \n1. Navigate to your App Service\n2. Select Configuration\n3. Go to the General Settings tab \n4. Set Incoming Client Certificates to Require","multiregional":true,"service":"App Service"},"ecc-azure-106":{"article":"The Storage Queue service stores messages that may be read by any client who has access to the storage account. A queue can contain an unlimited number of messages. Each of these messages can be up to 64KB in size using version 2011-08-18 or newer. Storage Logging happens server-side and allows details for both successful and failed requests to be recorded in the storage account. These logs allow users to see the details of read, write, and delete operations against the queues. Storage Logging log entries contain the following information about individual requests - timing information such as start time, end-to-end latency and server latency, authentication details , concurrency information, and the sizes of the request and response messages.","impact":"Disabled Storage logging for Queue service increases opportunities for malicious activities without the possibility to detect them or respond to. It is impossible to find out who, when, and where from has accessed the service configuration and performed malicious actions on message queues.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts.\n2. Select the specific Storage Account.\n3. Click the Diagnostics settings (classic) blade from the Monitoring (classic) section.\n4. Set Status to On, if set to Off.\n5. Select Queue properties.\n6. Select the Read, Write and Delete options under the Logging section to enable Storage Logging for Queue service.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-228":{"article":"To ensure secure configurations of in-guest settings of your machine, install the Guest Configuration extension. \nIn-guest settings that the extension monitors include the configuration of the operating system, application configuration or presence, and environment settings. \nOnce installed, in-guest policies will be available such as 'Windows Exploit guard should be enabled'. \nLearn more at https://aka.ms/gcpol.","impact":"Disabled or unconfigured audit and configuration operations inside virtual machines can lead to increased misconfigurations  and the emergence of security breaches.","report_fields":["id"],"remediation":"To install Guest Configuration for VM follow this article:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/extensions/guest-configuration","multiregional":true,"service":"Virtual Machines"},"ecc-azure-045":{"article":"Create an activity log alert for the Delete Security Solution event.","impact":"Monitoring for 'Delete Security Solution' events gives insight into changes to the active security solutions and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Security Solutions under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Click Done\n10. Verify Selection preview shows Security Solutions and your selected subscription name\n11. Under Condition click Add Condition\n12. Select the Delete Security Solutions signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter the Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-343":{"article":"Enable Advanced Threat Detection on your non-Basic tier Azure database for PostgreSQL servers to detect anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases.","impact":"Insufficient monitoring of your PostgreSQL servers can increase the amount of time necessary to detect and respond to potential database vulnerabilities, as well as to abnormal activities that may indicate a threat to your databases.","report_fields":["id"],"remediation":"Follow this article to enable Microsoft Defender for PostgreSQL Instances:\nhttps://docs.microsoft.com/en-us/azure/defender-for-cloud/defender-for-databases-usage","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-203":{"article":"Use customer-managed keys to manage the encryption at rest of your PostgreSQL servers. \nBy default, the data is encrypted at rest with service-managed keys,\nbut customer-managed keys are commonly required to meet regulatory compliance standards. \nCustomer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. \nYou have full control and responsibility for the key lifecycle, including rotation and management.","impact":"Unencrypted PostgreSQL instances with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFrom Azure Console:\n1. Go to Key Vaults and create a key vault with soft delete and purge protection enabled \n2. Under the Keys generate your own encryption key. \n3. Under the Access polices grant the Azure Database for PostgreSQL service permissions to the key vault with  get, wrapKey, unwrapKey permissions\n4. Go to Azure Database for PostgreSQL and under Data encryption fill the key vault and key information. Click Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-059":{"article":"Azure App Service Authentication is a feature that can prevent anonymous HTTP requests from reaching the API app, or authenticate those that have tokens before they reach the API app. If an anonymous request is received from a browser, App Service will redirect to a logon page. To handle the logon process, a choice from a set of identity providers can be made, or a custom authentication mechanism can be implemented.","impact":"Unconfigured App Service Authentication can help an attacker compromise the operation and integrity of the application data through anonymous HTTP requests to your API.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click each App\n4. Under the Settings section, click Authentication / Authorization\n5. Set App Service Authentication to On\n6. Choose other parameters as per your requirement and click Save","multiregional":true,"service":"App Service"},"ecc-azure-290":{"article":"When you deploy a container image to production, you might need an immutable container image. An immutable image is one that you can't accidentally delete or overwrite.","impact":"Locks will help to protect virtual machines from accidental or intentional deletion/overwriting.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Container Registries.\n2. Select Locks on Settings plane.\n3. Click Add:\n  - Add Lock name\n  - Lock type: Delete or Read-Only\n  - Add Note\n4. Click Ok","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-060":{"article":"Azure Web Apps allow sites to run under both HTTP and HTTPS by default. Web apps can be accessed by anyone using non-secure HTTP links by default. Non-secure HTTP requests can be restricted and all HTTP requests redirected to the secure HTTPS port. It is recommended to enforce HTTPS-only traffic.","impact":"An attacker can use HTTP requests to perform an injection attack or steal confidential data used for authorization. When HTTPS-only traffic is enabled, every incoming HTTP request is redirected to the HTTPS port. It adds an extra level of security to the HTTP requests made to the app.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click each App\n4. Under the Settings section, click SSL settings\n5. Set HTTPS Only to On under the Protocol Settings section","multiregional":true,"service":"App Service"},"ecc-azure-009":{"article":"Disable anonymous access to blob containers and disallow blob public access on storage account.","impact":"If a storage account is publicly available and an attacker has anonymous access to your blob container, he can compromise any data stored in the container, including sensitive data. This can lead to data exposure and potential phishing attacks. By default, a storage account allows configuring public access to containers in the account, but does not enable public access to your data. Public access to blob data is never permitted unless you take an additional step to explicitly configure the public access setting for a container.","report_fields":["id"],"remediation":"From Azure Portal First, follow Microsoft documentation and create shared access signature tokens for your blob containers. Then,\n1. Go to Storage Accounts\n2. For each storage account, go to Containers under BLOB SERVICE\n3. For each container, click Access policy\n4. Set Public access level to Private (no anonymous access)\n5. For each storage account, go to Allow Blob public access in Configuration\n6. Set Disabled if no anonymous access is needed on the storage account","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-258":{"article":"Remote debugging requires inbound ports to be opened on a web application. Remote debugging should be turned off.","impact":"Unrestricted debugging ports in your Web app increase a risk of data exposure and opens a way for an attacker to gain access to application resources.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service\n2. Under Settings select Configuration\n3. Set 'false' remote Debugging Enabled\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-033":{"article":"TDE with Customer-managed key support provides increased transparency and control over the TDE Protector, increased security with an HSM-backed external service, and promotion of separation of duties. With TDE, data is encrypted at rest with a symmetric key (called the database encryption key) stored in the database or data warehouse distribution. To protect this data encryption key (DEK) in the past, only a certificate that the Azure SQL Service managed could be used. Now, with Customer-managed key support for TDE, the DEK can be protected with an asymmetric key that is stored in the Key Vault. Key Vault is a highly available and scalable cloud-based key store which offers central key management, leverages FIPS 140-2 Level 2 validated hardware security modules (HSMs), and allows separation of management of keys and data, for additional security. Based on business needs or criticality of data/databases hosted a SQL server, it is recommended that the TDE protector is encrypted by a key that is managed by the data owner (Customer-managed key).","impact":"Insecure database encryption keys can be accessed by an attacker. The database is considered vulnerable if encryption keys are not protected with the asymmetric key.","report_fields":["id"],"remediation":"From Azure Portal:  \n1. Go to SQL servers for the desired server instance\n2. Click Transparent data encryption\n3. Set Use your own key to YES\n4. Browse through your key vaults to select an existing key or create a new key in Key Vault.\n5. Check Make selected key the default TDE protector","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-293":{"article":"SQL Server data should be replicated to avoid loss of unreplicated data","impact":"Lack of replication can increase the risk of data loss","report_fields":["id"],"remediation":"Azure Console:\n1. Go to SQL Servers\n2. For each SQL Server\n3. Select Failover groups\n4. Press the Add group link on top of the page","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-333":{"article":"Disabling the public network access property improves security by ensuring your IoT Hub can only be accessed from a private endpoint.","impact":"Publicly accessible IoT Hub are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to IoT Hub and click on the server you want to remediate\n2. Under the Settings blade, select Connection security\n3. Change Deny Public Network Access to Yes\n4. Configure firewall rules","multiregional":true,"service":"Azure IoT Hub"},"ecc-azure-170":{"article":"A private link provides a way to connect Key Vault to your Azure resources without sending traffic over the public internet. \nThe private link provides defense in depth protection against data exfiltration.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Key Vault service.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to KeyVault and select the one you want to remediate\n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint","multiregional":true,"service":"Key Vault"},"ecc-azure-011":{"article":"The Azure Storage blobs contain data like ePHI, Financial, secret or personal. Erroneously modified or accidentally deleted by an application or other storage account user cause data loss or data unavailability. It is recommended the Azure Storage be made recoverable by enabling soft delete configuration. This is to save and recover data when blobs or blob snapshots are deleted.","impact":"Users can accidentally run DELETE commands on Azure Storage blobs or blob snapshots, or an attacker/malicious user can do it deliberately to cause disruption.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Account\n2. For each Storage Account, navigate to Data Protection\n3. Select set Soft delete for the containers and blobs to Enabled and enter a number of days you want to retain soft deleted data.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-148":{"article":"Network access to Cognitive Services accounts should be restricted. \nConfigure network rules so that only applications from allowed networks can access the Cognitive Services account. \nTo allow connections from specific internet or on-premises clients,\naccess can be granted to traffic from specific Azure virtual networks or to public internet IP address ranges.","impact":"Unrestricted network access to Cognitive Services can lead to broken access controls and sensitive data exposure.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Cognitive Services \n2. Select Virtual network\n3. On the blade, check Selected networks\n4. Create a virtual network or add the existing one and configure the address range under Firewall settings \n5. Click Save","multiregional":true,"service":"Cognitive Services"},"ecc-azure-180":{"article":"Use a managed identity for enhanced authentication security","impact":"Unauthorized access to your App Service can allow a user or attacker to make unwanted changes \nand expose sensitive information about your function app, which can cause data loss and service degradation.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to App Service \n2. Under Platform features, select Identity \n3. Select ON within the System assigned tab 4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-158":{"article":"Disable the public network access property to improve security\nand ensure your Azure Database for PostgreSQL can only be accessed from a private endpoint.\nThis configuration disables access from any public address space outside of Azure IP range\nand denies all logins that match IP or virtual network-based firewall rules.","impact":"Publicly accessible PostgreSQL instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to PostgreSQL and click on the server you want to remediate\n2. Under the Settings blade, select Connection security \n3. Change Deny Public Network Access to Yes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-283":{"article":"Azure Kubernetes Service's resource logs can help recreate activity trails when investigating security incidents. Enable it to make sure the logs will exist when needed","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Kubernetes Services and select appropriate service.\n2. Under Diagnostic settings create new diagnostic settings.\n3. Select one of the options to store the diagnostics logs.\n4. Click Save.","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-216":{"article":"Security Center uses the Microsoft Dependency agent to collect network traffic data from your Azure virtual machines to enable advanced network protection features\nsuch as traffic visualization on the network map, network hardening recommendations and specific network threats.","impact":"Disabled or unconfigured network traffic logging and auditing on a vms can lead to increased response times to a network threats\nand identifying security breaches.","report_fields":["id"],"remediation":"To install Azure Monitor Dependency Agent for Windows VM follow this article:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-windows","multiregional":true,"service":"Virtual Machines"},"ecc-azure-144":{"article":"Restrict access to the Kubernetes Service Management API by granting API access only to IP addresses in specific ranges. \nIt is recommended to limit access to authorized IP ranges to ensure that only applications from allowed networks can access the cluster.","impact":"Unauthorized network access to the Kubernetes service can lead to DDoS attacks or data leakage, resulting in unstable operation of cluster services.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Kubernetes Services\n2. Under the Settings blade, select Networking\n3. Under the Networking blade:\n Security -> Check Set authorized IP ranges -> Specify ranges \n4. Click Save  \nFrom Azure CLI: \nRun the following command to update authorized IP ranges:   \n   az aks update --resource-group {rg-name} --name {aks-name} --api-server-authorized-ip-ranges {your-specified-ip-range}\nTo disable authorizied IP ranges:\n   az aks update --resource-group {rg-name} --name {aks-name} --api-server-authorized-ip-ranges","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-256":{"article":"Remote debugging requires inbound ports to be opened on API apps. Remote debugging should be turned off.","impact":"Unrestricted debugging ports in your API app increase a risk of data exposure and opens a way for an attacker to gain access to application resources.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service, choose your API App\n2. Under Settings select Configuration\n3. Set 'false' remote Debugging Enabled\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-012":{"article":"Enable sensitive data encryption at rest using Customer Managed Keys rather than Microsoft Managed keys","impact":"By default, data in a storage account is encrypted using Microsoft Managed Keys at rest. To avoid data leaks on the side of the cloud provider and access more granular control over encryption infrastructure, you should use your own encryption key.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts\n2. For each storage account, go to Encryption\n3. Set Customer Managed Keys\n4. Select the Encryption key and enter the appropriate setting value\n5. Click Save","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-025":{"article":"Enable SSL connection on MYSQLServers.","impact":"Unencrypted traffic between the MySQL server and client applications can lead to man-in-the-middle attacks that easily disrupt communication and compromise the data transmission channel.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for MySQL server\n3. For each database, click Connection security\n4. In the SSL settings\n5. Click Enabled for Enforce SSL connection","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-121":{"article":"This policy identifies network security group rules that allow inbound traffic to the FTP port (21) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted FTP access can increase opportunities for malicious activity\nsuch as brute-force attacks, FTP bounce attacks, spoofing attacks, and packet capture.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '20,21' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-197":{"article":"Virtual machines without enabled disk encryption will be monitored by Azure Security Center as recommendations.","impact":"Virtual Machines without Azure Disk Encryption configured can be vulnerable to unauthorized access to the data storage and watermarking attacks.","report_fields":["id"],"remediation":"To enable Azure Disk Encryption for Windows VMs, follow this guide: \n   - https://docs.microsoft.com/en-us/azure/virtual-machines/windows/disk-encryption-overview\nTo enable Azure Disk Encryption for Linux VMs, follow this guide:\n   - https://docs.microsoft.com/en-us/azure/virtual-machines/linux/disk-encryption-overview","multiregional":true,"service":"Virtual Machines"},"ecc-azure-166":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination. \nThe private link platform handles the connectivity between the consumer and services over the Azure backbone network. \nBy mapping private endpoints to your Azure SignalR Service resource instead of the entire service, you'll reduce your data leakage risks. \nLearn more about private links at https://aka.ms/asrs/privatelink.","impact":"An unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your SignalR service","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to SignalR service and select the instance you want to remediate\n2. Under the Settings, select Private endpoint connections\n3. Click Add and configure the private endpoint","multiregional":true,"service":"Azure SignalR Service"},"ecc-azure-371":{"article":"Enable audit_log_enabled on MySQL Servers","impact":"Enabling audit_log_enabled helps MySQL Database to log items such as connection attempts to the server, DDL/DML access, and more. Log data can be used to identify, troubleshoot, and repair configuration errors and suboptimal performance.","report_fields":["id"],"remediation":"From Azure Portal:\n  1. Select your Azure Database for MySQL server\n  2. For each database, under the Settings section in the sidebar, select Server parameters\n  3. Update the audit_log_enabled parameter to ON\n  4. Under the Monitoring section in the sidebar, select Diagnostic settings.\n  5. Provide a diagnostic setting name\n  6. Specify which data sinks to send the audit logs (storage account, event hub, and/or Log Analytics workspace)\n  7. Select \"MySqlAuditLogs\" as the log type\n  8. Once you've configured the data sinks to pipe the audit logs to, you can click Save\n  9. Access the audit logs by exploring them in the data sinks you configured. It may take up to 10 minutes for the logs to appear","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-372":{"article":"Set audit_log_events to include CONNECTION on MySQL Servers","impact":"Enabling CONNECTION helps MySQL Database to log items such as successful and failed connection attempts to the server. Log data can be used to identify, troubleshoot, and repair configuration errors and suboptimal performance.","report_fields":["id"],"remediation":"From Azure Portal:\n  1. From Azure Home select the Portal Menu\n  2. Select your Azure Database for MySQL server\n  3. For each database, under the Settings section in the sidebar, select Server parameters\n  4. Update the audit_log_enabled parameter to ON\n  5. Select the event types to be logged by updating the audit_log_events parameter; ensure CONNECTION is set\n  6. Under the Monitoring section in the sidebar, select Diagnostic settings.\n  7. Provide a diagnostic setting name 8 Specify which data sinks to send the audit logs (storage account, event hub, and/or Log Analytics workspace)\n  8. Select \"MySqlAuditLogs\" as the log type\n  9. Once you've configured the data sinks to pipe the audit logs to, you can click Save\n  10. Access the audit logs by exploring them in the data sinks you configured. It may take up to 10 minutes for the logs to appear","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-300":{"article":"Application Gateway allows to set TLS versions 1.0, 1.1 and 1.2. It is highly recommended to use the latest TLS 1.2 version for secure connections.","impact":"TLS 1.0 is vulnerable to man-in-the-middle attacks, risking the integrity and authentication of data. TLS 1.1 was formally deprecated in March 2021 and no longer treated as secure.","report_fields":["id"],"remediation":"az network application-gateway ssl-policy set --min-protocol-version TLSv1_2\nReferences:\n1. https://docs.microsoft.com/en-us/cli/azure/network/application-gateway/ssl-policy?view=azure-cli-latest#az_network_application_gateway_ssl_policy_set","multiregional":true,"service":"Application Gateway"},"ecc-azure-207":{"article":"Implementing Transparent Data Encryption (TDE) with your own key provides you with increased transparency and control over the TDE Protector,\nincreased security with an HSM-backed external service, and promotion of separation of duties.\nThis recommendation applies to organizations with a related compliance requirement.","impact":"Unencrypted SQL Managed instances with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to SQL Managed instances and select the one you want to remediate \n2. Under Security blade, select Transparent data encryption\n3. Select Customer managed key and fill the required fields Note You can create a new keyvault or use the existing one \n4. Click Save","multiregional":true,"service":"Azure SQL Managed Instance"},"ecc-azure-376":{"article":"Turning on Azure Defender enables threat detection for CosmosDB service, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Lack of monitoring of CosmosDB service can lead to insufficient detection of suspicious activity and response time to incidents.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade (Databases)\n5. Review the chosen pricing tier. For the Azure CosmosDB type Plan should be set to On.","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-098":{"article":"Turning on Azure Defender enables threat detection for Storage, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your Storage Service can allow an attacker to use access patterns, change access permissions, and upload malicious content without proper security response.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Storage Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-311":{"article":"The logging collector is a background process that captures log messages sent to stderr\nand redirects them into log files. The logging_collector setting must be enabled in order\nfor this process to run. It can only be set at the server start.","impact":"Enabled 'logging_collector' affects log messages storage point","report_fields":["id"],"remediation":"1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for logging_collector\n5. Click ON and Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-319":{"article":"The log_min_error_statement setting causes all SQL statements generating errors at or above the specified severity level to be recorded in the server log. Each level includes all the levels that follow it.","impact":"If the 'log_min_error_statement' parameter If this is not set to the correct value, too\tmany erring\tSQL\tstatements or too few erring SQL statements\tmay\tbe written to the server log","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'log_min_error_statement' to approptiate value (DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, PANIC)\n5. Save changes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-173":{"article":"Private endpoint connections enforce secure communication by enabling private connectivity to Azure Database for PostgreSQL. \nConfigure a private endpoint connection to enable access to traffic coming only from known networks \nand prevent access from all other IP addresses, including within Azure.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your PostgreSQL service.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to PostgreSQL and select the instance you want to remediate\n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-026":{"article":"Enable log_checkpoints on PostgreSQL Servers","impact":"Enabled 'log_checkpoints' affects server query and error logs to log each checkpoint","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for log_checkpoints.\n5. Click ON and Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-031":{"article":"Enable log_retention_days on PostgreSQL Servers.","impact":"Enabled 'log_retention_days' affects the number of days a log file is being retained.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for log_retention_days\n5. Enter a value in range 4-7 (inclusive) and save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-287":{"article":"Azure CNI networking for production allows for separation of control and management of resources. From a security perspective, you often want different teams to manage and secure those resources. With Azure CNI networking, you connect to existing Azure resources, on-premises resources, or other services directly via IP addresses assigned to each pod","impact":"Pods don't get full virtual network connectivity and can't be directly reached via their private IP address from connected networks. Azure CNI requires more IP address space.","report_fields":["id"],"remediation":"To configure Azure CNI Networking in AKS please refer to the following guide - https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-342":{"article":"The Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology.\nTo follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your MSSQL servers.","impact":"TLS 1.0 is vulnerable to man-in-the-middle attacks, risking the integrity and authentication of data. TLS 1.1 was formally deprecated in March 2021 and no longer treated as secure.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure SQL Servers.\n2. Under the Security select Firewalls and virtual networks.\n3. Under the Minimum TLS Version set 1.2. Click save.","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-177":{"article":"Deploy Azure Web Application Firewall (WAF) in front of public facing web applications for additional inspection of incoming traffic. \nWeb Application Firewall (WAF) provides centralized protection of your web applications from common exploits\nand vulnerabilities such as SQL injections, Cross-Site Scripting, local and remote file executions. \nYou can also restrict access to your web applications by countries, IP address ranges,\nand other http(s) parameters via custom rules.","impact":"Unrestricted and unfiltered network access to the App service can lead to DDoS attacks or data leakage, \nresulting in unstable operation of the application.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Web Application Firewall policies (WAF) \n2. Fill the Basics tab and configure:     \n  - Policy for Regional WAF (Application Gateway)   \n  - Policy state Enabled    \n  - Customize Azure Web Application Firewall as required\n3. Go to Azure Application Gateway and select the Azure Application Gateway that does not have Azure Web Application Firewall.\n4. From the left sidebar, select Settings, then select Web Application Firewall\nNote: If your current tier is not WAF V2, change it to WAF V2. There are differences in pricing when changing WAF tiers. \n5. Return to the Web Application Firewall created earlier:\n  - Select Associated application gateways on the sidebar\n  - Select Associate an application gateway and add your application gateway.\n6. Click Save","multiregional":true,"service":"Application Gateway"},"ecc-azure-151":{"article":"Enabling IP forwarding on a virtual machine's NIC allows the machine to receive traffic addressed to other destinations. \nIP forwarding is rarely required (e.g., when using the VM as a network virtual appliance), and therefore,\nthis should be reviewed by the network security team.","impact":"If IP forwarding is enabled on your virtual machine, an attacker can exploit this instance to route traffic\nand bypass firewalls in your networking perimeter.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Network interfaces and find the one attached to the VM you want to remediate \n2. Under Settings, click IP configurations \n3. Under the IP forwarding settings, check Disabled \n4. Click Save","multiregional":true,"service":"Virtual Network"},"ecc-azure-149":{"article":"Azure container registries by default accept connections over the internet from hosts on any network. \nTo protect your registries from potential threats, allow access from only specific public IP addresses or address ranges. \nIf your registry doesn't have an IP/firewall rule or a configured virtual network, it will appear in the unhealthy resources. \nLearn more about Container Registry network rules:\nhttps://aka.ms/acr/portal/public-network and https://aka.ms/acr/vnet.","impact":"Unrestricted network access to the Container Registries can lead to DDoS attacks or data leakage, \nresulting in unstable operation of container services.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Container Registries\n2. Under Settings, click Networking \n3. Under Public access, check Selected networks \n4. Under Firewall, specify IP address or IP ranges \n5. Click Save.","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-145":{"article":"Firewall rules should be defined on your Azure Cosmos DB accounts to prevent traffic from unauthorized sources. Accounts that have at least one IP rule defined with the virtual network filter enabled are deemed compliant. Accounts disabling public access are also deemed compliant.","impact":"CosmosDB account without a configured firewall is vulnerable to exposure and can be unstable due to malicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Azure Cosmos DB account\n2. Under the Settings blade, select Firewall and virtual networks \n3. Check Selected networks:\n   - Under Firewalls, specify IP ranges\n4. Click on Save","multiregional":true,"service":"Azure Cosmos DB"},"ecc-azure-323":{"article":"The default option for a Linux scale set uses basic authentication as an access credential for the secure shell network protocol.\nUsing SSH keys instead of common credentials (i.e. username and password) represents the best way to secure your Linux scale sets against malicious activities such as brute-force attacks, by providing a level of authorization that can only be fulfilled by privileged users who have ownership to the private key associated with the public key created on these sets. An attacker may be able to get access to the linux scale set\u2019s public key, but without the associated private key, he/she will be unable to gain shell access to the server.","impact":"Passwords are vulnerable to exposure, even if properly generated and stored. An SSH key pair is a more secure and easy way to administer authentication on a Linux virtual machine scale set.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Virtual machine scale sets\n2. Create an SSH key pair for the Linux virtual machine.\n3. Disable password authentication in the Linux virtual machine's configuration.\n4. Update the SSH key in your Azure Resource Manager template (replace the admin password with the adminSSHKey parameter) or via the Azure CLI (with the --generate-ssh-keys command).\nFollow this guide - https://docs.microsoft.com/en-us/azure/virtual-machines/linux/mac-create-ssh-keys","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-227":{"article":"It is recommended to enable Logs so that activity trail can be recreated when investigations are required in the event of an incident or a compromise.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"To install Azure Diagnostics extension on Virtual Machines Scale Sets follow this article:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-277":{"article":"Azure Database for MySQL allows you to choose the redundancy option for your database server. \nIt can be set to a geo-redundant backup storage in which the data is not only stored within the region in which your server is hosted,\nbut is also replicated to a paired region to provide recovery option in case of a region failure. \nConfiguring geo-redundant storage for backup is only allowed during server creation.","impact":"Replicating a backup to other regions affects data recovery if the region where the database is located suffers from technical failures \nas a result of unplanned outages or failure of the entire data center.","report_fields":["id"],"remediation":"To configure a Geo-Redundant Backup for your MySQL instance, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/mysql/concepts-backup","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-171":{"article":"Private endpoint connections enforce secure communication by enabling private connectivity to Azure Database for MariaDB.\nConfigure a private endpoint connection to enable access to traffic coming only from known networks\nand prevent access from all other IP addresses, including within Azure.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your MariaDB service.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to MariaDB and select the instance you want to remediate \n2. Under Settings, select Private endpoint connections\n3. Click Add and configure the private endpoint","multiregional":true,"service":"Azure Database for MariaDB"},"ecc-azure-240":{"article":"Client certificates allow for the app to request a certificate for incoming requests. Only clients that have a valid certificate will be able to reach the app.","impact":"Weak client certificate validation breaks the authentication of your web app within App Service, resulting in unrestricted access for an attacker if the client certificate has been forged.","report_fields":["id"],"remediation":"From Azure Portal: \n1. Navigate to your App Service\n2. Select Configuration\n3. Go to the General Settings tab \n4. Set Incoming Client Certificates to Require","multiregional":true,"service":"App Service"},"ecc-azure-071":{"article":"Periodically newer versions are released for PHP software either due to security flaws or to include additional functionality.\nUsing the latest PHP version for web apps is recommended in order to take advantage of security fixes, if any, and/or additional functionalities of the newer version.","impact":"Using old versions of TLS allows an attacker to execute well-known attacks.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click on each App\n4. Under the Settings section, click on Configuration\n5. Set PHP version to the latest version available under General settings","multiregional":true,"service":"App Service"},"ecc-azure-288":{"article":"For High-Availability reasons, ensure that you have at least 3 worker nodes running in your Cluster pool.","impact":"Less then 3 worker nodes running in your Cluster pool can be a reason of system has Low-Availability.","report_fields":["id"],"remediation":"In order to add additional worker nodes to your cluster pool, run the following CLI command:\n    az aks nodepool add --cluster-name --name --resource-group","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-318":{"article":"The log_line_prefix setting specifies a printf-style string that is prefixed to each logline.If blank, no prefix is used. You should configure this as recommended by the pgBadger development team unless directed otherwise by your organization's logging policy.","impact":"Properly setting log_line_prefix allows for adding additional information to each log entry (such as the user, or the database). Said information may then be of use in auditing or security reviews.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'log_line_prefix' to \"%m [%p]: [%l-1], db=%d,user=%u,app=%a,client=%h,\"\n5. Save changes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-257":{"article":"Remote debugging requires inbound ports to be opened on function apps. Remote debugging should be turned off.","impact":"Unrestricted debugging ports in your Function app increase a risk of data exposure and opens a way for an attacker to gain access to application resources.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service, choose your Function App\n2. Under Settings select Configuration\n3. Set 'false' remote Debugging Enabled\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-054":{"article":"Ensure that unattached disks in a subscription are encrypted with a Customer Managed Key (CMK).","impact":"Unencrypted unattached disks can be exposed to unauthorized reading and deleted without recovery by an attacker.","report_fields":["id"],"remediation":"If data stored in the disk is no longer useful, refer to Azure documentation to delete unattached data disks at https//docs.microsoft.com/en-us/rest/api/compute/disks/delete -https//docs.microsoft.com/en-us/cli/azure/disk?view=azure-cli-latest#az-disk-delete If data stored in the disk is important, to encrypt the disk, refer to Azure documentation at  https//docs.microsoft.com/en-us/azure/virtual-machines/disks-enable-customer-managed-keys-portal -https//docs.microsoft.com/en-us/rest/api/compute/disks/update#encryptionsettings","multiregional":true,"service":"Azure Disk Storage"},"ecc-azure-182":{"article":"Audit usage of client authentication only via Azure Active Directory in Service Fabric.","impact":"Broken client authentication on Service Fabric clusters can compromise access to the management endpoints.","report_fields":["id"],"remediation":"To setup Azure Active Directory for client authentication, follow this guide: \nhttps://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-setup-aad","multiregional":true,"service":"Azure Service Fabric"},"ecc-azure-072":{"article":"Encryption keys, Certificate thumbprints and Managed Identity Credentials can be coded into the APP service, this renders them visible as part of the configuration, to maintain security of these keys it is better to store in an Azure Keyvault and reference them from the Keyvault.","impact":"Storing secrets value of your App Service environment in a secure place reduce potential attack surface.","report_fields":["id"],"remediation":"To configure Key Vault reference for storing secrets of your Web App follow this article:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references?tabs=azure-cli","multiregional":true,"service":"App Service"},"ecc-azure-365":{"article":"Check an API Management on tags existence","impact":"Each tag consists of a name and a value pair, which makes resource administration easier.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to API Management\n2. Select Tags on Settings plane:  \n   - Add Tag name   \n   - Add Value name \n3. Click Apply","multiregional":true,"service":"API Management"},"ecc-azure-097":{"article":"Turning on Azure Defender enables threat detection for SQL servers on machines, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Lack of monitoring of your SQL Servers can lead to insufficient detection of suspicious activity and response to server incidents.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. Review the chosen pricing tier. For the SQL Servers on machines resource type Plan should be set to On.","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-109":{"article":"The Storage Blob service provides scalable, cost-efficient objective storage in the cloud. Storage Logging happens server-side and allows details for both successful and failed requests to be recorded in the storage account. These logs allow users to see the details of read, write, and delete operations against the blobs. Storage Logging log entries contain the following information about individual requests - timing information such as start time, end-to-end latency and server latency, authentication details , concurrency information, and the sizes of the request and response messages.","impact":"Disabled logging for Storage Account Blob Container increases opportunities for malicious activities that cannot be detected or responded to.  It is impossible to figure out what malicious actions were executed - where the container was accessed from and what operations were performed.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts.\n2. Select the specific Storage Account.\n3. Click the Diagnostics settings (classic) blade from the Monitoring (classic) section.\n4. Set Status to On, if set to Off.\n5. Select Blob properties.\n6. Select the Read, Write and Delete options under the Logging section to enable Storage Logging for Blob service.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-147":{"article":"Disabling public network access improves security by ensuring that Cognitive Services account isn't exposed to the public internet. Creating private endpoints can limit exposure of Cognitive Services account.\nLearn more at: https://go.microsoft.com/fwlink/?linkid=2129800.","impact":"Publicly accessible Cognitive Service are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Cognitive Service.\n2. Select an item from the list, and open the 'Networking'\n3. In 'Firewalls and virtual networks' select 'Disabled'\n5. Select Save.\nLearn more about Private Endpoints in https://go.microsoft.com/fwlink/?linkid=2129800.","multiregional":true,"service":"Cognitive Services"},"ecc-azure-302":{"article":"Redis Cache should not allow public access","impact":"Publicly accessible Redis Caches are vulnerable to leakage/disclosure of sensitive data and DDoS attacks","report_fields":["id"],"remediation":"To disable public network access to the Redis Cache follow this article:\nhttps://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-private-link","multiregional":true,"service":"Azure Cache for Redis"},"ecc-azure-010":{"article":"Restricting default network access helps to provide a new layer of security, since storage accounts accept connections from clients on any network. To limit access to selected networks, the default action must be changed.","impact":"Access to a storage account can be granted from all clients on any network. It can lead to data loss and compromised access to your resources.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts\n2. For each storage account, click the settings menu called Firewalls and Virtual networks.\n3. Ensure that you have elected to allow access from Selected networks.\n4. Add rules to allow traffic from specific network.\n5. Click Save to apply your changes.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-326":{"article":"Enabling encryption at rest using a customer-managed key on your Azure Data Explorer cluster provides additional control over the key being used by the encryption at rest. This feature is oftentimes applicable to customers with special compliance requirements and requires a Key Vault to managing the keys.","impact":"Unencrypted Azure Kusto clusters with CMK are vulnerable to data exposure. By default, Azure Kusto cluster is encrypted by Microsoft Managed Key.","report_fields":["id"],"remediation":"Follow this guide to enable Customer Managed Key encryption for Azure Kusto cluster:\nhttps://docs.microsoft.com/en-us/azure/data-explorer/customer-managed-keys-portal","multiregional":true,"service":"Azure Data Explorer"},"ecc-azure-006":{"article":"Enables emailing security alerts to the subscription owner or other designated security contact.","impact":"Enabling security alert emails ensures that these emails are received from Microsoft and that the right people are aware of any potential security issues and can mitigate the risk.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Click on Environment Settings\n3. Click on the appropriate Management Group, Subscription, or Workspace\n4. Click on Email notifications\n5. Under 'Notification types', check the check box next to Notify about alerts with the following severity (or higher): and select High from the drop down menu\n6. Click Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-132":{"article":"Check a delete lock on a virtual machine to prevent other users from accidentally deleting or modifying it.","impact":"Locks will help to protect virtual machines from accidental or intentional deletion.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Virtual Machines \n2. Select Locks on Settings plane. \n3. Click 'Add':\n   - Add Lock name\n   - Lock type 'Delete'  \n   - Add 'Note' \n4. Click 'Ok'","multiregional":true,"service":"Virtual Machines"},"ecc-azure-125":{"article":"This policy identifies network security group rules that allow inbound traffic to the MySQL DB port (3306) from public internet. \nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted MySQL access can increase opportunities for malicious activities\nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '3306' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-286":{"article":"In Kubernetes when you run modern, microservices-based applications, you often want to control which components can communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. The Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster.","impact":"Unlimited and unrestricted network traffic between pods can lead to anomalous activity such as DoS attacks.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nThe network policy feature can only be enabled when the cluster is created. You can't enable network policy on an existing AKS cluster. To create AKS cluster that supports network policy, please refer - https://docs.microsoft.com/en-us/azure/aks/use-network-policies?ocid=AID754288&wt.mc_id=CFID0471#create-an-aks-cluster-and-enable-network-policy","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-052":{"article":"Disable Internet exposed UDP ports on network security groups.","impact":"Publicly exposed UDP access can increase opportunities for malicious activities such as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks raising the risk of resource compromising.","report_fields":["id"],"remediation":"Disable a direct UDP access to your Azure Virtual Machines from the Internet. After the direct UDP access from the Internet is disabled, you have other options you can use to access UDP based services running on these virtual machines Point-to-site VPN Site-to-site VPN ExpressRoute","multiregional":true,"service":"Network security groups"},"ecc-azure-282":{"article":"To enhance data security, the data stored on the virtual machine (VM) host of your Azure Kubernetes Service nodes VMs should be encrypted at rest. This is a common requirement in many regulatory and industry compliance standards.","impact":"Unencrypted Kubernetes Clusters are vulnerable to eavesdropping and data exposure between all nodes in the cluster.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nTo enable Azure Kubernetes Service encryption, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/disks-enable-host-based-encryption-cli","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-067":{"article":"Create an activity log alert for the Create or Update Network Security Group Rule event.","impact":"Monitoring for Create or Update Network Security Group Rule events gives insight into network access changes and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Network Security Group Rules under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Click Done\n10. Verify Selection preview shows Network Security Group Rules and your selected subscription name\n11. Under Condition, click Add Condition\n12. Select the Create or Update Network Security Group Rule signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description  appropriate resource group to save the alert to\n16. Check Enable alert rule upon creation checkbox\n17. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-305":{"article":"Azure Storage sets the minimum TLS version to be version 1.0 by default. TLS 1.0 is a legacy version and has known vulnerabilities. This minimum TLS version can be configured to be later protocols such as TLS 1.2.","impact":"TLS 1.0 is vulnerable to man-in-the-middle attacks, risking the integrity and authentication of data. TLS 1.1 was formally deprecated in March 2021 and no longer treated as secure.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Navigate to your storage account in the Azure portal.\n2. Under Settings, select Configuration.\n3. Under Minimum TLS version, use the drop-down to select the minimum version of TLS required to access data in this storage account.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-168":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination.\nThe Private Link platform handles the connectivity between the consumer and services over the Azure backbone network.\nBy mapping private endpoints to your storage account, data leakage risks are reduced.\nLearn more about private links at https://aka.ms/azureprivatelinkoverview","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Container registry service.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Container registry and select the one you want to remediate.\n2. Under Settings, select Private endpoint connections\n3. Click Add and configure the private endpoint.","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-139":{"article":"Identifies volumes that have not had a backup or snapshot in the past 14 days. If a snapshot is not recent, it could be missing crucial patches and software updates. \nIt is recommended to keep backups as recent as you can.","impact":"Without a recently taken snapshot, it is impossible to recover data in the event of failure quickly.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Disks\n2. On the Overview board click on the 'Create snapshot'\n3. Fill required fields on the creation blade tabs (Basics, Encryption and Networking)\n4. Click 'Review + create'","multiregional":true,"service":"Azure Disk Storage"},"ecc-azure-179":{"article":"Use a managed identity for enhanced authentication security","impact":"Unauthorized access to your App Service can allow a user or attacker to make unwanted changes\nand expose sensitive information about your API app, which can cause data loss and service degradation.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to App Service \n2. Under Settings, select Identity\n3. Select ON within the System assigned tab\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-370":{"article":"Private endpoints limit network traffic to approved sources.","impact":"For sensitive data, private endpoints allow granular control of which services can communicate with Cosmos DB and ensure that this network traffic is private. You set this up on a case by case basis for each service you wish to be connected.","report_fields":["id"],"remediation":"From Azure Portal:\n  1. Open the portal menu.\n  2. Select the Azure Cosmos DB blade\n  3. Select the subscription you wish to audit.\n  4. Select the Database you wish to add an endpoint to.\n  5. Then in the portal menu column select the blade 'Private Endpoint Connections'.\n  6. Click '+ Private Endpoint'\n  7. Select which subscription and resource group you want the endpoint to be in. Name it as desired.\n  8. Select which subscription the endpoint will be under, its resource type.\n  9. Select which virtual network desired.\n  10. Select the DNS servers the endpoint will contact.\n  11. Tag as desired and create.\n  12. Back in the Private Endpoints view, select the endpoint and associate it with the Cosmos DB.\n  13. In the listing below confirm that the listed selected networks are set to the appropriate endpoints.","multiregional":true,"service":"Azure Cosmos DB"},"ecc-azure-349":{"article":"Limiting the number of concurrent sessions at the server and per user level helps to reduce the risk of DoS attacks. MySQL provides mechanisms to limit the number of simultaneous connections that can be made at the server level or by any given account.","impact":"Limiting concurrent connections to a MySQL server can be used to reduce risk of Denial of Service (DoS) attacks performed by exhausting connection resources.","report_fields":["id"],"remediation":"1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for MySQL server\n3. For each database, click Server parameters\n4. Search for max_user_connections\n5. Select appropriate value and Save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-028":{"article":"Enable log_disconnections on PostgreSQL Servers.","impact":"Enabled 'log_disconnections' affects the logging of each terminated session (with session duration) on the server.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for PostgreSQL server\n3. For each database, click Server parameters\n4. Search for log_disconnections\n5. Click ON and Save.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-141":{"article":"Azure Security Center has identified that some of your subnets weren't protected with a next-generation firewall.\nTo protect the subnets from potential threats, restrict the access to them\nusing Azure Firewall or a supported next-generation firewall.","impact":"All resources within a network where traffic routing to the firewall and firewall itself is not configured\nare vulnerable to exposure and can be unstable or lost due to malicious activity.","report_fields":["id"],"remediation":"From Azure Console: \n  1. Deploy Firewall in the existing or new virtual network (if required) \n  Note: Create a subnet named 'AzureFirewallSubnet'. The size of the subnet should be at least /26. \n   Search -> Firewall -> Create \n  2. Create Route tables:\n   Home -> Route tables -> Create \n  3. Add routes Create a route with Address prefix '0.0.0.0/0' addresses and Next hop type 'Internet'. \n  Also, create routes between the subnets that apply traffic routing.\n  For Next hop type, select Virtual appliance. \n  4. Add subnet association to the Route table: \n   Route tables -> Route name -> Subnets -> Associate","multiregional":true,"service":"Virtual Network"},"ecc-azure-007":{"article":"Enable security alert emails to subscription owners.","impact":"Disabled high severity email alerts can make risk mitigation impossible.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Click on Environment Settings\n3. Click on the appropriate Management Group, Subscription, or Workspace\n4. Click on Email notifications\n5. In the drop down of the All users with the following roles field select Owner\n6. Click Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-356":{"article":"API Management allows secure access to the backend service of an API using client certificates.","impact":"Appropriate client certificates should be configured to prevent broken authentication to the backend services.","report_fields":["id"],"remediation":"To configure client certificates to secure backend services follow this guide:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-mutual-certificates","multiregional":true,"service":"API Management"},"ecc-azure-346":{"article":"The Transport Layer Security (TLS) protocol secures transmission of data between servers and web browsers, over the Internet, using standard encryption technology. \nTo follow security best practices and the latest PCI compliance standards, enable the latest version of TLS protocol (i.e. TLS 1.2) for all your MySQL servers.","impact":"TLS 1.0 is vulnerable to man-in-the-middle attacks, risking the integrity and authentication of data. TLS 1.1 was formally deprecated in March 2021 and no longer treated as secure.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to MySQL Servers.\n2. Under the Settings select Connection Security\n3. Under the SSL Settings set Enforce SSL connection to Enabled\n4. Under the TLS Settings set Minimum TLS version to 1.2. Click save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-267":{"article":"Periodically, newer versions are released for Java software either due to security flaws or to include additional functionality. \nUsing the latest Java version for Function apps is recommended in order to take advantage of security fixes, if any, and/or new functionalities of the latest version. \nCurrently, this policy only applies to Linux web apps.","impact":"An outdated version of Java libraries may contain well-known vulnerabilities such as backdoors or code bugs, which are fixed in the latest version.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to to Azure App Service \n2. Under the Configuration blade, select the latest Java version","multiregional":true,"service":"App Service"},"ecc-azure-327":{"article":"Azure Data Factory is an ETL service for serverless data integration and data transformation. Azure Data Factory allows you to configure a Git repository with either Azure Repos or GitHub. Git is a version control system that allows for easier change tracking and collaboration.","impact":"Azure Data Factory version control absence can lead to data integrity and availability issues.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Data Factory resource\n2. Select Git configuration in the Source control section\n3. Configure repository","multiregional":true,"service":"Azure Data Factory"},"ecc-azure-065":{"article":"Periodically, newer versions are released for HTTP either due to security flaws or to include additional functionality. Use the latest HTTP version for web apps to take advantage of security fixes, if any, and/or new functionalities of the newer version.","impact":"Using old versions of HTTP allows an attacker to execute well-known attacks.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click each App\n4. Under the Settings section, click Configuration\n5. Set HTTP version to '2.0' under the General settings","multiregional":true,"service":"App Service"},"ecc-azure-099":{"article":"Turning on Azure Defender enables threat detection for Kubernetes, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your Kubernetes clusters can lead to a lack of security assessments and threat detection activities.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Kubernetes Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-265":{"article":"Audit each SQL Managed Instance which doesn't have recurring vulnerability assessment scans enabled. \nVulnerability assessment can discover, track, and help you remediate potential database vulnerabilities.","impact":"Misconfiguration and well-known SQL vulnerabilities can cause serious problems for managed instances without proper security scans.","report_fields":["id"],"remediation":"From Azure Console: \nNote: Azure Defender for SQL server must be enabled\n1. Go to SQL Managed instances and select the one you want to remediate\n2. Under the Security blade, select Security Center \n3. Select Configure under Azure Defender for SQL \n4. Under the Vulnerability Assessment settings, specify Subscription and Storage account \n5. Select On under Periodic reccuring scans (Also, recommended to specify email to recieve scan reports)\n6. Click Save","multiregional":true,"service":"Azure SQL Managed Instance"},"ecc-azure-345":{"article":"Enable infrastructure encryption for Azure Database for MySQL servers. If Double Encryption is enabled, another layer of encryption is implemented at the hardware level before the storage or network level.\nInformation will be encrypted before it is even accessed, preventing both interception of data in motion if the network layer encryption is broken and data at rest in system resources such as memory or processor cache.","impact":"Unencrypted MySQL instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"Follow this article to get familiar with double encryption for MySQL Servers:\nhttps://docs.microsoft.com/en-us/azure/mysql/concepts-infrastructure-double-encryption\nFollow this guide to enable Infrastructure double encryption:\nhttps://docs.microsoft.com/en-us/azure/mysql/howto-double-encryption","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-237":{"article":"Cross-Origin Resource Sharing (CORS) should not allow all domains to access your Function app. Allow only required domains to interact with your Function app.","impact":"Weak Cross-origin resource sharing mechanism which allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served can cause increasing of risk to be attacked with CSRF attack.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service\n2. Select CORS\n3. Set allowed domains in Allowed Origins\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-196":{"article":"Audit each SQL Managed Instance without advanced data security.","impact":"Insufficient monitoring of your SQL Managed Instances can lead to poor detection of suspicious activities \nand inadequate response to server incidents.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to SQL Managed instances and select the one you want to remediate \n2. Under the Security blade, select Security Center \n3. Select Enable Azure Defender for SQL servers \n4. Click Save","multiregional":true,"service":"Azure SQL Managed Instance"},"ecc-azure-094":{"article":"Turning on Azure Defender enables threat detection for Server, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your Servers' services can increase response time to incidents and make the search of endpoint detection and response (EDR) more difficult.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Servers Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-358":{"article":"Creating a workspace with a Managed workspace Virtual Network associated with it ensures that your workspace is network isolated from other workspaces. With a Managed workspace Virtual Network you can offload the burden of managing the Virtual Network to Azure Synapse.","impact":"Managed workspace Virtual Network along with Managed private endpoints protects against data exfiltration.","report_fields":["id"],"remediation":"Note: Managed virtual network for Azure Synapse workspaces can be only configured during resource creation.\nTo configure Managed virtual network for Azure Synapse workspaces follow this article:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-vnet","multiregional":true,"service":"Azure Synapse Analytics"},"ecc-azure-295":{"article":"Configure one Azure Active Directory account, either an individual or Network Security Group account, as an administrator. It is not necessary to configure an Azure AD administrator, but an Azure AD administrator must be configured if you want to use Azure AD accounts to connect to SQL Databases.It is recommended to avoid using names like 'admin' or 'administrator', which are targeted in brute force dictionary attacks.","impact":"Using \"admin\" like names for Admin account increasing the risk of Admin account compromization in some cases","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nAzure Console\n1. Go to SQL Servers\n2. For each SQL Server\n3. Select Active Directory admin\n4. Press the Set Admin at the top of the page\n5. Select the active directory user you want to set as AD Admin for the SQL server.\n6. Press the Remove admin if not needed.\nDefault Value - By default no AD Administrator is set for SQL server","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-281":{"article":"Upgrade your Kubernetes service cluster to a later Kubernetes version to protect against known vulnerabilities in your current Kubernetes version. Vulnerability CVE-2019-9946 has been patched in Kubernetes versions 1.11.9+, 1.12.7+, 1.13.5+, and 1.14.0+","impact":"CVE-2019-9946 has a network firewall misconfiguration which relates to CNI portmap plugin. It requires to check Kubernetes configuration if used one of the vulnerable versions.","report_fields":["id"],"remediation":"From Azure console:\n1. Go to Kubernetes services.\n2. Choose Kubernetes cluster.\n3. Go to Cluster configuration.\n4. Upgrade version.\n5. Click Save.","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-357":{"article":"Azure Databricks is a data analytics platform optimized for the Microsoft Azure cloud services platform. Azure Databricks offers three environments for developing data intensive applications: Databricks SQL, Databricks Data Science & Engineering, and Databricks Machine Learning.","impact":"Publicly accessible Azure Databricks are vulnerable to data exposure and unauthorized access","report_fields":["id"],"remediation":"To configure secure cluster connectivity follow Microsoft Docs article:\nhttps://docs.microsoft.com/en-us/azure/databricks/security/secure-cluster-connectivity","multiregional":true,"service":"Azure Databricks"},"ecc-azure-120":{"article":"This policy identifies network security group rules that allow inbound traffic to the DNS port (53) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted DNS access can increase opportunities for malicious activity \nsuch as Denial of Service (DoS) attacks or Distributed Denial of Service (DDoS) attacks. \nAlso, DNS can be used by attackers as one of their reconnaissance techniques.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '53' \n   - Protocol 'Any' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-325":{"article":"The disk encryption is implemented using either Azure Disk Encryption or encryption at host depending on the SKU of the cluster. The data is encrypted at rest using Microsoft-managed keys.","impact":"Azure Kusto cluster without disk encryption configured can be vulnerable to unauthorized access","report_fields":["id"],"remediation":"Follow this guide to enable disk encryption for Azure Kusto cluster:\nhttps://docs.microsoft.com/en-us/azure/data-explorer/cluster-encryption-disk","multiregional":true,"service":"Azure Data Explorer"},"ecc-azure-317":{"article":"The log_error_verbosity setting specifies the verbosity (amount of detail) of logged\nmessages. Valid values are:\n\u2022 TERSE\n\u2022 DEFAULT\n\u2022 VERBOSE\nwith each containing the fields of the level above it as well as additional fields.\nTERSE excludes the logging of DETAIL, HINT, QUERY, and CONTEXT error information.\nVERBOSE output includes the SQLSTATE, error code, and the source code file name, function name, and line number that generated the error. The appropriate value should be set based on your organization's logging policy","impact":"If the 'log_error_verbosity' parameter is not set to the correct value, too many details or too few details may be logged","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'log_error_verbosity' to approptiate value (TERSE, DEFAULT, VERBOSE)\n5. Save changes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-112":{"article":"Enable Network Watcher for Azure subscriptions. Network diagnostic and visualization tools available with Network Watcher help users understand, diagnose, and gain insights into the network in Azure.","impact":"Network diagnostic and visualization tools available with Network Watcher help users understand, diagnose, and gain insights into the network in Azure.","report_fields":["id"],"remediation":"Opting-out of Network Watcher automatic enablement is a permanent change. Once you opt-out you cannot opt-in without contacting support.","multiregional":true,"service":"Azure Subscription"},"ecc-azure-005":{"article":"Microsoft Defender for Cloud emails the Subscription Owner to notify them about security alerts. Adding your Security Contact's email address to the 'Additional email addresses' field ensures that your organization's Security Team is included in these alerts. This ensures that the proper people are aware of any potential compromise in order to mitigate the risk in a timely fashion.","impact":"Unconfigured notification of a security event to the right contact can lead to a late response to a threat and give an attacker additional time to exploit your resources.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Click on Environment Settings\n3. Click on the appropriate Management Group, Subscription, or Workspace\n4. Click on Email notifications\n5. Enter a valid security contact email address (or multiple addresses separated by commas) in the Additional email addresses field\n6. Click Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-167":{"article":"Azure Spring Cloud instances should use a virtual network injection for the following purposes: \n1. Isolate Azure Spring Cloud from Internet. \n2. Enable Azure Spring Cloud to interact with systems in either on-premises data centers or Azure service in other virtual networks. \n3. Empower customers to control inbound and outbound network communications for Azure Spring Cloud.","impact":"Poor management of network connectivity to your Azure Spring Cloud can lead to insufficient ingress/egress traffic control,\nwhich can cause unstable operation of microservices.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFrom Azure Console: \n1. Create a vnet or select the existing one\n2. Create a service runtime subnet and Spring Boot subnet\n3. Grant service permissions to the vnet (Owner):\n   - IAM -> Add role assignments -> Azure Spring Cloud Provider\n4. Deploy Azure Spring instance:\n   - Go to Azure Spring Cloud and click Add   \n   - Provide information about the subnets that you created before","multiregional":true,"service":"Azure Spring Apps"},"ecc-azure-374":{"article":"Create an activity log alert for the Create or Update Public IP Addresses rule","impact":"Monitoring of changes for 'Public IP Address events' can reduce the time it takes to detect unsolicited changes.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Policy Assignment under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Verify Selection preview shows All Policy assignment (policyAssignments) and your selected subscription name\n10. Click Done\n11. Under Condition, click Add Condition\n12. Select the Delete Public IP Address signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-272":{"article":"Audit the existence and health of an endpoint protection solution on your virtual machines scale sets to protect them from threats and vulnerabilities.","impact":"Lack of antimalware extensions on Virtual Machines Scale Sets can lead to instability of virtual machines due to the lack of virus scans in the signature database \nand decreased response to vulnerabilities caused by infected software.","report_fields":["id"],"remediation":"To install an Endpoint protection solution on your Azure VMSS follow this article:\nhttps://techcommunity.microsoft.com/t5/azure-security-center/security-control-enable-endpoint-protection/ba-p/1624653","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-022":{"article":"Configure 'Send scan reports to' with email ids of concerned data owners/stakeholders for a critical SQL servers.","impact":"Vulnerability Assessment (VA) scan reports and alerts will be sent to email IDs configured in 'Send scan reports to'.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. Select a server instance\n3. Click Security Center\n4. Ensure that Azure Defender for SQL is set to Enabled\n5. Select Configure next to Enabled at subscription-level\n6. In the Vulnerability Assessment Settings section, configure Storage Accounts unless not already configured\n7. Configure email ids for concerned data owners/stakeholders at Send scan reports to\n8. Click Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-124":{"article":"This policy identifies network security group rules that allow inbound traffic to the MongoDB port (27017) from public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted MongoDB Database access can increase opportunities for malicious activities \nsuch as hacking, denial-of-service (DoS) attacks, and Brute Force attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '27017,27018' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-275":{"article":"Ensure protection of your Azure Virtual Machines by enabling Azure Backup.\nAzure Backup is a secure and cost-effective data protection solution for Azure.","impact":"If a backup for virtual machines is not enabled, there is a risk of data loss after accidental or targeted deletion beyond recovery.","report_fields":["id"],"remediation":"To enable Azure Backup for Virtual Machines, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/backup/backup-azure-arm-vms-prepare","multiregional":true,"service":"Virtual Machines"},"ecc-azure-354":{"article":"By default, access to pull or push content from an Azure container registry is only available to authenticated users. Enabling anonymous (unauthenticated) pull access makes all registry content publicly available for read (pull) actions. Anonymous pull access can be used in scenarios that do not require user authentication such as distributing public container images.","impact":"Anonymous pull access applies to all images in a Container Registry. It is recommended to use default authentication methods (service principals, managed identity, etc.) or repository-scoped tokens to reduce read public access to Container Registry images.","report_fields":["id"],"remediation":"From Azure CLI:\naz acr update --name myregistry --anonymous-pull-enabled false\n\nFor more information use as a reference following article:\nhttps://docs.microsoft.com/en-us/azure/container-registry/anonymous-pull-access","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-095":{"article":"Turning on Azure Defender enables threat detection for App Service, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your App Service can lead to a lack of security assessments and threat detection activities.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for App Service Select On under Plan.\n6. Select `Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-055":{"article":"Ensure that all keys in the Azure Key Vault have an expiration time set.","impact":"Expired keys can be misused or exposed during their life cycle, which can lead to potential threats to data integrity and confidentiality.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Key vaults\n2. For each Key vault, click on Keys.\n3. Under the Settings section, make sure Enabled is set to Yes\n4. Set an appropriate EXPIRATION DATE on all keys.","multiregional":true,"service":"Key Vault"},"ecc-azure-036":{"article":"The storage account container containing the activity log export should not be publicly accessible.","impact":"If the activity log storage container is publicly available, an attacker can compromise log records for the entire subscription. This data can tell the attacker who can access your resources, when and where the resource was accessed, etc.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Search for Storage Accounts to access Storage account blade\n2. Click the storage account name\n3. In the Blob Service section, click Containers. It will list all the containers in the next blade\n4. Look for a record with the container named as insight-operational-logs.\n5. Click the right-most column to open Context menu\n6. From Context Menu, click Access Policy and set Public Access Level to Private (no anonymous access)","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-340":{"article":"Set your Application Gateway (WAF) to prevent executing such mechanism using the rule definition below.","impact":"Using a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string which can potentially lead to a risky code execution and data leakage.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Application Gateways\n2. Under the Settings select Web application firewall\n3. On the Rules tab select OWASP 3.0 or above in Rule set field\n   - or\n   If OWASP 3.0 or above selected, enable 800100 rule under Known-CVEs group.\n4. Click Save.","multiregional":true,"service":"Application Gateway"},"ecc-azure-301":{"article":"Ensure there are no firewall rules allowing unrestricted access to Redis from other Azure sources","impact":"Unauthorized network access to the Redis Cache can lead to DDoS attacks or data leakage","report_fields":["id"],"remediation":"Azure Console:\n1. Go to Redis Cache,\n2. For each Redis Cache:\n3. Select Firewall.\n4. Delete any Rule that has 0.0.0.0 in it's start and end ip address.\n5. Select Save.\nDefault Value:\nNo firewalls rules are set\nReferences:\nhttps://docs.microsoft.com/en-us/azure/redis-cache/cache-configure#firewall","multiregional":true,"service":"Azure Cache for Redis"},"ecc-azure-101":{"article":"Turning on Azure Defender enables threat detection for Key Vault, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your KeyVault service can lead to undetected unusual and potentially harmful attempts to access or exploit your keys, secrets, or certificates.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Key Vault Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-201":{"article":"Use customer-managed keys to manage the encryption at rest of your Azure Cosmos DB. \nBy default, the data is encrypted at rest with service-managed keys,\nbut customer-managed keys are commonly required to meet regulatory compliance standards. \nCustomer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. \nYou have full control and responsibility for the key lifecycle, including rotation and management. \nLearn more at https://aka.ms/cosmosdb-cmk.","impact":"Unencrypted CosmosDB accounts with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFrom Azure Console: \n1. Go to Key Vaults and create a key vault with soft delete and purge protection enabled \n2. Under the Keys generate your own encryption key. \n3. Under the Access polices grant the Azure CosmosDB service permissions to the key vault with  get, wrapKey, unwrapKey permissions \n4. Go to CosmosDB accounts and under Data encryption fill the key vault and key information. Click Save.","multiregional":true,"service":"Azure Cosmos DB"},"ecc-azure-064":{"article":"By default, Azure Functions, Web and API Services can be deployed over FTP. If FTP is required for an essential deployment workflow, FTPS should be required for FTP login for all App Service Apps and Functions.","impact":"FTP has a few exploits used by hackers - Anonymous Authentication, Directory Traversal Attack, Cross-Site Scripting (XSS), Dridex-based Malware Attack. Any deployment workflows that rely on FTP or FTPs rather than the WebDeploy or HTTPS endpoints can be affected.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Azure Portal\n2. Select App Services\n3. Click App\n4. Select Settings > Configuration\n5. Under Platform Settings, FTP state should be Disabled or FTPS Only","multiregional":true,"service":"App Service"},"ecc-azure-339":{"article":"Azure Key Vault is a service for Secrets management to securely store and control access to tokens, passwords, certificates, API keys, and other secrets.\nDifferent secrets have different rotation requirements. Content type tag should be set on secrets.","impact":"A content type tag helps identify whether a secret is a password, connection string, etc.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Key Vaults\n2. Under the Settings tab select Secrets\n3. Select secret with current version\n4. Under the Secret section provide content type of your secret\n5. Click Save.","multiregional":true,"service":"Key Vault"},"ecc-azure-116":{"article":"Install the endpoint protection for all virtual machines. Installing endpoint protection systems (like Antimalware for Azure) provides for real-time protection capability that helps identify and remove viruses, spyware, and other malicious software, with configurable alerts when known malicious or unwanted software attempts to install itself or run on Azure systems.","impact":"Lack of antimalware extensions can lead to instability of virtual machines due to the lack of virus scans in the signature database and decreased response to vulnerabilities caused by infected software.","report_fields":["id"],"remediation":"Follow Microsoft Azure documentation to install endpoint protection from the security center. Alternatively, you can employ your own endpoint protection tool for your OS.","multiregional":true,"service":"Virtual Machines"},"ecc-azure-276":{"article":"Azure Database for MariaDB allows you to choose the redundancy option for your database server. \nIt can be set to a geo-redundant backup storage in which the data is not only stored within the region in which your server is hosted,\nbut is also replicated to a paired region to provide recovery option in case of a region failure. \nConfiguring geo-redundant storage for backup is only allowed during server creation.","impact":"Replicating a backup to other regions affects data recovery if the region where the database is located suffers from technical failures\nas a result of unplanned outages or failure of the entire data center.","report_fields":["id"],"remediation":"To configure a Geo-Redundant Backup for your MariaDB instance, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/mariadb/concepts-backup","multiregional":true,"service":"Azure Database for MariaDB"},"ecc-azure-113":{"article":"Migrate BLOB-based VHDs to Managed Disks on Virtual Machines to exploit the default features of this configuration. The features include1. Default Disk Encryption2. Resilience as Microsoft will manage the disk storage and move it around if underlying hardware goes faulty3. Reduction of costs over storage accounts","impact":"VHDs stored in BLOB are vulnerable to malicious exploits if a container is misconfigured such as anonymously accessed and exposed to READ/WRITE operations from the Internet.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Using the search feature, go to Virtual Machines\n2. Select the virtual machine you would like to convert\n3. Select Disks in the menu for the VM\n4. At the top, select Migrate to managed disks\n5. You may follow the prompts to convert the disk and finish by selecting Migrate to start the process","multiregional":true,"service":"Virtual Machines"},"ecc-azure-066":{"article":"Create an activity log alert for the Delete Policy Assignment event.","impact":"Monitoring for Delete Policy Assignment events gives insight into changes done in 'azure policy - assignments' and can reduce the time it takes to detect unsolicited changes.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Policy Assignment under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription from the entries populated under Resource\n9. Verify Selection preview shows All Policy assignment (policyAssignments) and your selected subscription name\n10. Click Done\n11. Under Condition click Add Condition\n12. Select the Delete policy assignment signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-157":{"article":"Disable the public network access property to improve security \nand ensure your Azure Database for MySQL can only be accessed from a private endpoint. \nThis configuration strictly disables access from any public address space outside of Azure IP range \nand denies all logins that match IP or virtual network-based firewall rules.","impact":"Publicly accessible MySQL instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to MySQL and click on the server you want to remediate\n2. Under the Settings blade, select Connection security \n3. Change Deny Public Network Access to Yes","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-102":{"article":"This setting enables Windows Defender ATP (WDATP) integration with Security Center. WDATP integration brings comprehensive Endpoint Detection and Response (EDR) capabilities within security center. This integration helps to spot abnormalities, detect and respond to advanced attacks on Windows server endpoints monitored by Azure Security Center. Windows Defender ATP in Security Center supports detection on Windows Server 2016, 2012 R2, and 2008 R2 SP1 operating systems in a Standard service subscription. WDATP works only with Standard Tier subscriptions.","impact":"Ignoring this function may allow a hacker to exploit weaknesses on account of insufficient protection. WDATP integration brings comprehensive Endpoint Detection and Response (EDR) capabilities within the security center. This integration helps to spot abnormalities, detect and respond to advanced attacks on Windows server endpoints monitored by Azure Security Center. Windows Defender ATP in Security Center supports detection on Windows Server 2016, 2012 R2, and 2008 R2 SP1 operating systems in a Standard service subscription.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Select Security policy blade\n4. Click On Edit Settings to alter the the security policy for a subscription\n5. Select the Integrations blade\n6. Check/Enable option Allow Microsoft Defender for Endpoint to access my data\n7. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-032":{"article":"Use Azure Active Directory Authentication for authentication with SQL Database.","impact":"Broken access control allows an attacker to compromise admin access to your database.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each SQL server, click Active Directory admin\n3. Click Set admin\n4. Select an admin\n5. Click Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-050":{"article":"Ensure that no SQL Databases allow ingress from 0.0.0.0/0 (ANY IP).","impact":"Allowed connection from 0.0.0.0/0 or the Internet to the SQL instance is a severe vulnerability within the perimeter. This misconfiguration can increase the attack surface and lead to DoS attacks as one of the attack options that can degrade the state of the service.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each SQL server, click Firewall / Virtual Networks\n4. Set Allow access to Azure services to 'OFF'\n5. Set firewall rules to limit access to only authorized connections","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-213":{"article":"Azure Defender for DNS provides an additional layer of protection for your cloud resources by continuously monitoring all DNS queries from your Azure resources. Azure Defender alerts you about suspicious activity at the DNS layer. Learn more about the capabilities of Azure Defender for DNS at https://aka.ms/defender-for-dns. Enabling this Azure Defender plan results in charges. Learn about the pricing details per region on Security Center's pricing page at https://aka.ms/pricing-security-center.","impact":"Insufficient monitoring of your DNS services can lead to increased response time to various disruptions and downtime of domain services.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Security Center \n2. Select the Pricing & settings blade \n3. Click on the subscription name \n4. Select the Azure Defender plans blade\n5. On the line in the table, for DNS, select On under Plan. \n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-200":{"article":"It is important to enable encryption of Automation account variable assets when storing sensitive data","impact":"Unencrypted Automation account variables are vulnerable to disclosure","report_fields":["id"],"remediation":"Note: Variable encryption can only be done at creation time. If you want to encrypt existing variables, they need to be recreated.\nFrom Azure Portal:\n1. Go to Automation Account\n2. Under Shared Resources select Variables\n3. Click on Add a variable\n4. Fill the required information and then select 'Yes' under Encrypted setting\n5. Click Save","multiregional":true,"service":"Automation"},"ecc-azure-289":{"article":"The value that indicates whether the admin user is enabled. Each container registry includes an admin user account, which is disabled by default. \nYou can enable the admin user and manage its credentials in the Azure portal, or by using the Azure CLI or other Azure tools.\nAll users authenticating with the admin account appear as a single user with push and pull access to the registry.\nChanging or disabling this account disables registry access for all users who use its credentials.","impact":"Container registry may be weak to unauthorized access by malicious authorities","report_fields":["id"],"remediation":"From Azure CLI:\naz acr update -n {name} --admin-enabled true","multiregional":true,"service":"Azure Container Registry"},"ecc-azure-225":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Cognitive Search and select appropriate service\n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Azure Cognitive Search"},"ecc-azure-241":{"article":"Client certificates allow for the app to request a certificate for incoming requests. \nOnly clients that have a valid certificate will be able to reach the app.","impact":"Weak client certificate validation breaks the authentication of your function app within App Service, \nresulting in unrestricted access for an attacker if the client certificate has been forged.","report_fields":["id"],"remediation":"From Azure Console: \n1. Navigate to your App Service\n2. Select Configuration\n3. Go to the General Settings tab \n4. Set Incoming Client Certificates to Require","multiregional":true,"service":"App Service"},"ecc-azure-206":{"article":"Service Fabric provides three levels of protection (None, Sign and EncryptAndSign) for node-to-node communication using a primary cluster certificate. \nSet the protection level to ensure that all node-to-node messages are encrypted and digitally signed.","impact":"Unencrypted Service Fabric clusters are vulnerable to eavesdropping between all nodes in the cluster.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFrom Azure Console: \n1. Go to Service Fabric cluster \n2. Click Custom fabric settings and click Add New \n3. Configure the Security section and update the ClusterProtectionLevel property to EncryptAndSign","multiregional":true,"service":"Azure Service Fabric"},"ecc-azure-152":{"article":"Possible network Just In Time (JIT) access will be monitored by Azure Security Center as recommendations","impact":"Unprotected management ports allow an attacker to take remote control of an instance.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Security Center \n2. Open the Azure Defender dashboard and from the advanced protection area, select Just-in-time VM access\n3. The Just-in-time VM access page opens with your VMs grouped into the following tabs:    \n   - Configured     \n   - Not configured     \n   - Unsupported \n4. From the Not configured tab, mark the VMs to protect with JIT and select Enable JIT on VMs. \nThe JIT VM access page opens the list of ports recommended for protection by Security Center:\n    - 22    \n    - 3389    \n5. To accept the default settings, select Save or customize the JIT options","multiregional":true,"service":"Virtual Machines"},"ecc-azure-184":{"article":"Although SSH itself provides an encrypted connection, using passwords with SSH still leaves the VM vulnerable to brute-force attacks.\nThe most secure option for authenticating to an Azure Linux virtual machine over SSH is with a public-private key pair, also known as SSH keys.\nLearn more at https://docs.microsoft.com/azure/virtual-machines/linux/create-ssh-keys-detailed.","impact":"Passwords are vulnerable to exposure, even if properly generated and stored.\nAn SSH key pair is a more secure and easy way to administer authentication on a Linux virtual machine.","report_fields":["id"],"remediation":"To use SSH for authentication to your Linux virtual machine: \n1. Create an SSH key pair for the Linux virtual machine. \n2. Disable password authentication in the Linux virtual machine's configuration. \n3. Update the SSH key in your Azure Resource Manager template (replace the admin password with the adminSSHKey parameter)\nor via the Azure CLI (with the --generate-ssh-keys command). Follow this guide:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/mac-create-ssh-keys","multiregional":true,"service":"Virtual Machines"},"ecc-azure-329":{"article":"Use customer-managed keys to manage the encryption at rest of your Batch account's data.","impact":"By default, customer data is encrypted with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. CMK management have full control and responsibility for the key lifecycle, including rotation and management.","report_fields":["id"],"remediation":"To encrypt Batch account with CMK follow this article:\nhttps://docs.microsoft.com/en-us/azure/batch/batch-customer-managed-key","multiregional":true,"service":"Batch"},"ecc-azure-023":{"article":"Enable Vulnerability Assessment (VA) setting 'Also send email notifications to admins and subscription owners'.","impact":"Enabling 'Also send email notifications to admins and subscription owners' will ensure that VA scan reports and alerts are sent to admins and subscription owners. This may help reduce the time required for identifying risks and taking corrective measures.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. Select a server instance\n3. Click Security Center\n4. Select Configure next to Enabled at subscription-level\n5. In the Vulnerability Assessment Settings section, configure Storage Accounts unless not already configured\n6. Check/enable Also send email notifications to admins and subscription owners\n7. Click Save","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-294":{"article":"Availability sets ensure that the VMs you deploy on Azure are distributed across multiple isolated hardware clusters.","impact":"If a hardware or software failure within virtual machine happens, impacted vms are inaccessible unless they reside in availability set.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nTo assign Azure Virtual Machine to an availability set please refer to the following document:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/manage-availability#configure-multiple-virtual-machines-in-an-availability-set-for-redundancy","multiregional":true,"service":"Virtual Machines"},"ecc-azure-129":{"article":"This policy identifies network security group rules that allow inbound traffic to the PostgreSQL port (5342) from the public internet. \nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to port 5432 can increase opportunities for malicious activities \nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '5342' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-105":{"article":"Regenerate storage account access keys periodically. When a storage account is created, Azure generates two 512-bit storage access keys. These keys are used for authentication when the storage account is accessed. Rotating these keys periodically ensures that any inadvertent access or exposure does not result in these keys being compromised.","impact":"Old access keys are vulnerable to attacker disclosure, which could lead to data leakage and ultimately loss of access to the storage account.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts\n2. Under Security+Networking select Access Keys\n3. Click 'Rotate key'","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-332":{"article":"requestTracingEnabled should be set to 'true'","impact":"Lack of failed request tracing logs may cause difficulties in the incident response process.","report_fields":["id"],"remediation":"From Azure Console\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click on each App\n4. Under the Settings section, click on Log\n5. Set requestTracing to appropriate value","multiregional":true,"service":"App Service"},"ecc-azure-058":{"article":"Ensure that RBAC is enabled on all Azure Kubernetes Services Instances","impact":"Without configured role-based access controls, an attacker can get the elevation of privileges.","report_fields":["id"],"remediation":"WARNING This setting cannot be changed after AKS deployment, the cluster will require recreation.","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-379":{"article":"Enable AppServiceHTTPLogs diagnostic log category for Azure App Service instances to ensure all http requests are captured and centrally logged.","impact":"Capturing web requests can be important supporting information for security analysts performing monitoring and incident response activities.","report_fields":["id"],"remediation":"From Azure Portal: \n1. Go to App Service and select appropriate service \n2. Under Diagnostic settings create new diagnostic settings. \n3. Select HTTP Logs to store the diagnostics logs (Storage Account, Log Workpsace, etc.)\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-202":{"article":"Manage encryption at rest of Azure Machine Learning workspace data with customer-managed keys.\nBy default, customer data is encrypted with service-managed keys, but customer-managed keys are commonly required to meet regulatory compliance standards. \nCustomer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. \nYou have full control and responsibility for the key lifecycle, including rotation and management.\nLearn more at https://aka.ms/azureml-workspaces-cmk.","impact":"Unencrypted Machine Learning workspace instances with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nTo encrypt Azure Machine Learning Workspaces with customer-managed-key follow this article (CLI):\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace-cli?tabs=createnewresources%2Cvnetpleconfigurationsv2cli#customer-managed-key-and-high-business-impact-workspace","multiregional":true,"service":"Azure Machine Learning"},"ecc-azure-350":{"article":"MySQL can operate using a variety of log files, each used for different purposes. These are the binary log (which can be encrypted), error log, slow query log, relay log, general log, and in the enterprise edition, the audit log (which can be encrypted).","impact":"The slow query log can be used to find queries that take a long time to execute and are therefore candidates for optimization.","report_fields":["id"],"remediation":"1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for MySQL server\n3. For each database, click Server parameters\n4. Search for slow_query_log\n5. Select ON and Save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-355":{"article":"Azure Machine Learning compute cluster is a managed-compute infrastructure that allows you to easily create a single or multi-node compute. The compute cluster is a resource that can be shared with other users in your workspace.","impact":"To avoid charges when no jobs are running, set the minimum nodes to 0. This setting allows Azure Machine Learning to de-allocate the nodes when they aren't in use. Any value larger than 0 will keep that number of nodes running, even if they are not in use.","report_fields":["id"],"remediation":"To configure Compute cluster nodes follow this article:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python","multiregional":true,"service":"Azure Machine Learning"},"ecc-azure-278":{"article":"Azure Database for PoatgreSQL allows you to choose the redundancy option for your database server. \nIt can be set to a geo-redundant backup storage in which the data is not only stored within the region in which your server is hosted,\nbut is also replicated to a paired region to provide recovery option in case of a region failure. \nConfiguring geo-redundant storage for backup is only allowed during server creation.","impact":"Replicating a backup to other regions affects data recovery if the region where the database is located suffers from technical failures \nas a result of unplanned outages or failure of the entire data center.","report_fields":["id"],"remediation":"To configure a Geo-Redundant Backup for your PostgreSQL instance, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/postgresql/concepts-backup","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-378":{"article":"Network Security Group Flow Log Analytics should be enabled.","impact":"Traffic analytics is a cloud-based solution that provides visibility into user and application activity in a cloud networks. Specifically, traffic analytics analyzes Azure Network Watcher network security group (NSG) flow logs to provide insights into traffic flow.","report_fields":["id"],"remediation":"From Azure Portal:\nNote: Log Analytics workspace and Storage Account are needed for enabling Traffic Analytics\n1. Go to NSG\n2. Select NSG flow logs blade in the Logs section\n3. Select each Network Security Group from the list\n4. Ensure Status is set to On\n5. Select your storage account in the Storage account field\n6. On a Configure tab select Enable Traffic Analytics\n7. Select your workspace in the Log Analytics workspace field\n8. Select Save","multiregional":true,"service":"Network security groups"},"ecc-azure-362":{"article":"Defender for Cloud collects data from your machines using agents and extensions. To save you the process of manually installing the extensions, such as the manual installation of the Log Analytics agent, Defender for Cloud reduces management overhead by installing all required extensions on existing and new machines.","impact":"Onboarded vulnerability assessment for virtual machines reduce the risk of exposure and simplify threat management.","report_fields":["id"],"remediation":"To configure Auto-deploy of Vulnerability Assessment for Virtual Machines follow this articles:\nhttps://learn.microsoft.com/en-us/azure/defender-for-cloud/auto-deploy-vulnerability-assessment","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-334":{"article":"In 2019, Microsoft added a feature called Jupyter Notebook to Cosmos DB that lets customers visualize their data and create customized views. The feature was automatically turned on for all Cosmos DBs in February 2021. A series of misconfigurations in the notebook feature opened up a new attack vector - the notebook container allowed for a privilege escalation into other customer notebooks. As a result, an attacker could gain access to customers' Cosmos DB primary keys and other highly sensitive secrets such as the notebook blob storage access token.\nOne way to reduce risk is to prevent management plane changes for clients using key based authentication. CosmosDB access keys are mainly used by applications to access data in CosmosDB containers. It is rare for organizations to have use cases where the keys are used to make management changes.","impact":"Priveleges escalation can give an attacker access to the sensitive information contains in CosmosDB account","report_fields":["id"],"remediation":"For more details visit - https://msrc-blog.microsoft.com/2021/08/27/update-on-vulnerability-in-the-azure-cosmos-db-jupyter-notebook-feature/","multiregional":true,"service":"Azure Cosmos DB"},"ecc-azure-142":{"article":"Azure Security Center has identified some of your network security groups inbound rules to be too permissive. \nInbound rules should not allow access from 'Any' or 'Internet' ranges. \nThis can potentially enable attackers to target your resources.","impact":"Opened network ports on the network security group level can increase opportunities for malicious activities\nsuch as denial-of-service (DoS) attacks. This is a serious network security breach. \nUse VM-JIT-access as a way to access your virtual machines securely.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules\n4. Create Inbound rule:\n   - Source 'Any'   \n   - Source port ranges '*'   \n   - Destination 'Any'   \n   - Service 'Custom'  \n   - Destination port ranges '*' \n   - Protocol 'Any'   \n   - Action 'Deny'   \n   - Priority {define high priority}   \nClick Add \nNote: You can create your custom rules depending on the resources you have. \nInbound rules should not allow access from 'Any' or 'Internet' ranges.   \n4. Check Network interfaces or Subnets assignments:  \n  - On the Settings plane, select Network interfaces   \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-306":{"article":"Enable encryption at rest for PostgreSQL Databases. If Double Encryption is enabled, another layer of encryption is implemented at the hardware level before the storage or network level. Information will be encrypted before it is even accessed, preventing both interception of data in motion if the network layer encryption is broken and data at rest in system resources such as memory or processor cache.","impact":"Unencrypted PostgreSQL instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nFrom Azure Console:\n1. Select Databases > Azure Database for PostgreSQL. You can also enter PostgreSQL in the search box to find the service. Enabled the Single server deployment option.\n2. Provide the basic information of the server. Select Additional settings and enabled the Infrastructure double encryption checkbox to set the parameter.\n3. Select Review + create to provision the server.\n4. Once the server is created you can validate the infrastructure double encryption by checking the status in the Data encryption server blade.","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-070":{"article":"Periodically, newer versions are released for Python software either due to security flaws or to include additional functionality.\nUsing the latest Python version for web apps is recommended in order to take advantage of security fixes, if any, and/or additional functionalities of the newer version.","impact":"Using old versions of TLS allows an attacker to execute well-known attacks.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click on each App\n4. Under the Settings section, click on Application settings\n5. Set Python version to the latest version available under General settings","multiregional":true,"service":"App Service"},"ecc-azure-181":{"article":"Use a managed identity for enhanced authentication security","impact":"Unauthorized access to your App Service can allow a user or attacker to make unwanted changes and expose sensitive information about your web app, which can cause data loss and service degradation.","report_fields":["id"],"remediation":"From Azure Portal: \n1. Go to App Service \n2. Under Settings, select Identity\n3. Select ON within the System assigned tab \n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-359":{"article":"Azure Synapse Analytics workspaces support enabling data exfiltration protection for workspaces. With exfiltration protection, you can guard against malicious insiders accessing your Azure resources and exfiltrating sensitive data to locations outside of your organization's scope.","impact":"Azure Synapse workspace may be vulnerable to data exfiltration.","report_fields":["id"],"remediation":"Note: Data Exfiltration protection may be enabled along with the Managed virtual network for Azure Synapse workspaces and can be only configured during resource creation.\nTo configure Data Exfiltration protection for Azure Synapse workspaces follow this articles:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-vnet\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection","multiregional":true,"service":"Azure Synapse Analytics"},"ecc-azure-351":{"article":"When data changing statements are made (i.e., INSERT, UPDATE), MySQL can handle invalid or missing values differently depending on whether strict SQL mode is enabled. When strict SQL mode is enabled, data may not be truncated or otherwise \"adjusted\" to make the data changing statement work.","impact":"Without strict mode the server tries to proceed with the action when an error might have been a more secure choice.","report_fields":["id"],"remediation":"1. Login to Azure Portal using https//portal.azure.com\n2. Go to Azure Database for MySQL server\n3. For each database, click Server parameters\n4. Search for sql_mode\n5. Add \"STRICT_ALL_TABLES\" checkmark and Save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-231":{"article":"This policy audits any Windows/Linux virtual machines (VMs) if the Log Analytics agent is not installed which Security Center uses to monitor for security vulnerabilities and threats","impact":"Missing Log Analytics agent can lead to insufficient integration with Log Analytics Workspace or SIEM, such as Azure Sentinel, if the virtual machines is in the incident management scope","report_fields":["id"],"remediation":"To install Log Analytics Agent follow this article:\nFor Windows VM - https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-windows?toc=/azure/azure-monitor/toc.json\nFor Linux VM - https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-linux?toc=/azure/azure-monitor/toc.json","multiregional":true,"service":"Virtual Machines"},"ecc-azure-337":{"article":"This policy audits any Windows virtual machine not configured with automatic update of Microsoft Antimalware protection signatures.","impact":"Azure will handle all signatures updates for Microsoft Antimalware extension.","report_fields":["id"],"remediation":"Follow Microsoft Azure documentation to install endpoint protection from the security center and configure auto updates.","multiregional":true,"service":"Virtual Machines"},"ecc-azure-164":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination.\nThe Private Link platform handles the connectivity between the consumer and services over the Azure backbone network. \nBy mapping private endpoints to your Event Grid topic instead of the entire service, you'll also be protected against data leakage risks.\nLearn more at: https://aka.ms/privateendpoints.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Event Grid topics.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Event Grid topics and select the one you want to remediate\n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint","multiregional":true,"service":"Event Grid"},"ecc-azure-155":{"article":"Disabling the public network access property improves security by ensuring your Azure SQL Database can only be accessed from a private endpoint. \nThis configuration denies all logins that match IP or virtual network-based firewall rules.","impact":"Publicly accessible SQL instances are vulnerable to leakage and disclosure of sensitive data.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Azure SQL Database and click on the SQL server you want to remediate \n2. Under the Settings blade, select Connection security\n3. Change Deny Public Network Access to Yes","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-061":{"article":"The TLS (Transport Layer Security) protocol secures transmission of data over the internet using standard encryption technology. Encryption should be set with the latest version of TLS. App service allows TLS 1.2 by default, which is the recommended TLS level by industry standards, such as PCI DSS.","impact":"Using old versions of TLS allows an attacker to exploit well-known attacks and vulnerabilities - Padding Oracle On Downgraded Legacy Encryption (POODLE), Browser Exploit Against SSL/TLS (BEAST), Compression Ratio Info-leak Made Easy (CRIME), Browser Reconnaissance and Exfiltration via Adaptive Compression of Hypertext (BREACH), etc.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Login to Azure Portal using https//portal.azure.com\n2. Go to App Services\n3. Click each App\n4. Under the Settings section, click on SSL settings\n5. Set Minimum TLS Version to '1.2' under the Protocol Settings section","multiregional":true,"service":"App Service"},"ecc-azure-004":{"article":"Enable automatic provisioning of the monitoring agent to collect security data.","impact":"Microsoft Monitoring Agent scans for various security-related configurations and events such as system updates, OS vulnerabilities, endpoint protection, and provides alerts.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Click on Environment Settings\n3. Click on a subscription\n4. Click on Auto Provisioning in the left column.\n5. Ensure that Log Analytics agent for Azure VMs is set to On","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-130":{"article":"This policy identifies network security group rules that allow inbound traffic to the SMTP port (25) from the public internet.\nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to port 25 can increase opportunities for malicious activities \nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '25' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-373":{"article":"Create an activity log alert for the Create or Update Public IP Addresses rule.","impact":"Monitoring of changes for 'Public IP Address events' can reduce the time it takes to detect unsolicited changes.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select Policy Assignment under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription resource from the entries populated under Resource\n9. Verify Selection preview shows All Policy assignment (policyAssignments) and your selected subscription name\n10. Click Done\n11. Under Condition, click Add Condition\n12. Select the Create or Update Public IP Address signal\n13. Click Done\n14. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n15. Under Alert rule details, enter Alert rule name and Description\n16. Select the appropriate resource group to save the alert to\n17. Check Enable alert rule upon creation checkbox\n18. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-238":{"article":"Cross-Origin Resource Sharing (CORS) should not allow all domains to access your Web app. Allow only required domains to interact with your Web app.","impact":"Weak Cross-origin resource sharing mechanism which allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served can cause increasing of risk to be attacked with CSRF attack.","report_fields":["id"],"remediation":"From Azure Console:\n1. Navigate to your App Service\n2. Select CORS\n3. Set allowed domains in Allowed Origins\n4. Click Save","multiregional":true,"service":"App Service"},"ecc-azure-002":{"article":"Classic subscription admin roles offer basic access management and include Account Administrator, Service Administrator, and Co-Administrators. It is recommended the least necessary permissions be given initially. Permissions can be added as needed by the account holder. This ensures the account holder cannot perform actions which were not intended.","impact":"Compromised users assigned a custom role with 'Owner' permissions can cause significant harm to cloud resources by escalating and changing privileges within the entire subscription scope.","report_fields":["id"],"remediation":"Using Azure CLI:\n1. az role definition list \n2. Check for entries with assignableScope of / or a subscription, and an action of * \n3. Verify the usage and impact of removing the role identified\n4. az role definition delete --name \"rolename\" \nBy default, no custom owner roles are created.","multiregional":true,"service":"Azure RBAC"},"ecc-azure-199":{"article":"Audit enabling of only connections via SSL to Azure Cache for Redis. \nUse of secure connections ensures authentication between the server and the service\nand protects data in transit from network layer attacks such as man-in-the-middle, eavesdropping, and session-hijacking","impact":"Unencrypted connection to your Redis Cache increases the possibility of MITM attacks,\nwhich means the data transmitted to your cache can be easily read and intercepted by an attacker.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Cache for Redis \n2. Under Settings, select Advanced settings \n3. For Allow access via SSL, select Yes \n4. Click Save","multiregional":true,"service":"Azure Cache for Redis"},"ecc-azure-298":{"article":"Application Service Logging gathers STDOUT (commands normal output) and STDERR (error messages) output from the container. The output logs are saved via FTP/FTPS and can be viewed at the configured endpoint.","impact":"Lack of logging can make it harder to find a problem when it occurs","report_fields":["id"],"remediation":"Using Azure Portal:\n1. Go to `Function App` service.\n2. Choose your containerized function app.\n3. Under `Monitoring` go to `App service logs`.\n4. Make sure to Enable this feature.","multiregional":true,"service":"App Service"},"ecc-azure-046":{"article":"Create an activity log alert for the Create or Update or Delete SQL Server Firewall Rule event.","impact":"Monitoring for 'Create' or 'Update' or 'Delete SQL Server Firewall Rule' events gives insight into network access changes and may reduce the time it takes to detect suspicious activity.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Monitor\n2. Select Alerts\n3. Click New Alert Rule\n4. Under Scope, click Select resource\n5. Select the appropriate subscription under Filter by subscription\n6. Select SQL servers under Filter by resource type\n7. Select All for Filter by location\n8. Click on the subscription from the entries populated under Resource\n9. Verify Selection preview shows SQL servers and your selected subscription name\n10. Under Condition click Add Condition\n11. Select the All Administrative operations signal\n12. Click Done\n13. Under Action group, select Add action groups and complete creation process or select the appropriate action group\n14. Under Alert rule details, enter the Alert rule name and Description\n15. Select the appropriate resource group to save the alert to\n16. Check Enable alert rule upon creation checkbox\n17. Click Create alert rule","multiregional":true,"service":"Azure Subscription"},"ecc-azure-217":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Data Lake Storage and select appropriate service \n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Azure Data Lake Storage"},"ecc-azure-291":{"article":"Identify Storage Accounts outside of the following regions: northeurope, westeurope","impact":"GDPR requires Europe only regions (supported by Azure).","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nPlease refer to Azure documentations about Storage Accounts:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account\nand\nAzure documentations about Regions:\nhttps://azure.microsoft.com/en-us/global-infrastructure/regions/#services","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-049":{"article":"Disable an SSH access on network security groups from the Internet.","impact":"Publicly exposed SSH access can increase opportunities for malicious activities such as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks raising the risk of resource compromising.","report_fields":["id"],"remediation":"Disable a direct SSH access to your Azure Virtual Machines from the Internet. After the direct SSH access from the Internet is disabled, you have other options you can use to access these virtual machines for remote management - Point-to-site VPN - Site-to-site VPN - ExpressRoute","multiregional":true,"service":"Network security groups"},"ecc-azure-313":{"article":"The log_min_messages setting the message levels that are written to the server log. Each level includes all the levels that follow it. The lower the level (vertically, below), the fewer messages are sent.","impact":"If this is not set to the correct value, too many messages or too few messages may be written to the server log","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Database for PostgreSQL single server\n2. Choose your server\n3. Under Settings go to Server Parameters\n4. Set 'log_min_error_statement' to approptiate value (DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, PANIC)\n5. Save changes","multiregional":true,"service":"Azure Database for PostgreSQL"},"ecc-azure-204":{"article":"Customer-managed keys are commonly required to meet regulatory compliance standards. Customer-managed keys enable the data stored in Cognitive Services to be encrypted with an Azure Key Vault key created and owned by you. You have full control and responsibility for the key lifecycle, including rotation and management. Learn more about customer-managed keys at https://go.microsoft.com/fwlink/?linkid=2121321.","impact":"Unencrypted Cognitive Services accounts with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nNote: For some cognitive services, you need to contact Microsoft support to enable CMK encryption at the subscription level.\n\nMore information available at: https://docs.microsoft.com/en-us/azure/cognitive-services/encryption/cognitive-services-encryption-keys-portal\nFrom Azure Console:\n1. Create a key vault with soft delete and purge protection enabled and key to encrypt Cognitive Service account\nNote: You need managed identity precreated for Cognitive account to setup proper Access Policy\n2. Go to Cognitive service -> All Cognitive Services and select the appropriate servuce\n3. Under Resource Management select Encryption\n4. Select Customer Managed Keys and fill data about encryption key\n5. Click Save.","multiregional":true,"service":"Cognitive Services"},"ecc-azure-279":{"article":"Disabling local authentication methods improves security by ensuring that Azure Kubernetes Service Clusters should exclusively require Azure Active Directory identities for authentication. \nLearn more at: https://aka.ms/aks-disable-local-accounts.","impact":"Anyone can authenticate using local account to Kubernetes cluster and also local accounts require more administrative effort to ensure secure auth.","report_fields":["id"],"remediation":"To disable local accounts for AKS, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/aks/managed-aad","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-235":{"article":"Azure Policy Add-on for Kubernetes service (AKS) extends Gatekeeper v3, an admission controller webhook for Open Policy Agent (OPA), \nto apply at-scale enforcements and safeguards on your clusters in a centralized, consistent manner.","impact":"To improve the security of your Azure Kubernetes Service (AKS) cluster, you can apply and enforce built-in security policies on your cluster using Azure Policy.","report_fields":["id"],"remediation":"To install Azure Policy Add-on for AKS, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes?WT.mc_id=Portal-Microsoft_Azure_Security#install-azure-policy-add-on-for-aks","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-178":{"article":"Deploy Azure Web Application Firewall (WAF) in front of public facing web applications for additional inspection of incoming traffic.\nWeb Application Firewall (WAF) provides centralized protection of your web applications from common exploits\nand vulnerabilities such as SQL injections, Cross-Site Scripting, local and remote file executions. \nYou can also restrict access to your web applications by countries IP address ranges, \nand other http(s) parameters via custom rules.","impact":"Unrestricted and unfiltered network access to the Front door service can lead to DDoS attacks or data leakage, \nresulting in unstable operation of cloud resources.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Web Application Firewall policies (WAF) \n2. Fill the Basics tab and configure:     \n   - Policy for Global WAF (Front Door)    \n   - Policy state Enabled    \n   - Customize Azure Web Application Firewall as required\n3. Select Front Door in the search bar and then select the service that does not have Azure Web Application Firewall. \n4. From the left sidebar, select Web application firewall. \n5. Select the frontend to which you're adding the Azure Web Application Firewall policy. Select Apply policy \n6. From the dropdown, select Azure Web Application Firewall policy. Select Add \n7. Click Save","multiregional":true,"service":"Azure Front Door"},"ecc-azure-172":{"article":"Private endpoint connections enforce secure communication by enabling private connectivity to Azure Database for MySQL. \nConfigure a private endpoint connection to enable access to traffic coming only from known networks \nand prevent access from all other IP addresses, including within Azure.","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your MySQL service.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to MySQL and select the instance you want to remediate\n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-226":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Service Bus and select appropriate service\n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Service Bus"},"ecc-azure-324":{"article":"Enabling double encryption helps protect and safeguard your data to meet your organizational security and compliance commitments.","impact":"When double encryption has been disabled, data in the storage account is unencrypted which can lead to data exposure.","report_fields":["id"],"remediation":"Follow this guide to enable double encryption for Azure Kusto cluster:\nhttps://docs.microsoft.com/en-us/azure/data-explorer/cluster-encryption-double","multiregional":true,"service":"Azure Data Explorer"},"ecc-azure-143":{"article":"Azure Virtual Network deployment provides enhanced security, isolation and allows you to place your API Management service in a non-internet routable network that you control access to. \nThese networks can then be connected to your on-premises networks using various VPN technologies. \nIt enables access to your backend services within the network and/or on-premises. \nThe developer portal and API gateway can be configured to be accessible either from the Internet or only within the virtual network.","impact":"API management service can be accessed directly from the internet if the virtual network type is not configured with 'external' or 'internal' mode. \nUnsecured publicly accessible API services can be vulnerable to attacks such as injection of malicious code, sensitive data exposure, parameter tampering, etc.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to API Management services and select your service\n2. Under the Properties blade, select Virtual Network\n3. Under the Virtual network blade, select 'External' or 'Internal' (depends on your needs) and specify:\n   - Virtual Network   \n   - Subnet   \n   - Management public IP address\n4. Click Save","multiregional":true,"service":"API Management"},"ecc-azure-280":{"article":"Enable the private cluster feature for your Azure Kubernetes Service cluster to ensure network traffic between your API server and your node pools remains on the private network only. This is a common requirement in many regulatory and industry compliance standards.","impact":"Publicly exposed network traffic between API server and node pool in a Kubernetes cluster can lead to high risk of networks attacks such as DoS assaults","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nTo enable Azure Kubernetes Service Private Clusters, follow this guide:\nhttps://docs.microsoft.com/en-us/azure/aks/private-clusters.","multiregional":true,"service":"Azure Kubernetes Service"},"ecc-azure-369":{"article":"Enabling double encryption at the hardware level on top of the default software encryption for Storage Accounts accessing Azure storage solutions.","impact":"The read and write speeds to the storage will be impacted if both default encryption and Infrastructure Encryption are checked, as a secondary form of encryption requires more resource overhead for the cryptography of information. This performance impact should be considered in an analysis for justifying use of the feature in your environment. Customer-managed keys are recommended for the most secure implementation, leading to overhead of key management. The key will also need to be backed up in a secure location, as loss of the key will mean loss of the information in the storage.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Azure Portal:\n  1. When creating a storage account, proceed as normal, but stop on the 'Advanced' tab.\n  2. Select 'Enabled' next to Infrastructure Encryption.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-100":{"article":"Turning on Azure Defender enables threat detection for Container Registries, providing threat intelligence, anomaly detection, and behavior analytics in the Azure Security Center.","impact":"Insufficient monitoring of your Container Registries can lead to a lack of security assessments and threat detection activities.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Microsoft Defender for Cloud\n2. Select Environment Settings blade\n3. Click on the subscription name\n4. Select the Defender plans blade\n5. On the line in the table for Container Registries Select On under Plan.\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-218":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes;\nwhen a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Azure Stream Analytics and select appropriate service\n2. Under Diagnostic settings create new diagnostic settings.\n3. Select one of the options to store the diagnostics logs\n4. Click Save","multiregional":true,"service":"Azure Stream Analytics"},"ecc-azure-368":{"article":"On September 14, 2021, Microsoft released fixes for three Elevation of Privilege (EoP) vulnerabilities and one unauthenticated Remote Code Execution (RCE) vulnerability in the Open Management Infrastructure (OMI) framework:  CVE-2021-38645, CVE-2021-38649, CVE-2021-38648, and CVE-2021-38647, respectively.  Open Management Infrastructure (OMI) is an open-source Web-Based Enterprise Management (WBEM) implementation for managing Linux and UNIX systems. Several Azure Virtual Machine (VM) management extensions use this framework to orchestrate configuration management and log collection on Linux VMs.","impact":"Log Analytics Agent v1.13.39 or less bundled with vulnerable OMI version. It can be point of Local Elevation Privileges on affected VM Scale Set instances.\nLog Analytics Agent v.1.13.40 considered as safe and requires additional manual check.","report_fields":["id"],"remediation":"Follow this article to detect and remediate OMI vulnerability:\nhttps://techcommunity.microsoft.com/t5/azure-observability-blog/detecting-and-updating-agents-using-the-omi-vulnerability/ba-p/2795462","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-103":{"article":"This setting enables Microsoft Cloud App Security (MCAS) integration with Security Center. Security Center offers an additional layer of protection by using Azure Resource Manager events, which is considered to be the control plane for Azure.\nBy analyzing the Azure Resource Manager records, Security Center detects unusual or potentially harmful operations in the Azure subscription environment.\nSeveral of the preceding analytics are powered by Microsoft Cloud App Security. To benefit from these analytics, subscription must have a Cloud\nApp Security license. MCAS works only with Standard Tier subscriptions.","impact":"Several of the preceding analytics are powered by Microsoft Cloud App Security.\nTo benefit from these analytics, a subscription must have a Cloud App Security license.","report_fields":["id"],"remediation":"From Azure Console\n1. Go to Microsoft Defender for Cloud\n2. Select Security policy blade\n3. Click On Edit Settings to alter the the security policy for a subscription\n4. Select the Integrations blade\n5. Check/Enable option Allow Microsoft Defender for Cloud Apps to access my data\n6. Select Save","multiregional":true,"service":"Microsoft Defender for Cloud"},"ecc-azure-174":{"article":"Azure Private Link allows you to connect your virtual network to Azure services without a public IP address at the source or destination. The Private Link platform handles the connectivity between the consumer and services over the Azure backbone network. By mapping private endpoints to your storage account, data leakage risks are reduced. Learn more about private links at https://aka.ms/azureprivatelinkoverview","impact":"Unconfigured Private Link with private endpoints enabled can lead to unauthorized access to your Storage account service.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Storage accounts and select the one you want to remediate. \n2. Under Settings, select Private endpoint connections \n3. Click Add and configure the private endpoint.","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-347":{"article":"Use customer-managed keys to manage the encryption at rest of your MySQL servers. \nBy default, the data is encrypted at rest with service-managed keys,\nbut customer-managed keys are commonly required to meet regulatory compliance standards. \nCustomer-managed keys enable the data to be encrypted with an Azure Key Vault key created and owned by you. \nYou have full control and responsibility for the key lifecycle, including rotation and management.","impact":"Unencrypted MySQL instances with CMK are vulnerable to data exposure.","report_fields":["id"],"remediation":"From Azure Console:\n1. Go to Key Vaults and create a key vault with soft delete and purge protection enabled \n2. Under the Keys generate your own encryption key. \n3. Under the Access polices grant the Azure Database for MySQL service permissions to the key vault with  get, wrapKey, unwrapKey permissions\n4. Go to Azure Database for MySQL and under Data encryption fill the key vault and key information. Click Save.","multiregional":true,"service":"Azure Database for MySQL"},"ecc-azure-037":{"article":"The storage account with the activity log export container is configured to use BYOK (Use Your Own Key).","impact":"An unprotected storage account with the activity log container can lead to compromising log records for the entire subscription. An account configured to use BYOK (Use Your Own Key) provides additional confidentiality controls on log data.","report_fields":["id"],"remediation":"From Azure Portal:\n1. In right column, click Storage Accounts to access Storage account blade\n2. Click the storage account name\n3. In the Settings section, click Encryption. It will show the Storage service encryption configuration pane.\n4. Check Use your own key which will expand Encryption Key Settings\n5. Use option Enter key URI or Select from Key Vault to set up encryption with your own key","multiregional":true,"service":"Azure Storage Accounts"},"ecc-azure-222":{"article":"Audit enabling of resource logs. This enables you to recreate activity trails to use for investigation purposes; \nwhen a security incident occurs or when your network is compromised.","impact":"Insufficient logging of what operations were taken on a resource affects the effectiveness of incident management.","report_fields":["id"],"remediation":"From Azure Console \n1. Go to IoT Hub and select appropriate service \n2. Under Diagnostic settings create new diagnostic settings. \n3. Select one of the options to store the diagnostics logs \n4. Click Save","multiregional":true,"service":"Azure IoT Hub"},"ecc-azure-131":{"article":"This policy identifies network security group rules that allow inbound traffic to the Telnet port (23) from the public internet. \nAllowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to port 23 can increase opportunities for malicious activities\nsuch as hacking and denial-of-service (DoS) attacks and lead to data loss.","report_fields":["id"],"remediation":"From Azure Console: \n1. Go to Network Security Groups \n2. Create an NSG or select the existing NSG assigned to a network interface or subnet where a VM resides. \n3. On the Settings plane, select Inbound rules \n4. Create Inbound rule:\n   - Source 'Any'\n   - Source port ranges '*'\n   - Destination 'Any'\n   - Service 'Custom' \n   - Destination port ranges '23' \n   - Protocol 'TCP' \n   - Action 'Deny' \n   - Priority {define high priority}\n   Click Add \n5. Check Network interfaces or Subnets assignments: \n  - On the Settings plane, select Network interfaces \n  - On the Settings plane, select Subnets","multiregional":true,"service":"Network security groups"},"ecc-azure-296":{"article":"Avoid using names like 'Admin' for an Azure SQL Server admin account login","impact":"Using \"admin\" like names for Admin account increasing the risk of Admin account compromization in some cases","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\n\nThis is set when the SQL Server is created. After created, only using external tools connected to the SQL Server, such as SSMS, can be used to alter the user, but it will not change the value in the Portal.","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-336":{"article":"Use encryption at host to get end-to-end encryption for your virtual machine and virtual machine scale set data. Encryption at host enables encryption at rest for your temporary disk and OS/data disk caches. Temporary and ephemeral OS disks are encrypted with platform-managed keys when encryption at host is enabled. OS/data disk caches are encrypted at rest with either customer-managed or platform-managed key, depending on the encryption type selected on the disk.","impact":"Unencrypted VMSS OS/data disks caches are vulnerable to the data exposure.","report_fields":["id"],"remediation":"Follow Microsoft documentation to enabled end-to-end encryption at host:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/disks-enable-host-based-encryption-portal","multiregional":true,"service":"Azure Virtual Machine Scale Sets"},"ecc-azure-013":{"article":"Enable auditing on SQL Servers.","impact":"Lack of auditing and logging can lead to insufficient response time to threats from an attacker, which can result in data loss and degradation of the service.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to SQL servers\n2. For each server instance\n3. Click Auditing\n4. Set Auditing to On","multiregional":true,"service":"Azure SQL Database"},"ecc-azure-008":{"article":"Enable data encryption in transit.","impact":"When a connection is unprotected, data in transit can be easily intercepted by an attacker. With secure transfer enabled, a call to an Azure Storage REST API operation is made over HTTPS. Any request made over HTTP will be rejected.","report_fields":["id"],"remediation":"From Azure Portal:\n1. Go to Storage Accounts\n2. For each storage account,\ngo to Configuration\n3. Set Secure transfer required to Enabled \nFrom Azure Command Line Interface 2.0:\nUse the below command to enable Secure transfer required for a Storage Account:\naz storage account update --name <storageAccountName> --resource-group <resourceGroupName> --https-only true","multiregional":true,"service":"Azure Storage Accounts"},"ecc-gcp-246":{"article":"This rule detects when a function doesn't contain a VPC connector. VPC connectors helps the function connect to resourced inside a VPC on the same project.","impact":"Not using a VPC connector can increase the opportunity for malicious activities such as unauthorized access, brute-force attacks that increase the risk of compromising resources from the Internet.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a function.\n3. Click on edit.\n4. Click on Runtime, build, connections and security settings.\n5. Click on Connections.\n6. On Egress Settings select vpc connector.\n7. Click on next.\n8. Click on Deploy.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-401":{"article":"It is recommended to set the 'password_reuse_interval' database flag for Cloud SQL Mysql instance.","impact":"Using of 'password_reuse_interval' database flag allow to establish password-reuse policy and prevent using the old passwords.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the Mysql instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'password_reuse_interval' flag from the drop-down menu, and set its value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the 'password_history' and 'password_reuse_interval' database flags for every Cloud SQL Mysql database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags password_reuse_interval={number of days that must pass before the password can be reused}\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-206":{"article":"The 'log_statement_stats' flag controls the inclusion of end-to-end performance statistics of a SQL query in the PostgreSQL logs for each query. This cannot be enabled with other module statistics (log_parser_stats, log_planner_stats, log_executor_stats).","impact":"The 'log_statement_stats' flag enables a crude profiling method for logging end-to-end performance statistics of a SQL query. This can be useful for troubleshooting, but may increase the amount of logs significantly and have performance overhead.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_statement_stats' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_statement_stats database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_statement_stats=off\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-065":{"article":"Use the Key Management Service to manage secrets. KMS allows you to create, import, and manage cryptographic keys and perform cryptographic operations in a single centralized cloud service.","impact":"Not using a Key Management Service to manage KMS secrets compromises the security of your project.","report_fields":["name"],"remediation":"From Console:\n1. Go to Security.\n2. Go to Cryptographic Keys.\n3. Create secrets using the link https://cloud.google.com/kms/docs/creating-keys#kms-create-keyring-console.","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-272":{"article":"GCP firewall logging helps to inspect and understand the system access on specific ports. Firewall logging is not enabled by default and must be configured by the user.","impact":"Lack of logging can lead to failure to detect the records with information about configured firewall rules that allow malicious activities.","report_fields":["selfLink"],"remediation":"From Console:\n1. From the Google Cloud Console, go to VM firewall.\n2. Select firewall for which logging is to be enabled.\n3. Select EDIT.\n4. In Logs section, turn on firewall logs.\n5. Select SAVE.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-262":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned a service-level IAM role in the Cloud Run revision. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. For the revision of a service, click on the service, then click Edit and Deploy New Revision.\n3. Under Container tab, Click the Service account dropdown and select the desired service account.\n4. Select Serve this revision immediately.\n5. Click on Deploy.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-310":{"article":"This rule detects when redis instance has in-transit encryption disabled.\nMemorystore for Redis supports encrypting all Redis traffic using the Transport Layer Security (TLS) protocol. When in-transit encryption is enabled Redis clients communicate exclusively across a secure port connection. Redis clients that are not configured for TLS will be blocked. If you choose to enable in-transit encryption you are responsible for ensuring that your Redis client is capable of using the TLS protocol.","impact":"Without enabled in-transit encryption, Memorystore for Redis accepts a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Memorystore for Redis.\n3. Select Enable in-transit encryption when Creating a Redis instance.\n4. Download the Certificate Authority and Installing a Certificate Authority on your client: https://cloud.google.com/memorystore/docs/redis/enabling-in-transit-encryption#downloading_the_certificate_authority","multiregional":true,"service":"Cloud Memorystore"},"ecc-gcp-202":{"article":"PostgreSQL logs only the IP address of the connecting hosts. The 'log_hostname' flag controls the logging of hostnames in addition to the IP addresses logged. The performance hit is dependent on the configuration of the environment and the host name resolution setup. This parameter can only be set in the 'postgresql.conf' file or on the server command line.","impact":"Logging hostnames can incur overhead on server performance as for each statement logged, DNS resolution will be required to convert IP address to hostname. Depending on the setup, this may be non-negligible. Additionally, the IP addresses that are logged can be resolved to their DNS names later when reviewing the logs excluding the cases where dynamic hostnames are used.","report_fields":["selfLink"],"remediation":"1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_hostname' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\n1. Configure the log_hostname database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags log_hostname=on\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-294":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned a database-level IAM role in the Cloud Spanner service. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Spanner page at Google Cloud Console: https://console.cloud.google.com/spanner/instances.\n2. Select an instance.\n3. Select a database from Databases.\n4. Review each role and find Members having admin or owner access.\n5. Click Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Spanner"},"ecc-gcp-076":{"article":"VPC network peering enables you to connect virtual networks in the same region. Once peered, the virtual networks are still managed as separate resources.\nWhen a peering configuration is deleted on one virtual network, the other VPC network will report that peering is disconnected. With 'Restrict VPC Peering Usage' constraint policy, you have the ability to define the VPC networks that are allowed to be peered with other networks within your project, folder, or organization, in order to enhance access security and comply with internal regulations.","impact":"By default, anyone with the right set of permissions can peer your VPC network with any other network within your organization and this can pose a major security risk if the process is not managed correctly or if someone peers your VPC network with a malicious entity.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nTo configure the peering connection, navigate to the VPC network:\n1. Click 'VPC network peering'.\n2. Delete the peering with 'INACTIVE' status.\n3. Click 'Create Peering Connection'.\n4. Check Status  Link https://cloud.google.com/vpc/docs/using-vpc-peering.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-281":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the memcached port (11211) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 11211 (memcached) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-123":{"article":"Ensure your GKE Master node version is supported. This policy checks your GKE master node version and generates an alert if the running version is unsupported.","impact":"Using an unsupported version of the master has bugs and security vulnerabilities found in a supported minor version.","report_fields":["selfLink"],"remediation":"Manually initiate a master upgrade.\n1. From Google Cloud Platform Console, go to the Google Kubernetes Engine menu.\n2. Select the desired cluster.\n3. Click the available Upgrade link next to Master version.\n4. Select the desired version, then click Change.\n5. Click the arrow at the top of the screen to go back to the cluster overview page.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-127":{"article":"Binary Authorization helps to protect supply-chain security by only allowing images with verifiable cryptographically signed metadata into the cluster.","impact":"By not applying Binary Authorization, you can not gain tighter control over your container environment by ensuring only verified images are integrated into the build-and-release process.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Binary Authorization using 'https://console.cloud.google.com/security/binary-authorization'.\n2. Enable the Binary Authorization API (if disabled).\n3. Go to Kubernetes Engine using 'https://console.cloud.google.com/kubernetes/list'.\n4. Select the Kubernetes cluster for which Binary Authorization is disabled.\n5. Click EDIT.\n6. Set 'Binary Authorization' to 'Enabled'.\n7. Click SAVE.\n8. Return to Binary Authorization at 'https://console.cloud.google.com/security/binary-authorization'.\n9. Set an appropriate policy for your cluster.\nUsing Command Line:\nThe Remediation script for this recommendation utilizes 2 variables $CLUSTER_NAME, $COMPUTE_ZONE.\nPlease set these parameters for the system where you will be executing your gcloud audit script or command.\nUpdate the cluster to enable Binary Authorization:\ngcloud container cluster update [CLUSTER_NAME] --zone [COMPUTE-ZONE] --enable-binauthz\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference ('https://cloud.google.com/binary-authorization/docs/policy-yaml-reference') for guidance.\nImport the policy file into Binary Authorization:\ngcloud container binauthz policy import [YAML_POLICY]","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-311":{"article":"This rule detects when Datafusion instance has stackdriver logging disabled.\nStackdriver logging is a fully managed service that allows you to store, search, analyze, monitor, and alert on logging data and events.","impact":"Lack of Stackdriver Datafusion logging can result in insufficient response time to detect issues.","report_fields":["name"],"remediation":"From Command Line:\n1. After you create your instance, you cannot enable Cloud Logging in the console. Instead, run this gcloud CLI command:\ngcloud beta data-fusion instances update INSTANCE_NAME --project=PROJECT_ID --location=LOCATION --enable_stackdriver_logging","multiregional":true,"service":"Cloud Data Fusion"},"ecc-gcp-214":{"article":"BigQuery by default encrypts the data at-rest by employing Envelope Encryption using Google managed cryptographic keys. The data is encrypted using data encryption keys and the data encryption keys themselves are further encrypted using key encryption keys. This is seamless and does not require any additional input from the user. However, if you want to have greater control, Customer-managed encryption keys (CMEK) can be used as the encryption key management solution for BigQuery Data Sets.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of your keys.","report_fields":["selfLink"],"remediation":"The default CMEK for existing data sets can be updated by specifying the default key in the 'EncryptionConfiguration.kmsKeyName' field when calling the 'datasets.insert' or 'datasets.patch' methods.","multiregional":true,"service":"BigQuery"},"ecc-gcp-068":{"article":"It is recommended to set an expiration date for all keys.","impact":"Expired keys can be misused or exposed during their life cycle, which can lead to potential threats to data integrity and confidentiality.","report_fields":["name"],"remediation":"From Console:\n1. Go to Security\n2. Go to Cryptographic Keys.\n3. Choose Keys and click on Edit rotation period and then select a value (that should never be) shold not be never.\n4. Choose Security policy.\n6. Click on Save.\nhttps://cloud.google.com/kms/docs/creating-keys#kms-create-keyring-console\ngcloud kms keys create [KEY_NAME] --location [LOCATION] --keyring [KEYRING_NAME] --purpose encryption --rotation-period [ROTATION_PERIOD] --next-rotation-time [NEXT_ROTATION_TIME]","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-268":{"article":"This rule detects when a Cloud Run service is publicly accessible. Remove public access and assign least privileges to GCP Cloud Run service according to requirements.","impact":"Granting permissions to allUsers or allAuthenticatedUsers allows anyone to access the run service.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. Select a service.\n3. Click on Triggers tab.\n4. Click on Show Info Panel in the top right corner to show the Permissions tab.\n5. Click on the delete button to remove the allUsers and authenticatedUsers.\n6. Click on the Add Member button to can grant Cloud Run Invoker role to required members as per the requirements.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-220":{"article":"Compute instances template should not have external IP addresses.","impact":"An instance with a public IP address could potentially be compromised and an attacker could gain access to it and other resources connected to this instance, which may lead to malicious activity with sensitive data.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the instance template page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. Replace all affected templates with new ones with External IP set to 'None' in the Network interfaces section.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-415":{"article":"Use GKE Sandbox to restrict untrusted workloads as an additional layer of protection when running in a multi-tenant environment.","impact":"Without Sandbox enabled, multi-tenant clusters and clusters whose containers run untrusted workloads are more exposed to security vulnerabilities than other clusters. The potential exists for a malicious tenant to gain access to and exfiltrate another tenant's data in memory or on disk, by exploiting such a defect.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Google Cloud Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/\n2. Click ADD NODE POOL.\n3. Configure the Node pool with following settings:\n - For the node version, select v1.12.6-gke.8 or higher.\n - For the node image, select 'Container-Optimized OS with Containerd (cos_containerd) (beta)'.\n - Under 'Security', select 'Enable sandbox with gVisor'.\n4. Configure other Node pool settings as required.\n5. Click SAVE.\nUsing Command Line:\nTo enable GKE Sandbox on an existing cluster, a new Node pool must be created.\ngcloud container node-pools create [NODE_POOL_NAME] \\\n--zone=[COMPUTE-ZONE] \\\n--cluster=[CLUSTER_NAME] \\\n--image-type=cos_containerd \\\n--sandbox type=gvisor","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-251":{"article":"This rule detects when a service account with elevated privileges (editor, viewer or owner status) is assigned in the GKE service.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["selfLink"],"remediation":"Note: Service account can only be changed by adding a new node pool and deleting the affected node pool.\nUsing Console:\n1. Go to Kubernetes GCP Console visiting https://console.cloud.google.com/kubernetes/list?\n2. o to Kubernetes Engine page at Google Cloud Console.\n3. Click ADD NODE POOL.\n4. Select Security.\n5. In Service account, select service account with least security privileges.\n6. Fill up the other details as required.\n7. Click CREATE.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-118":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the POP3 port (110) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted POP3 access to your Google Cloud virtual machine (VM) instances via VPC network firewall rules can increase opportunities for malicious activities such as brute-force attacks, spoofing, and packet capture attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-126":{"article":"Alpha clusters are not covered by an SLA and are not production-ready. They are designed for early adopters to experiment with workloads that take advantage of new features before those features are production-ready.","impact":"Alpha clusters do not receive security updates, have node auto-upgrade and node auto-repair disabled, and cannot be upgraded. They are also automatically deleted after 30 days.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/.\n2. Click CREATE CLUSTER.\n3. Unless Node Auto-Upgrade and Node Auto-Repair are disabled, the option 'Enable Kubernetes alpha features in this cluster' will not be available under 'Availability, networking, security, and additional features'. Ensure this feature is not checked.\n4. Click CREATE.\nFrom Command Line:\nThe Remediation script for this recommendation utilizes 2 variables $CLUSTER_NAME, $COMPUTE_ZONE.\nPlease set these parameters for the system where you will be executing your gcloud audit script or command. Upon creating a new cluster:\ngcloud container clusters create [CLUSTER_NAME] --zone [COMPUTE_ZONE]\nDo not use the --enable-kubernetes-alpha argument.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-122":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic from anywhere with no target filtering. The default target is all instances in the network. The use of target tags or target service accounts allows the rule to apply to the selected instances. Failure to use firewall rules that ensure target filtering may allow a bad actor to brute-force their way into the system and potentially get access to the entire network.","impact":"Failure to use firewall rules that ensure target filtering may allow a bad actor to brute-force their way into the system and potentially get access to the entire network.","report_fields":["selfLink"],"remediation":"Follow the instructions below to restrict the default target parameter (all instances in the network):\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on each reported Firewall rule.\n5. Click on Edit.\n6. Change the Targets field from 'All instances in the network' to 'Specified target tags'.\n7. Type the tags into the Target tags field.\n8. Review Source IP ranges and change to specific IP ranges if traffic is not to be allowed from anywhere.\n9. Click on Save.\nReference:\nhttps://cloud.google.com/vpc/docs/add-remove-network-tags.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-244":{"article":"This rule detects when a HTTP trigger function is configured with unauthenticated access. Remove unnecessary access and assign least privileges to GCP Cloud Functions function according to requirements.","impact":"Granting roles/cloudfunctions.invoker to 'allUsers' members can allow anyone to invoke HTTP functions with restricted access.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a function.\n3. Click on Permission.\n4. Review role Cloudfunctions Invoker and find Members having allUsers access.\n5. Click Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-324":{"article":"The User has an IAM policy containing permissions that allow privilege escalation, at the project level. The existing permissions allow the user to impersonate a service account with higher permissions than their own. The user can then utilize that service account to perform API calls that the user may not be authorized to perform.","impact":"The permission allow the user to impersonate a service account with higher permissions than their own.","report_fields":["member","roles"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/iam.\n2. Click Edit for reported user.\n3. For all non-critical members of these roles, remove their membership by clicking the Trash icon on the right.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-443":{"article":"Cloud KMS bills monthly for the number of active primary key versions which you have for the month. This means that disabled primary key versions are adding costs to your GCP bill while not being of any use. For more information, see https://cloud.google.com/kms/pricing#pricing_overview","impact":"Keeping Disabled primary key versions will inflict unnecessary monthly costs","report_fields":["name"],"remediation":"From Console\n1. Go to Cryptographic Keys using https://console.cloud.google.com/security/kms.\n2. Click on the specific key ring.\n3. From the list of keys, choose the specific key \n4. From the list of key versions, choose the specific version and click on Right side pop up the blade (3 dots).\n4. Click on Enable or Destroy.","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-409":{"article":"Patch deployment keeps VM instances at the current OS security patch level to mitigate known vulnerabilities.","impact":"Without Patch deployment you can face security holes that have been fixed in new versions.","report_fields":["selfLink"],"remediation":"Use the next instruction to schedule patch jobs for all the instances:\nhttps://cloud.google.com/compute/docs/os-patch-management/schedule-patch-jobs#schedule-patch-console","multiregional":true,"service":"Compute Engine"},"ecc-gcp-200":{"article":"Enabling the 'log_duration' setting causes the duration of each completed statement to be logged. This does not logs the text of the query and thus, behaves different from the 'log_min_duration_statement' flag. This parameter cannot be changed after the session start.","impact":"Monitoring the time taken to execute the queries can be crucial in identifying any resource hogging queries and assessing the performance of the server. Further steps such as load balancing and use of optimized queries can be taken to ensure the performance and stability of the server.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_duration' flag from the drop-down menu and set the value as 'on'.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_duration database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_duration=on\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-049":{"article":"Legacy Authorization also known as Attribute-Based Access Control (ABAC) has been superseded by Role-Based Access Control (RBAC) and is not under active development. RBAC is the recommended way to manage permissions in Kubernetes.","impact":"It increases the risk of an attacker getting elevation of privileges without configured role-based access controls.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Select Kubernetes clusters for which Legacy Authorization is enabled.\n3. Click on EDIT.\n4. Set 'Legacy Authorization' to 'Disabled'.\n5. Click on SAVE.\nUsing Command Line:\nTo disable Legacy Authorization for an existing cluster, run the following command:\ngcloud container clusters update [CLUSTER_NAME]  --zone [COMPUTE_ZONE] --no-enable-legacy-authorization","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-083":{"article":"These High Availability deployments will improve primary instance reachability by providing read replica in case of network connectivity loss or loss of availability in the primary availability zone for read/write operations.","impact":"When Cloud SQL instances are not configured for multiple Availability Zones, as well as in case of any issue with the Availability Zone availability, and during regular Cloud SQL maintenance, the data stored in a database will not be reachable.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to SQL.\n2. Select the instance for which you want to configure High-Availability.\n3. Click Edit.\n3. Under Zonal availability select Multiple zones (Highly available).\n4. Click Save.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-051":{"article":"A cluster label is a key-value pair that helps you organize your Google Cloud Platform resources, such as clusters. You can attach a label to each resource, then filter the resources based on their labels. Information about labels is forwarded to the billing system, so you can break down your billing charges by the label.","impact":"Applying labels to your Kubernetes clusters helps organize them logically. Lack of labels can make administration difficult.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to Kubernetes GCP Console visiting https://console.cloud.google.com/kubernetes/list?\n2. Select cluster from the list.\n3. Click 'Edit labels' in the 'Labels' string ('Metadata' section).\n4. Click 'Add label', set the key and value pair.\n5. Click 'Save changes'.\nUsing Command Line:\nTo configure Labels for an existing cluster, run the following command:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --update-labels [Key]=[Value]","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-167":{"article":"Dataset-level permissions determine the users, groups, and service accounts allowed to access the tables, views, and table data in a specific dataset.","impact":"Granting the individual Google account access in a specific dataset can not be controlled by organizations.","report_fields":["selfLink"],"remediation":"From Console:\n1. Select a dataset from 'Resources', then click 'Share dataset' from the right side of the window.\n2. In the 'Share dataset' panel, in the 'Dataset permissions' tab, click 'Add members'.\n3. In the 'Add members' panel, enter the entity you want to add into the 'New members' textbox. You can add any of the following entities:\n- Google account e-mail Grants an individual Google account access to the dataset\n- Google Group Grants all members of a Google group access to the dataset\n- Google Apps Domain Grants all users and groups in a Google domain access to the dataset\n- Service account Grants a service account access to the dataset\n- Anybody Enter 'allUsers' to grant access to the general public\n- All Google accounts Enter 'allAuthenticatedUsers' to grant access to any user signed in to a Google Account\n4. For 'Select a role', select 'BigQuery' and choose the appropriate pre-defined IAM role for the new members. For more information on the permissions assigned to each predefined BigQuery role, see the 'Roles' section of the access control page.\n5. Click Done.","multiregional":true,"service":"BigQuery"},"ecc-gcp-022":{"article":"It is recommended that a metric filter and alarm be established for VPC network changes.","impact":"Lack of monitoring and logging of VPC changes can lead to insufficient response time to detect accidental or intentional modifications that may lead to unauthorized network access or other security breaches.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click 'CREATE METRIC'.\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nresource.type=\"gce_network\"\nAND (protoPayload.methodName:\"compute.networks.insert\"\nOR protoPayload.methodName:\"compute.networks.patch\"\nOR protoPayload.methodName:\"compute.networks.delete\"\nOR protoPayload.methodName:\"compute.networks.removePeering\"\nOR protoPayload.methodName:\"compute.networks.addPeering\")\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.\n6. Click Create Metric.\nCreate the prescribed alert policy:\n1. Identify the newly created metric under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page appears.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of 0 for the most recent value will ensure that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notification channels in the section Notifications.\n5. Name the policy and click Save.\nFrom Google Cloud CLI:\nCreate the prescribed Log Metric:\ngcloud logging metrics create\nCreate the prescribed alert policy:\ngcloud alpha monitoring policies create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-203":{"article":"The PostgreSQL planner/optimizer is responsible for parsing and verifying the syntax of each query received by the server. If the syntax is correct, a parse tree is built up. Otherwise, an error is generated. The 'log_parser_stats' flag controls the inclusion of parser performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_parser_stats' flag enables a crude profiling method for logging parser performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_parser_stats' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_parser_stats database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_parser_stats=off\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-136":{"article":"Ensure that your Kubernetes cluster size contains 3 or more nodes. (Clusters smaller than 3 may experience downtime during upgrades.) This policy checks the size of your cluster pools and alerts if there are fewer than 3 nodes in a pool.","impact":"Clusters smaller than 3 may experience downtime during upgrades.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine using 'https://console.cloud.google.com/kubernetes/list'.\n2. Click on the name of a cluster to be upgraded and click on NODES.\n3. Click on the name of node pools to be upgraded.\n4. In the Node pools section, click on the Edit button and change the value of the Size (Number of nodes) field to the desired value.\n5. Click on SAVE.\n6. Repeat for each node pool with Enable auto-upgrade function enabled as needed.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-208":{"article":"It is recommended to set the 'external scripts enabled' database flag for Cloud SQL SQL Server instance to 'off'","impact":"The 'External Scripts Enabled' feature allows scripts external to SQL, such as files located in an R library, to be executed. This could adversely affect the security of the system. Hence, this feature should be disabled.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'external scripts enabled' flag from the drop-down menu, and set its value to 'off'.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the external scripts enabled database flag for every Cloud SQL SQL Server database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags \"external scripts enabled=off\"\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-125":{"article":"This policy identifies HTTPS Load balancers that are not using restrictive profile in their SSL Policy. A restrictive profile controls sets of features used in negotiating SSL with clients. As a best security practice, use RESTRICTED as SSL policy profile as it meets stricter compliance requirements and does not include any out-of-date SSL features.","impact":"If you do not use the RESTRICTED profile, there is a chance that the chosen protocol will be TLS 1.0, which is vulnerable to attacker-in-the-middle attacks, posing a risk to the integrity and authentication of data transmitted between the website and the browser.","report_fields":["selfLink"],"remediation":"1. Login to GCP Portal.\n2. Go to Network services (Left Panel).\n3. Select Load balancing.\n4. Click on the 'advanced menu' hyperlink to view target proxies.\n5. Click on the 'Target proxies' tab.\n6. Click on the reported HTTPS target proxy.\n7. Click on the hyperlink under 'URL map'.\n8. Click on the 'EDIT' button.\n9. Select 'Frontend configuration', Click on HTTPS protocol rule.\n10. Select the SSL policy that uses the RESTRICTED/CUSTOM profile. If no SSL policy is already present, then create a new SSL policy with RESTRICTED as Profile.\nNOTE: If you choose CUSTOM as a profile, make sure you are using profile features as restrictive as the RESTRICTED profile or more restrictive than the RESTRICTED profile.\n11. Click on 'Done'.\n12. Click on 'Update'.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-347":{"article":"This rule detects when GCP cloud spanner instance is deployed without multi-region configuration.\nMulti-region configurations allow you to replicate the database's data not just in multiple zones, but in multiple zones across multiple regions, as defined by the instance configuration. These additional replicas enable you to read data with low latency from multiple locations close to or within the regions in the configuration. There are trade-offs though, because in a multi-region configuration, the quorum (read-write) replicas are spread across more than one region. Hence, they can incur additional network latency when these replicas communicate with each other to vote on writes. In other words, multi-region configurations enable your application to achieve faster reads in more places at the cost of a small increase in write latency.","impact":"Disabled multi-region configuration for cloud spanner threaten the availability of stored data.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the Spanner page at Google Cloud Console: https://console.cloud.google.com/spanner/instances.\n2. Click Create Instance.\n3. Name your instance and allocate desired compute capacity.\n4. In 'Choose Configuration' use Multi-region configuration.","multiregional":true,"service":"Cloud Spanner"},"ecc-gcp-306":{"article":"This rule detects when Pub/Sub topic is anonymously or publicly accessible.\nPub/Sub is commonly used for asynchronous communication for applications in GCP. Messages are published to a Pub/Sub Topic and the ability to publish a message is controlled via IAM policies. It is possible to make Pub/Sub Topics publicly or anonymously accessible.","impact":"Granting permissions to allUsers or allAuthenticatedUsers allows anyone to access the Pub/Sub topic. Public notification topics can expose sensitive data and are a target for data exfiltration.","report_fields":["name"],"remediation":"From Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Topics.\n3. Select the Pub/Sub Topic checkbox next to your Topic ID.\n4. Select the INFO PANEL tab to view the topic's permissions.\n5. To remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Delete.","multiregional":true,"service":"Pub/Sub"},"ecc-gcp-044":{"article":"To minimize attack surface on a Database server instance, only trusted/known and required IP(s) should be white-listed to connect to it.\nDatabase Server should accept connections only from trusted Network(s)/IP(s) and restrict access from the world.","impact":"IPs/networks configured to 0.0.0.0/0 can allow access to the instance from anywhere in the world. It increases opportunities for malicious activities such as hacking, brute-force attacks, and DDoS attacks.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Click the instance name to open its Instance details page.\n3. Under the Configuration section, click Edit configurations.\n4. Under Configuration options, expand the Connectivity section.\n5. Click the Delete icon for the authorized network 0.0.0.0/0.\n6. Click Save to update the instance.\nFrom Command Line:\nUpdate the authorized network list by dropping off any addresses:\ngcloud sql instances patch INSTANCE_NAME --authorized networks=IP_ADDR1,IP_ADDR2...","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-314":{"article":"This rule detects when 'Access Approval' is not Enabled on the project.\nGCP Access Approval enables you to require your organizations' explicit approval whenever Google support try to access your projects. You can then select users within your organization who can approve these requests through giving them a security role in IAM. All access requests display which Google Employee requested them in an email or Pub/Sub message that you can choose to Approve. This adds an additional control and logging of who in your organization approved/denied these requests.\nBy default Access Approval and its dependency of Access Transparency are not enabled.","impact":"Without Access Approval you can not ensure that Cloud Customer Care and engineering require your explicit approval whenever they need to access your customer content.","report_fields":["projectId"],"remediation":"From Console:\n1. From the Google Cloud Home, within the project you wish to enable, click on the Navigation hamburger menu in the top left. Hover over the Security Menu. Select Access Approval in the middle of the column that opens.\n2. The status will be displayed here. On this screen, there is an option to click Enroll. If it is greyed out and you see an error bar at the top of the screen that says Access Transparency is not enabled please view the corresponding reference within this section to enable it. Enabling Access Transparency: https://cloud.google.com/cloud-provider-access-management/access-transparency/docs/enable#requirements.\n3. In the second screen click Enroll.\nGrant an IAM Group or User the role with permissions to Add Users to be Access Approval message Recipients:\n1. From the Google Cloud Home, within the project you wish to enable, click on the Navigation hamburger menu in the top left. Hover over the IAM and Admin. Select IAM in the middle of the column that opens.\n2. Click the blue button the says +add at the top of the screen.\n3. In the principals field, select a user or group by typing in their associated email address.\n4. Click on the role field to expand it. In the filter field enter Access Approval Approver and select it.\n5. Click save.\nAdd a Group or User as an Approver for Access Approval Requests:\n1. As a user with the Access Approval Approver permission, within the project where you wish to add an email address to which request will be sent, click on the Navigation hamburger menu in the top left. Hover over the Security Menu. Select Access Approval in the middle of the column that opens.\n2. Click Manage Settings.\n3. Under Set up approval notifications, enter the email address associated with a Google Cloud User or Group you wish to send Access Approval requests to. All future access approvals will be sent as emails to this address.\nFrom Command Line:\n1. To update all services in an entire project, run the following command from an account that has permissions as an 'Approver for Access Approval Requests'\ngcloud access-approval settings update --project=<project name> --enrolled_services=all --notification_emails='<email recipient for access approval requests>@<domain name>'","multiregional":true,"service":"Access Approval"},"ecc-gcp-260":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned a table-level IAM role in the Cloud Bigtable service.","impact":"An elevated service account has more access than necessary. Without Separation of duties any individual can have all the necessary permissions to perform a malicious action.","report_fields":["selfLink"],"remediation":"1. Go to the Bigtable page at Google Cloud Console.\n2. Select an instance.\n3. Click on Tables.\n4. Select a table.\n5. Review each role and find Members having Editor or Owner access.\n6. Click Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Bigtable"},"ecc-gcp-342":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the Hadoop/HDFS port (8020) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 8020 (Hadoop/HDFS) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-237":{"article":"Ensure that your Google Cloud PostgreSQL database instances are using the latest major version of PostgreSQL database in order to receive new or enhanced features and the most recent security fixes.","impact":"If your PostgreSQL database instances are not using the latest major version of the PostgreSQL database, you cannot benefit from security updates.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nPostgreSQL database version cannot be automatically upgraded within Google Cloud Platform (GCP).\nTo upgrade your Google Cloud PostgreSQL instances to the latest major version of the PostgreSQL database, you have to re-create the existing instance, export data from the existing (source) instance, and importing that data into a new (target) instance running the latest major version of PostgreSQL.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-072":{"article":"Firewall rules  provide stateful filtering of ingress/egress network traffic to GCP resources. It is recommended that no rules allow unrestricted egress access","impact":"Allowing unrestricted egress access can increase opportunities for malicious activities such as Denial of Service (DoS) attacks or Distributed Denial of Service (DDoS) attacks.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to VPC Network.\n2. Go to the Firewall Rules.\n3. Click the Firewall Rule you want to modify.\n4. Click Edit.\n5. Modify Source IP ranges to specific IP.\n6. Click Save.\nVia CLI gcloud:\n1.Update Firewall rule with new SOURCE_RANGE from below command:\ngcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...]l","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-256":{"article":"For App Engine, the App Engine firewall only applies to incoming traffic routed to your app or service.\nThis policy identifies App Engine Firewall rules that allow access from any IP address.\nBy default, any request that does not match a rule is allowed access to your app. If you need to block all requests that do not match a specific rule (excluding requests from internal services allowed by default, change the default rule's action to deny. To improve security, remove any rules that allow public access and create an individual rule for each IP that should have access to the application. For more information https://cloud.google.com/appengine/docs/standard/python/understanding-firewalls.","impact":"Allowing access from any IP address may increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["description"],"remediation":"From Console:\n1. Login to GCP Console.\n2. Go to the App Engine page at Google Cloud Console.\n3. Click on Firewall Rules.\n4. Click on the firewall rule that allows ingress access to all ('0.0.0.0/0', '0.0.0.0', '*', '::/0').\n5. Click on Delete and change action of the default rule to Deny.\n6. Create a rule for each IP or subnetwork that should have access to the application","multiregional":true,"service":"App Engine"},"ecc-gcp-054":{"article":"Node auto-upgrade keeps nodes at the current Kubernetes and OS security patch level to mitigate known vulnerabilities.","impact":"Without Node auto-upgrade, you can face security holes that have been fixed in new versions.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Select Kubernetes clusters for which node auto-upgrade is disabled.\n3. Click on the name of the Node pool that requires node auto-upgrade to be enabled.\n4. Within the Node pool details pane, click EDIT.\n5. Under the 'Management' heading, ensure the 'Enable auto-upgrade' box is checked.\n6. Click SAVE.\nUsing Command Line:\nTo enable node auto-upgrade for an existing cluster's Node pool, run the following command:\ngcloud container node-pools update [NODE_POOL] --cluster [CLUSTER_NAME] --zone [COMPUTE_ZONE] --enable-autoupgrade","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-230":{"article":"The Automatic Restart feature configures the virtual machine restart behavior when an instance crashes or it is terminated by the system. When the feature is enabled, Google Cloud Compute Engine restarts the instance if this crashes or it is terminated. This behavior does not affect any terminations initiated by the user, for example, when the instance is taken offline through a user action, such as calling sudo shutdown.","impact":"It can reduce reliability if the Cloud Engine service does not restart automatically your VM instances when they are terminated due to non-user initiated reasons, such as maintenance events, hardware, and software failures.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances.\n2. In the navigation panel, select VM instances to access the list with all the Compute Engine instances provisioned forthe selected project.\n3. Click on the name of the virtual machine (VM) instance that you want to reconfigure.\n4. On the selected resource configuration page, click EDIT to enter the instance edit mode.\n5. In the Availability policies section, turn on  Automatic restart.\n6. Click Save to apply the configuration changes.\nFrom Command Line:\n1. Run compute instances set-scheduling command using the name of the instance that you want to reconfigure as identifier parameter to enable automatic restart for the selected Google Cloud VM instance. Once this setting is enabled, the selected instance will be restarted if crashes or it is terminated by the Compute Engine service:\ngcloud compute instances set-scheduling INSTANCE_NAME --zone ZONE --restart-on-failure\n2. The command output should return the URL of the reconfigured virtual machine instance:\nUpdated [https://www.googleapis.com/compute/v1/projects/PROJECT/zones/ZONE/instances/INSTANCE_NAME].","multiregional":true,"service":"Compute Engine"},"ecc-gcp-277":{"article":"Sender Policy Framework (SPF) records are used to authorize mail servers to send mail on behalf of the registered domain. It is recommended to add SPF information in TXT type record.","impact":"The absence of this records can give attacker the opportunity of email spoofing.","report_fields":["name","id"],"remediation":"Add TXT type record in Cloud DNS managed zone with Sender Policy Framework (SPF) information.\nFrom Console:\n1. Go to Cloud DNS page https://console.cloud.google.com.\n2. Click the name of the managed zone you want to add the record to.\n3. On the Zone details screen, click Add record set.\n4. On the Create record set screen, in the DNS Name field, enter the subdomain of the DNS zone.\n5. Select the Resource record type as TXT.\n6. In the TTL field, enter a numeric value for the resource record's time to live, which is the amount of time it can be cached. This value must be a positive integer. From the TTL Unit dropdown menu, select the unit of time.\n7. Depending on the resource record type you have selected, populate the remaining fields as required in the form.\n8. Enter SPF information, starting with v=spf1.\n9. Click Create.","multiregional":true,"service":"Cloud DNS"},"ecc-gcp-008":{"article":"Google Cloud Key Management Service stores cryptographic keys in a hierarchical structure designed for useful and elegant access control management.\nThe format for the rotation schedule depends on the client library that is used. For the gcloud command-line tool, the next rotation time must be in  ISO or RFC3339 format, and the rotation period must be in the form INTEGER[UNIT], where units can be one of seconds (s), minutes (m), hours (h) or days (d).","impact":"A collection of files could be encrypted with the same key, and people with decrypt permissions on that key would be able to decrypt those files. Therefore, it's necessary to make sure that the rotation period is set to a specific time.","report_fields":["name"],"remediation":"From Console\n1. Go to Cryptographic Keys using https://console.cloud.google.com/security/kms.\n2. Click on the specific key ring.\n3. From the list of keys, choose the specific key and click on Right side pop up the blade (3 dots).\n4. Click on Edit rotation period.\n5. In the pop-up window, Select a new rotation period in days which should be less than 90 and then choose Starting on date (date from which the rotation period begins).\nFrom Command Line:\n1. Update and schedule rotation by running  a command with the ROTATION_PERIOD and NEXT_ROTATION_TIME parameters set for each key:\ngcloud kms keys update new --keyring=KEY_RING --location=LOCATION --nextrotation-time=NEXT_ROTATION_TIME --rotation-period=ROTATION_PERIOD","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-250":{"article":"GKE container scanning service scans images stored in Google Container Registry for vulnerabilities.","impact":"Without GKE container scanning service weaknesses that can either cause an accidental system failure or be intentionally exploited can be missed.","report_fields":["projectId"],"remediation":"Enable container scanning API:\n1. Go to GCR page at Google Cloud Console https://console.cloud.google.com/gcr.\n2. Select Settings.\n3. Click Enable Vulnerability Scanning.\nUsing Command Line:\ngcloud services enable containerscanning.googleapis.com","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-304":{"article":"This rule detects when Vertex AI Workbench has public IPs assigned.","impact":"Vertex AI Workbenches with public IPs assigned can increase your attack surface and expose sensitive data.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Vertex AI Workbench.\n3. Scroll down to the Networking section and expand.\n4.  Locate the External IP dropdown and select None.","multiregional":true,"service":"Vertex AI Workbench"},"ecc-gcp-152":{"article":"VM instance does not have any Custom metadata. Custom metadata can be used for easy identification and search.","impact":"VM Instances without any Custom metadata make identification and search difficult.","report_fields":["selfLink"],"remediation":"1. Login to GCP Console and select 'Compute Engine' from 'Compute'.\n2. Select the identified VM instance to see the details.\n3. In the details page, click on Edit and navigate to the Custom metadata section.\n4. Add the appropriate KeyValue information and save.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-252":{"article":"This policy identifies GCP Kubernetes Engine clusters that are not using Release Channel for version management. The Regular release channel upgrades every few weeks and is for production users who need features not yet offered in the Stable channel. These versions have passed internal validation, but don't have enough historical data to guarantee their stability. Known issues generally have known workarounds. The Stable release channel upgrades every few months and is for production users who need stability above all else, and for whom frequent upgrades are too risky. These versions have passed internal validation and have been shown to be stable and reliable in production, based on the observed performance of those clusters.","impact":"Without a specific release channel, the complexity of version control increases, leading to the use of an unsupported version of the master version with bugs and security vulnerabilities.","report_fields":["selfLink"],"remediation":"From Console:\n1. Navigate to service 'Kubernetes Engine'\n2. From the list of available clusters, select the reported cluster.\n3. Go to the 'Release channel' configuration.\n4. To edit, Click on the 'UPGRADE AVAILABLE' or 'Edit release channel'(Whichever available).\n5. In the 'Edit version' pop-up, select the required release channel(Regular Channel/ Stable Channel/ Rapid Channel) from the 'Release channel' dropdown.\n6. Click on 'SAVE CHANGES' or 'CHANGE'. Know more on Release Channels here: https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-445":{"article":"The VM instance is utilizing disks that have multiple snapshots associated with them. Snapshots are used to progressively back up data from your persistent disks. Once you create a snapshot to capture the current disk state, you can utilize it to recover the data onto a new disk. Typically, you would only require the most recent snapshot to restore data in case of any issues.","impact":"Keeping redundant snapshots will increase your monthly bill.","report_fields":["sourceDisk","selfLink"],"remediation":"From Console:\n1. Go to the Snapshots page in the Google Cloud Platform Console.\n2. Click on redundant snapshot and Delete it.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-070":{"article":"Armor should be enabled to protect your  applications running behind the LB from common threats and vulnerabilities.","impact":"Disabled Armor will not automatically detect and prevent high volume Level 7 DDoS attacks with machine learning trained locally for your applications.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to Network services.\n2. Go to Load balancing.\n3. Click the Backend services.\n4. Choose Security policy.\n6. Click on Save.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-035":{"article":"It is recommended to use Instance-specific SSH key(s) instead of using common/shared project-wide SSH key(s) to access Instances.","impact":"Using project-wide SSH keys eases the SSH key management but, if compromised, poses a security risk that can impact all the instances within the project.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances. It will list all the instances in your project.\n2. Click on the name of the Impacted instance.\n3. Click on Edit in the toolbar.\n4. Under SSH Keys, go to the Block project-wide SSH keys checkbox.\n5. To block users with project-wide SSH keys from connecting to this instance, select Block project-wide SSH keys.\n6. Click on Save at the bottom of the page.\n7. Repeat the above steps for every impacted Instance.\nFrom CLI:\nBlock project-wide public SSH keys, set the metadata value to TRUE:\ngcloud compute instances add-metadata [INSTANCE_NAME] --metadata block-project-ssh-keys=TRUE","multiregional":true,"service":"Compute Engine"},"ecc-gcp-017":{"article":"In order to prevent unnecessary project ownership assignments to users/service-accounts and further misuses of project and resources, all roles/Owner assignments should be monitored.\nMembers (users/Service-Accounts) with a role assignment to primitive role roles/owner are Project Owners.\nProject Owner has all the privileges on a project it belongs to. These can be summarized as below:\n- All viewer permissions on All GCP Services within the project\n- Permissions for actions that modify state of All GCP Services within the project\n- Manage roles and permissions for a project and all resources within the project\n- Set up billing for a project\nGranting the owner role to a member (user/Service-Account) will allow that member to modify the Identity and Access Management (IAM) policy. Therefore, grant the owner role only if the member has a legitimate purpose to manage the IAM policy. This is because the project IAM policy contains sensitive access control data. Having a minimal set of users allowed to manage the IAM policy will simplify any auditing that may be necessary.","impact":"Project ownership gives the highest level of project privileges. The absence of a log metric filter for Project Ownership assignments/changes can result in insufficient response time to misuse of project resources, actions to assign/change ownership of the project mentioned in the description.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click \"CREATE METRIC\".\n2. Click on the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\n(protoPayload.serviceName=\"cloudresourcemanager.googleapis.com\")\nAND (ProjectOwnership OR projectOwnerInvitee)\nOR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\"REMOVE\"\nAND protoPayload.serviceData.policyDelta.bindingDeltas.role=\"roles/owner\")\nOR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\"ADD\"\nAND protoPayload.serviceData.policyDelta.bindingDeltas.role=\"roles/owner\")\n4. Click on Submit Filter. The logs display based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the advanced logs query.\n6. Click on Create Metric.\nCreate the display prescribed Alert Policy:\n1. Identify the newly created metric under the section User-defined Metrics at\nhttps://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the desired metric and select Create alert from Metric. A new page opens.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the project:\nSet `Aggregator` to `Count`\nSet `Configuration`:\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notifications channels in the section Notifications.\n5. Name the policy and click Save.","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-141":{"article":"This policy identifies Storage buckets that are encrypted with the default Google-managed keys. As a best practice, use Customer-managed key to encrypt the data in your storage bucket and ensure full control over your data.\nNote: If you lose your CMKs, GCP won't be able  help you to decrypt your data.","impact":"Not using Customer-Managed Keys results in less control over aspects of your keys' lifecycle and management.","report_fields":["selfLink"],"remediation":"1. Login to GCP Portal.\n2. Go to 'Storage' (Left Panel).\n3. Select 'Browser'.\n4. Click on the reported bucket.\n5. Click on the 'Edit bucket' button.\n6. Click on 'Show advanced settings', change 'Encryption' to 'Customer-managed key' and select a customer-managed key from the drop-down list or enter the key resource ID.\n7. Click on 'Save'.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-193":{"article":"Enable VPC Flow Logs and Intranode Visibility to see pod-level traffic, even for traffic within a worker node.","impact":"Enabling Intranode Visibility makes intranode pod-to-pod traffic visible to the networking fabric. Without enabled VPC Flow Logs, you miss out on the opportunity to detect and access security issues like overly permissive security groups or network ACLs. Also, you cannot receive alerts about abnormal activities triggered within your VPC network, such as rejected connection requests or unusual levels of data transfer.","report_fields":["selfLink"],"remediation":"From Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Select Kubernetes clusters for which intranode visibility is disabled.\n3. Click on 'Edit'.\n4. Set 'Intranode visibility' to 'Enabled'.\n5. Click on 'Save'.\nFrom Command Line:\nTo enable intranode visibility for an existing cluster, run the following command:\ngcloud beta container clusters update [CLUSTER_NAME] --enable-intra-node-visibility","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-175":{"article":"In order to maintain the highest level of security, all connections to an application should be secure by default.","impact":"Not using uniform bucket-level access can not guarantee that if a Storage bucket is not publicly accessible, no object in the bucket is publicly accessible either.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Open the Cloud Storage browser in the Google Cloud Console using https://console.cloud.google.com/storage/browser\n2. In the list of buckets, click on the name of the desired bucket.\n3. Select the Permissions tab near the top of the page.\n4. In the textbox that starts with 'This bucket uses fine-grained access control..', click Edit.\n5. In the pop-up menu that appears, select Uniform.\n6. Click Save.\nFrom Command Line:\nUse the 'on' option in a uniformbucketlevelaccess set command:\ngsutil uniformbucketlevelaccess set on gs://BUCKET_NAME/","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-199":{"article":"The 'log_error_verbosity' flag controls the verbosity/details of messages logged. Valid values are: TERSE, DEFAULT, VERBOSE, TERSE excludes the logging of DETAIL, HINT, QUERY, and CONTEXT error information. VERBOSE output includes the SQLSTATE error code, source code file name, function name, and line number that generated the error. Ensure an appropriate value is set to 'DEFAULT' or stricter.","impact":"Auditing helps in troubleshooting operational problems and also permits forensic analysis. If 'log_error_verbosity' is not set to the correct value, too many details or too few details may be logged.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_error_verbosity' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the skip_show_database database flag for every Cloud SQL Mysql database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_error_verbosity=<TERSE|DEFAULT|VERBOSE>\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-185":{"article":"It is recommended to set the 'contained database authentication' database flag for Cloud SQL on the SQL Server instance to 'off'.","impact":"Threats are related to the USER WITH PASSWORD authentication process, which moves the authentication boundary from the Database Engine level to the database level.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'contained database authentication' flag from the drop-down menu, and set its value to 'off'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database Instances using the following command:\ngcloud sql instances list\n2. Configure the 'contained database authentication' database flag for every Cloud SQL SQL Server database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags ''contained database authentication=off''\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-131":{"article":"This policy identifies Google Kubernetes Engine (GKE) Clusters which are not configured with a private nodes feature. A private nodes feature makes your master inaccessible from the public internet, and nodes do not have public IP addresses, so your workloads run in an environment that is isolated from the internet.","impact":"External IP addresses are publicly announced, which means they are accessible to any host on the Internet. Using public nodes can increase opportunities for malicious activities such as hacking and Distributed Denial-of-Service (DDoS) attacks.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Google Cloud Console:\nTo create a new Kubernetes engine cluster with private node feature enabled, perform the following:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list.\n2. Click CREATE CLUSTER.\n3. In the Networking section, check the 'Enable VPC-native traffic routing (using alias IP)' option.\n4. From 'IPv4 network access', select the 'Private cluster' checkbox.\n5.  Configure other settings as required\n6. Click on 'Create'\nNOTE: When you create a private cluster, you must specify a /28 CIDR range for the VMs that run the Kubernetes master components.\nUsing Command Line:\nTo create a cluster with Private Nodes enabled, include the --enable-private-nodes flag within the cluster create command:\ngcloud container clusters create [CLUSTER_NAME] \\\n--enable-private-nodes\nSetting this flag also requires the setting of --enable-ip-alias and --master-ipv4-cidr=[MASTER_CIDR_RANGE].","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-207":{"article":"The 'log_min_error_statement' flag defines the minimum message severity level that is considered as an error statement. Messages for error statements are logged with the SQL statement. Valid values include DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC. Each severity level includes the subsequent levels mentioned above. Ensure a value of ERROR or stricter is set.","impact":"If 'log_min_error_statement' is not set to the correct value, messages may not be classified as error messages appropriately. Considering general log messages as error messages would make it difficult to find actual errors and considering only stricter severity levels as error messages may skip actual errors to log their SQL statements.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_min_error_statement' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_min_error_statement database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_min_error_statement=<DEBUG5|DEBUG4|DEBUG3|DEBUG2|DEBUG1|INFO|NOTICE|WARNING|ERROR>\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-004":{"article":"A service account is a special Google account that belongs to an application or a VM, instead of an individual end-user. The application uses the service account to call the service's Google API so that users aren't directly involved.\nIt's recommended not to use admin access for ServiceAccount.","impact":"Enrolling ServiceAccount with Admin rights gives full access to an assigned application or a VM. A ServiceAccount Access holder can perform critical actions like delete, update or change settings, and others without user intervention.","report_fields":["member","roles"],"remediation":"From Console:\n1. Go to IAM & admin/IAM using https://console.cloud.google.com/iam-admin/iam.\n2. Go to Members.\n3. Identify User-Managed user created service account with roles containing *Admin or *admin or role matching Editor or role matching Owner.\n4. Click the Delete bin icon to remove the role from the member (service account in this case).\nFrom Command Line:\ngcloud projects get-iam-policy PROJECT_ID --format json > iam.json\n1. Using a text editor, remove Role which contains roles/*Admin or roles/*admin or matched roles/editor or roles/owner. Add a role to the bindings array that defines group members and the roles for those members.\nFor example, to grant the role roles/appengine.appViewer to the ServiceAccount which is roles/editor, you would change the example shown below as follows:\n{\n\"bindings\": [\n{\n  \"members\": [\n    \"serviceAccount:our-project-123@appspot.gserviceaccount.com\",\n  ],\n  \"role\": \"roles/appengine.appViewer\"\n},\n{\n  \"members\": [\n    \"user:email1@gmail.com\"\n  ],\n  \"role\": \"roles/owner\"\n  },\n{\n  \"members\": [\n    \"serviceAccount:our-project-123@appspot.gserviceaccount.com\",\n    \"serviceAccount:123456789012-compute@developer.gserviceaccount.com\"\n  ],\n  \"role\": \"roles/editor\"\n}\n],\n\"etag\": \"BwUjMhCsNvY=\"\n}\n2. Update the project's IAM policy:\ngcloud projects set-iam-policy PROJECT_ID iam.json","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-289":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the RPC port (135) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted ingress/inbound access on TCP port 135 (RPC) through VPC network firewall rules can increase opportunities for malicious activities such as hacking (e.g. backdoor command shell), ransomware attacks, and Denial-of-Service (DoS) attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-104":{"article":"Default firewall rule should not be in use.","impact":"The default firewall rule can contain vulnerabilities that lead to hacker attacks such as brute force attacks, denial of service (DoS) attacks, etc.","report_fields":["selfLink"],"remediation":"From Console:\n1. Select 'VPC network' from the side menu.\n2. Select 'Firewall rules'.\n3. Delete firewall rules containing 'default' in the network column.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-273":{"article":"Adaptive Protection builds machine-learning models that detect and alert on anomalous activity, generate a signature describing the potential attack, and generate a custom Google Cloud Armor WAF rule to block the signature.","impact":"Armor with disabled Adaptive Protection will not automatically detect and prevent high volume Level 7 DDoS attacks with machine learning trained locally for your applications.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud Armor page in the Cloud Console.\n2. On the Security policies page, click the name of the security policy. The Policy details page is displayed.\n3. Click the Edit button.\n4. Under Adaptive Protection select Enable.\n5. Click Update.","multiregional":true,"service":"Cloud Armor"},"ecc-gcp-234":{"article":"By default, Cloud SQL provides encryption at rest for all data using Google-managed keys. For additional control over encryption, encrypt data using customer supplied encryption keys.\nWarning: Never delete the primary key version that you initially use when you create your instance. You can not restore the backup for the instance if the key version that was originally used when the instance was created is destroyed. Even when the key version is rotated, the original key version must be maintained. If it is destroyed, it can't be re-created.","impact":"Not using CMEK can cause problems if your project is compromised and all data will be disclosed. At least, business-critical database instances should have a CMEK-encrypted storage.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the Cloud SQL Instances page at Google Cloud Console.\n2. Click Create instance.\n3. Choose the database engine. Enter a name for the instance. Enter the password for the 'root'@'%' user.\n4. Set the region for your instance. Place your instance in the same region as the resources that access it.\n5. Under Configuration options, select all your configurations options until you reach Machine type and storage.\n6. Expand Machine type and storage.\n7. Under Encryption, choose Customer-managed key.\n8. Select the KMS key from the dropdown menu or manually enter the KMS_RESOURCE_ID.\n9. If the service account does not have permission to encrypt/decrypt with the selected key, a message displays. If this happens, click Grant to grant the service account the IAM role on the selected KMS key.\n10. Once the configuration options are selected, click Create.\n11. You see a message explaining the implications of using customer-managed encryption key. Read and acknowledge it to proceed further with instance creation.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-117":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the Oracle DB port (1521) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted ingress/inbound access on TCP port 1521 through VPC network firewall rules can increase opportunities for malicious activities such as denial-of-service attacks, brute-force and man-in-the-middle (MITM) attacks, and can ultimately lead to data loss.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-309":{"article":"This rule detects when a Dataproc cluster is configured with public IP. This security misconfiguration could put your data at risk of accidental exposure, because a public IP accompanied by an open firewall rule allows potentially unauthorized access to the Dataproc VMs.","impact":"Public IP addresses allow potentially unauthorized access to Dataproc virtual machines.","report_fields":["clusterName","projectId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Log in to the GCP Console.\n2. Navigate to Dataproc https://console.cloud.google.com/dataproc.\n2. Click on the name of the Dataproc cluster that you want to re-create and collect all the configuration information available for the selected resource.\n7. Go back to the Clusters console and click on the CREATE CLUSTER button from the dashboard top menu to initiate the Dataproc cluster setup process.\n3. Select Customize Cluster to view Network Configuration settings.\n4. Locate the Internal IP Only section and select the checkbox next to Configure all instances to have only internal IP addresses.\n5. Configure the required configuration and click Create.","multiregional":true,"service":"Dataproc"},"ecc-gcp-103":{"article":"Ensure that protection against accidental deletion of instances is enabled","impact":"An instance without deletion protection is not protected from accidental or intentional deletion.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM instances page.\n2. Choose VM instance.\n3. Click the Edit button.\n4. Toggle the Enable deletion protection checkbox.\n5. Click Save.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-271":{"article":"This rule finds networks without deny egress rule for all resources. Specific IP addresses or ports should be opened in the firewall on an as-needed basis.","impact":"If network does not have deny egress rule for all resources, then  any GCP resource in this network can connect to any external destination.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the firewall page https://console.cloud.google.com/networking/firewalls.\n2. Select CREATE FIREWALL RULE.\n3. Enter appropriate rule name, for example deny-egress-all.\n4. Turn on logging in the Log section.\n5. Select the appropriate Network.\n6. Enter Priority as 65535.\n7. Select Direction of traffic as Egress.\n8. Select Action on match as Deny.\n9. Select Targets as All instances in the network.\n10. Select Source filter as IP ranges and enter 0.0.0.0/0.\n11. Select Protocols and ports as Deny all.\n12. Click CREATE.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-346":{"article":"Network tags are used by networks to identify which Compute Engine virtual machine (VM) instances are subject to certain firewall rules and network routes. It is recommended to use network tags instead of setting rules that will effect all of the instances in a network.","impact":"Instances without tags have no micro-segmentation and are only controlled by high level firewall rules that are applicable to the entire network.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances\n2. Click on the instance you want to add or update tags for.\n3. Click EDIT.\n4. Make changes in the Network tags section.\n5. Save your changes.\nFrom Command Line:\n1. Run the following command:\ngcloud compute instances add-tags INSTANCE_NAME --tags TAG_NAME","multiregional":true,"service":"Compute Engine"},"ecc-gcp-020":{"article":"It is recommended that a metric filter and alarm be established for VPC Network Firewall rule changes.","impact":"Lack of monitoring and logging of VPC Network Firewall rule changes can lead to insufficient response time to detect suspicious activities.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click 'CREATE METRIC'.\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nresource.type=\"gce_firewall_rule\"\nAND (protoPayload.methodName:\"compute.firewalls.patch\"\nOR protoPayload.methodName:\"compute.firewalls.insert\"\nOR protoPayload.methodName:\"compute.firewalls.delete\")\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the advanced logs query.\n6. Click Create Metric.\nCreate the prescribed Alert Policy:\n1. Identify the newly created metric under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page displays.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notifications channels in the Notifications section.\n5. Name the policy and click Save.\nFrom Google Cloud CLI:\nCreate the prescribed Log Metric\ngcloud logging metrics create\nCreate the prescribed alert policy\ngcloud alpha monitoring policies create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-432":{"article":"In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network. \nAlthough Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API.\nBy default, the Private Endpoint is disabled","impact":"Failure to disable the public endpoint of the master node in a private cluster may expose the Kubernetes API to unrestricted access from outside the master's VPC network, increasing the risk of vulnerabilities and unauthorized attacks on the cluster.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Google Cloud Console\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list.\n2. Click CREATE CLUSTER.\n3. Configure the cluster as desired then click 'Availability, networking, security, and additional features'.\n4. Under 'Network Security' ensure the 'Private cluster' checkbox is checked\n5. Clear the 'Access master using its external IP address' checkbox.\n6. Configure other settings as required.\n7. Click CREATE.\nUsing Command Line\nCreate a cluster with a Private Endpoint enabled and Public Access disabled by including the --enable-private-endpoint flag within the cluster create command:\n  gcloud container clusters create [CLUSTER_NAME] --enable-private-endpoint\nSetting this flag also requires the setting of --enable-private-nodes, --enable-ip-alias and --master-ipv4-cidr=[MASTER_CIDR_RANGE].","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-087":{"article":"Ensure that SSL/TLS certificates stored in Cloud SQL are renewed one week before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid, and the communication between a client and a GCP resource that implements the certificates is no longer secure.","report_fields":["selfLink"],"remediation":"You must take the following steps to complete the rotation:\n1.Download the new server certificate information.\n2.Update your clients to use the new server certificate information.\n3.Complete the rotation, which moves the currently active certificate into the \"previous\" slot and updates the newly added certificate to be the active certificate.\nDownload the new server certificate information:\n1.Go to the Cloud SQL Instances page in the Google Cloud Platform Console.\n2.Click the instance name to open its Instance details page.\n3.Select the CONNECTIONS tab.\n4.Scroll down to the Configure SSL server certificates section.\n5.Click Create new certificate.\n6.Scroll down to Download SSL server certificates section.\n7.Click Download.\nThe server certificate information, encoded as a PEM file, is displayed and can be downloaded to your local environment.\n8.Update all of your clients to use the new information.\nAfter you have updated your clients, complete the rotation:\n1.Return to the Configure SSL server certificates section.\n2.Click Rotate certificate.\n3.Confirm that your clients are connecting properly.\nIf any clients are not connecting using the newly rotated certificate, you can click Rollback certificate to roll back to the previous configuration.\nLink: https://cloud.google.com/sql/docs/mysql/configure-ssl-instance.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-448":{"article":"If you have workloads that are fault tolerant, like HPC, big data, media transcoding, CI/CD pipelines or stateless web applications, using preemptible VMs to batch-process them can provide massive cost savings. \nUsing preemptible VMs in your architecture is a great way to scale compute at a discounted rate, but you need to be sure that the workload can handle the potential interruptions if the VM needs to be reclaimed.","impact":"Not using preemptible VMs in your architecture can result in missed opportunities to efficiently scale your compute resources while enjoying cost savings.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the VM compute metadata page using https://console.cloud.google.com/compute/metadata.\n2. Click on reported instance and Delete it.\n3. Click CREATE INSTANCE.\n4. Fill out the desired configuration for your instance.\n5. Under the 'Availability policies' section, choose Spot VM provisioning model and other desired setting for VM provisioning model.\n6. Click Create.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-032":{"article":"Private Google Access enables virtual machine instances in a subnet to reach Google APIs and services using an internal IP address rather than an external IP address. External IP addresses are routable and reachable over the Internet. Internal (private) IP addresses are internal to Google Cloud Platform and are not routable or reachable over the Internet. You can use Private Google Access to allow VMs without Internet access to reach Google APIs, services, and properties that are accessible over HTTP/HTTPS.","impact":"Disabling Google Access private access can increase the scope for malicious acts through public IP addresses, such as hacking, Man-In-The-Middle (MITM) attacks, and brute force attacks that increase the risk of resource compromise.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to VPC network GCP Console using https://console.cloud.google.com/networking/networks/list.\n2. Click the name of a subnet, the Subnet details page is displayed.\n3. Click on the EDIT button.\n4. Set Private Google access to On.\n5. Click on Save.\nUsing Command Line:\nTo set Private Google access for an network subnets, run the following command:\ngcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-private-ip-google-access","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-186":{"article":"It is recommended to configure Second Generation SQL instance to use private IPs instead of public IPs.","impact":"To lower the organization's attack surface, Cloud SQL databases should not have public IPs. Private IPs provide improved network security and lower latency for your application.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console https://console.cloud.google.com/sql/instances.\n2. Click the instance name to open its Instance details page.\n3. Select the Connections tab.\n4. Deselect the Public IP checkbox.\n5. Click Save to update the instance.\nFrom Command Line:\n1. For every instance remove its public IP and assign a private IP instead:\ngcloud sql instances patch INSTANCE_NAME --network=VPC_NETWOR_NAME --noassign-ip\n2. Confirm the changes using the following command:\ngcloud sql instances describe INSTANCE_NAME","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-063":{"article":"Create and use minimally privileged Service accounts to run GKE cluster nodes instead of using a Compute Engine default Service account. Unnecessary permissions could be abused in the case of a node compromise.","impact":"The Compute Engine default service account has broad access by default. But it also has more permissions than are required to run your Kubernetes Engine cluster, which makes it an easy target for attackers.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Google Cloud Console:\nFirstly, create a minimally privileged service account.\n1. Go to Service Accounts by visiting https://console.cloud.google.com/iam-admin/serviceaccounts.\n2. Click on CREATE SERVICE ACCOUNT.\n3. Enter Service Account Details.\n4. Click CREATE.\n5. Within Service Account permissions add the following roles:\n\u2022 Logs Writer\n\u2022 Monitoring Metric Writer\n\u2022 Monitoring Viewer.\n6. Click CONTINUE.\n7. Grant users access to this service account and create keys as required.\n8. Click DONE.\nTo create a Node pool to use a Service account:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Click on the cluster name within which the Node pool will be launched.\n3. Click on ADD NODE POOL.\n4. Within the Node Pool options, under the 'Security' heading, select the minimally privileged service account from the Service Account drop-down list.\n5. Click on SAVE to launch the Node pool. You will need to migrate your workloads to the new Node pool and delete Node pools that use the default service account to complete the remediation.\nUsing Command Line:\nFirstly, create a minimally privileged service account:\ngcloud iam service-accounts create [SA_NAME] --display-name 'GKE Node Service Account'\nexport NODE_SA_EMAIL=`gcloud iam service-accounts list  --format='value(email)' --filter='displayNameGKE Node Service Account'`\nGrant the following roles to the service account:\nexport PROJECT_ID=`gcloud config get-value project`\ngcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount$NODE_SA_EMAIL --role roles/monitoring.metricWriter\ngcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount$NODE_SA_EMAIL --role roles/monitoring.viewer\ngcloud projects add-iam-policy-binding $PROJECT_ID --member serviceAccount$NODE_SA_EMAIL --role roles/logging.logWriter\nTo create a new Node pool using the Service account, run the following command:\ngcloud container node-pools create [NODE_POOL] --service-account=[SA_NAME]@[PROJECT_ID].iam.gserviceaccount.com --cluster=[CLUSTER_NAME] --zone [COMPUTE_ZONE]\nYou will need to migrate your workloads to the new Node pool, and delete Node pools that use the default service account to complete the remediation.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-258":{"article":"This rule detects when a Cloud Bigtable backup is created with an expiration date of 29 days or less. A table backup should persist for at least 30 days or longer before it is replaced with a new backup, preferably on an automated cadence.","impact":"Too short an expiration time reduces the benefit of backing up your data to protect against loss.","report_fields":["sourceTable","name"],"remediation":"From Console:\n1. Go to the Bigtable page at Google Cloud Console https://console.cloud.google.com/bigtable/instances.\n2. Select an instance.\n3. Click on Tables.\n4. Select a table.\n5. Click on Create Backup.\n6. Enter a unique ID for the backup.\n7. Set an expiration date for 30 days or more.\n8. Click on Create.","multiregional":true,"service":"Cloud Bigtable"},"ecc-gcp-166":{"article":"Checks for Google Kubernetes Engine (GKE) clusters that are configured to use the default network. It is recommended not to use the default network on GKE.","impact":"Since GKE uses the default network when creating routes and firewalls for the cluster, this does not guarantee that it will meet your security and network requirements for inbound and outbound traffic. As a result, it can lead to malicious activity such as hacking, data loss, etc.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Navigate to the 'Kubernetes Engine', and select 'Clusters'.\n2. Create cluster.\n3. Set the new cluster parameters as per your requirement and ensure 'Network' is not set to 'default' under Networking section.\nTo delete the old cluster:\n1. Navigate to the 'Kubernetes Engine', and select 'Clusters'.\n2. Select the old cluster.\n3. Click DELETE.\n4. On 'Delete a cluster' popup dialog, click DELETE to confirm the deletion of the cluster.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-449":{"article":"Compute snapshot without label info","impact":"Compute snapshot without label information make identification and search difficult.","report_fields":["selfLink","sourceDisk"],"remediation":"From Console:\n1. Go to the Snapshots page in the Google Cloud Platform Console.\n2. Click on reported Snapshot name.\n3. Click \"Add label\" and add proper key-value pair.\n4. Save changes.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-227":{"article":"This policy identifies GCP compute engine images that are not encrypted using customer-managed keys. If you use a customer-managed encryption key, your encryption keys are stored within Cloud KMS. The project that holds your encryption keys can then be independent of the project that contains your buckets, thus allowing for better separation of duties.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of keys.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nThe image encryption can't be modified once the image is created for a disk. To remediate we have to create a new image for the same disk by replicating all the settings from current image and deleting the old image.\nFollow below steps to create a compute image.\nFrom console:\n1. Login to google cloud console.\n2. Navigate to service 'Compute Engine'.\n3. On left menu, go to Section 'Storage' and under that select 'Images'.\n4. Click on 'CREATE IMAGE', replicate all the details from existing image.\n5. Under section 'Encryption', select 'Customer-managed key'.\n6. Provide the encryption key by selecting it from the dropdown menu of 'Select a customer-managed key'.\n7. Click on 'Create'.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-124":{"article":"Ensure your GKE Master node version is supported. This policy checks your GKE master node version and generates an alert if the running version is unsupported.","impact":"Using an unsupported version of the master has bugs and security vulnerabilities found in a supported minor version.","report_fields":["selfLink"],"remediation":"Note: Upgrading a node pool may disrupt workloads running in that node pool. To avoid this, you can create a new node pool with the desired version and migrate the workload. After migration, you can delete the old node pool.\nManually initiate a master upgrade.\n1. From Google Cloud Platform Console, go to the Google Kubernetes Engine menu.\n2. Select the desired cluster, clik Actions, then click edit Edit.\n3. On the Cluster details page, click the Nodes tab.\n4. In the Node Pools section, click the name of the node pool that you want to upgrade.\n5. Click Edit.\n6. Click Change under Node version.\n7. Select the desired version, then click Change.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-021":{"article":"It is recommended that a metric filter and alarm be established for VPC network route changes.","impact":"Lack of monitoring and logging of route table changes can lead to insufficient response time to detect accidental or intentional modifications that may result in in uncontrolled network traffic.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed Log Metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click 'CREATE METRIC'.\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter\n3. Clear any text and add:\nresource.type=\"gce_route\"\nAND (protoPayload.methodName:\"compute.routes.delete\"\nOR protoPayload.methodName:\"compute.routes.insert\")\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.\n6. Click Create Metric.\nCreate the prescribed alert policy:\n1. Identify the newly created metric under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page displays.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notification channels in the Notifications section.\n5. Name the policy and click Save\nFrom Google Cloud CLI:\nCreate the prescribed Log Metric:\ngcloud logging metrics create\nCreate the prescribed the alert policy:\ngcloud alpha monitoring policies create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-173":{"article":"Compute instances should not be configured to have external IP addresses.","impact":"An instance with a public IP address could potentially be compromised, and an attacker could gain anonymous access to it and other resources connected to this instance, which may lead to malicious activity with sensitive data.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to the VM instances page using 'https://console.cloud.google.com/compute/instances'.\n2. Click on the instance name to go the the Instance detail page.\n3. Click on 'Edit'.\n4. For each Network interface, ensure that External IP is set to 'None'.\n5. Click on 'Done' and then click on 'Save'.\nUsing Command Line:\n1. Describe the instance properties:\ngcloud compute instances describe INSTANCE_NAME --zone=ZONE\n2. Identify the access config name that contains the external IP address. This access config appears in the following format:\nnetworkInterfaces:\n- accessConfigs:\n  - kind: compute#accessConfig\n    name: External NAT\n    natIP: 130.211.181.55\n    type: ONE_TO_ONE_NAT\n3. Delete the access config:\ngcloud compute instances delete-access-config INSTANCE_NAME --zone=ZONE -- access-config-name ''ACCESS_CONFIG_NAME''\nIn the above example, the ACCESS_CONFIG_NAME is External NAT. The name of your access config might be different.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-061":{"article":"A private cluster is a cluster that makes your master inaccessible from the public internet. In a private cluster, nodes do not have public IP addresses, so your workloads run in an environment that is isolated from the internet. Nodes should have addresses only in the private RFC 1918 address space. Nodes and masters communicate with each other privately using VPC peering.","impact":"Without Private cluster, networking suffers higher latency, and services are exposed to the public Internet, which increases the attack surface.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to Kubernetes GCP Console using https://console.cloud.google.com/kubernetes/list?.\n2. Click on CREATE CLUSTER.\n3. Choose the required name/value for cluster fields.\n4. Click on More.\n5. From the Private cluster drop-down menu, select Enabled.\n6. Verify that VPC native (alias IP) is set to Enabled.\n7. Set Master IP range to as per your required IP range.\n8. Click on Create.\nUsing Command Line:\nTo create cluster with Private cluster enabled, run the following command:\ngcloud beta container clusters create [CLUSTER_NAME] --zone [COMPUTE_ZONE] --private-cluster --master-ipv4-cidr 172.16.0.16/28 --enable-ip-alias --create-subnetwork \"\"","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-228":{"article":"When Google Cloud Compute Engine performs periodic infrastructure maintenance it can migrate your virtual machine instances to other hardware without downtime. The virtual machine maintenance behavior determines whether the VM instances are live migrated or terminated during a maintenance event. To ensure that your Google Cloud VM instances are migrated to new hardware, set \"On Host Maintenance\" configuration setting to \"Migrate\".","impact":"If you have not configured the instance`s availability policy to use live migration instead of instance termination. Compute Engine will not live migrate your VM instance. This can not prevents your production applications from experiencing disruptions during maintenance events.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances.\n2. Click on the name of the virtual machine (VM) instance that you want to reconfigure.\n3. On the selected resource configuration page, click EDIT to access the instance edit mode.\n4. In the Availability policies section, select Migrate VM instance (recommended) from the On host maintenance dropdown list, to migrate (instead of terminate) the selected Google Cloud VM instance during maintenance events.\n5. Click Save to apply the configuration changes.\nFrom Command Line:\n1. Run compute instances set-scheduling command  using the name of the instance that you want to reconfigure as identifier parameter, to change the maintenance behavior for the selected Google Cloud virtual machine instance from TERMINATE to MIGRATE. Once the maintenance policy is set to MIGRATE, the VM instance should be migrated to a new host during maintenance:\ngcloud compute instances set-scheduling INSTANCE_NAME --zone ZONE --maintenance-policy=MIGRATE\n2. The command output should return the URL of the reconfigured virtual machine instance:\nUpdated [https://www.googleapis.com/compute/v1/projects/PROJECT_NAME/zones/ZONE/instances/INSTANCE_NAME].","multiregional":true,"service":"Compute Engine"},"ecc-gcp-119":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the PostgreSQL port (5432) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound access on TCP port 5432 (PostgreSQL Database) via VPC network firewall rules can increase opportunities for malicious activities such as hacking, brute-force attacks, DDoS, and SQL injection attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-218":{"article":"Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management. If enable-oslogin is not present or set to false in metadata then it means OS login is disabled.","impact":"if an instance based on template with not enabled osLogin, It makes difficult centralized and automated SSH key pair management, which is useless in handling cases like response to compromised SSH key pairs and/or revocation of external/third-party/Vendor users.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the instance template page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. To enable oslogin, create a new template and set enable-oslogin=true in instance template metadata.\n3. Repeat the above steps for every impacted Instance template.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-093":{"article":"It is recommended to use HTTPS instead of HTTP to encrypt the communication between the application clients and the application Load balancer.","impact":"An HTTP protocol is not a secure method of transmitting data. Any person monitoring the Internet traffic can see unencrypted data, which leads to a breach of confidentiality.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Load balancing page in the Google Cloud Platform Console.\n2. Find the target load balancer name.\n3. Click Edit (pencil).\n4. Configure Backend.\n5. Choose HTTPS as a protocol in the backend configuration.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-215":{"article":"To improve the security of your instance templates, use instance-specific SSH key(s) instead of using common/shared project-wide SSH key(s) to access instances.","impact":"Creating an instance based on template with project-wide SSH keys eases the SSH key management but, if compromised, poses a security risk that can impact all the instances within the project.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. To block users with project-wide SSH keys from connecting to the instance, create a new template and set block-project-wide-ssh-keys=true in instance template metadata.\n3. Repeat the above steps for every impacted Instance template.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-446":{"article":"VM instance template does not have any Labels. Labels can be used for easy identification and search.","impact":"VM instance templates without label information make identification and search difficult.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the instance template page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. Delete affected template.\n3. Create a new instance template with labels.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-025":{"article":"The default network has a preconfigured network configuration and automatically generates insecure firewall rules. To prevent the use of a default network, a project should not have a default network.","impact":"Automatically created firewall rules of the default network do not get audit logged and cannot be configured to enable firewall rule logging. A number of open ports are available by default, including port 22. This may increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the VPC networks page by visiting: https://console.cloud.google.com/networking/networks/list.\n2. Click the network named default.\n3. On the network detail page, click EDIT.\n4. Click DELETE VPC NETWORK.\n5. If needed, create a new network to replace the default network.\nFrom Command Line:\nFor each Google Cloud Platform project,\n1. Delete the default network:\ngcloud compute networks delete default\n2. If needed, create a new network to replace it:\ngcloud compute networks create NETWORK_NAME","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-001":{"article":"Use corporate login credentials instead of personal accounts, such as Gmail accounts.","impact":"Organizations do not have any control over personal accounts. Thus, your administrators may not see and control these accounts through the Google Admin console.","report_fields":["member","roles"],"remediation":"To find if the GCP project is associated with the corporate organization domain:\n1. List the accounts that have been granted access to that project\ngcloud projects get-iam-policy PROJECT_ID\n2. Also list the accounts added on each folder\ngcloud resource-manager folders get-iam-policy FOLDER_ID\n3. And list your organization's IAM policy\ngcloud organizations get-iam-policy ORGANIZATION_ID\nNote No email accounts outside the organization domain should be granted permissions in the IAM policies.\nFollow the documentation and setup corporate login accounts.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-023":{"article":"It is recommended that a metric filter and alarm be established for Cloud Storage Bucket IAM changes.","impact":"Lack of monitoring and logging of Cloud Storage IAM permission changes can lead to insufficient response time to detect and correct permissions on sensitive cloud storage buckets and objects inside the bucket.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click 'CREATE METRIC'.\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nresource.type=\"gcs_bucket\"\nAND protoPayload.methodName=\"storage.setIamPermissions\"\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.\n6. Click Create Metric.\nCreate the prescribed Alert Policy:\n1. Identify the newly created metric under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page appears.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero (0) for the most recent value will ensure that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notifications channels in the Notifications section.\n5. Name the policy and click Save.\nFrom Google Cloud CLI:\nCreate the prescribed Log Metric:\ngcloud beta logging metrics create\nCreate the prescribed alert policy:\ngcloud alpha monitoring policies create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-253":{"article":"This policy identifies GCP Kubernetes Engine clusters for which workload identity is disabled. Manual approaches for authenticating Kubernetes workloads violates the principle of least privilege on a multi-tenanted node when one pod needs to have access to a service, but every other pod on the node that uses the service account does not. Enabling Workload Identity manages the distribution and rotation of Service account keys for the workloads to use.","impact":"Disabled Workload Identity does not allow you to assign distinct, fine-grained identities and authorization for each application in your cluster.","report_fields":["selfLink"],"remediation":"The GKE Metadata Server requires Workload Identity to be enabled on a cluster. Modify the cluster to enable Workload Identity and enable the GKE Metadata Server. Using Google Cloud Console\nFrom Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. From the list of clusters, select the cluster for which Workload Identity is disabled.\n3. Click on EDIT\n4. Set Workload Identity to 'Enabled' and set the Workload Identity Namespace to the namespace of the Cloud project containing the cluster, e.g: \n[PROJECT_ID].svc.id.goog\n5. Click SAVE and wait for the cluster to update\n6. Once the cluster has updated, select each Node pool within the cluster Details page\n7. For each Node pool, select EDIT within the Node pool details page\n8. Within the Edit node pool pane, check the 'Enable GKE Metadata Server checkbox'\n9. Click SAVE.\nUsing Command Line:\ngcloud beta container clusters update [CLUSTER_NAME] --identity-namespace=[PROJECT_ID].svc.id.goog\nNote that existing Node pools are unaffected. New Node pools default to --workloadmetadata-from-node=GKE_METADATA_SERVER.\nTo modify an existing Node pool to enable GKE Metadata Server:\ngcloud beta container node-pools update [NODEPOOL_NAME] --cluster=[CLUSTER_NAME] --workload-metadata-from-node=GKE_METADATA_SERVER\nYou may also need to modify workloads in order for them to use Workload Identity as described within https://cloud.google.com/kubernetes-engine/docs/how-to/workloadidentity.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-288":{"article":"Check your Google Cloud VPC network firewall rules for inbound rules that allow unrestricted access (i.e. 0.0.0.0/0) to any hosts using ICMP and restrict the ICMP-based access to trusted IP addresses/IP ranges only, in order to implement the principle of least privilege (POLP) and reduce the attack surface.","impact":"Allowing unrestricted inbound/ingress ICMP access using VPC network firewall rules can increase opportunities for malicious activities such as Denial-of-Service (DoS) attacks, Smurf and Fraggle attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-184":{"article":"It is recommended to set the 'cross db ownership chaining' database flag for Cloud SQL SQL Server instance to 'off'.","impact":"This server option allows a user to control cross-database ownership chaining at the database level or allow cross-database ownership chaining for all databases. Enabling 'cross db ownership' is not recommended unless all of the databases hosted by the instance of SQL Server must participate in cross-database ownership chaining, and you are aware of the security implications of this setting.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'cross db ownership chaining' flag from the drop-down menu, and set its value to 'off'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'cross db ownership chaining' database flag for every Cloud SQL SQL Server database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags ''cross db ownership chaining=off''\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-172":{"article":"To defend against advanced threats and ensure that the boot loader and firmware on your VMs are signed and untampered, it is recommended that Compute instances are launched with Shielded VM enabled.","impact":"Without Shielded VM, there is no protection of production workloads from cybersecurity threats like remote attacks, privilege escalation, malicious actors. Also, advanced platform security capabilities cannot be used, such as secure and measured boot, a Virtual Trusted Platform Module (vTPM), UEFI firmware, and integrity monitoring.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue if an instance does not use an image with Shielded VM support\nUsing Console:\n1. Go to the VM instances page using 'https://console.cloud.google.com/compute/instances'.\n2. Click on the instance name to see its 'VM instance details' page.\n3. Click on 'Stop' to stop the instance.\n4. When the instance has stopped, click on 'Edit'.\n5. In the 'Shielded VM' section, select 'Turn on vTPM' and 'Turn on Integrity Monitoring'.\n6. Optionally, if you do not use any custom or unsigned drivers on the instance, select 'Turn on Secure Boot'.\n7. Click on the 'Save' button to modify the instance and then click on 'Start' to restart it.\nUsing Command Line:\nYou can only enable Shielded VM options on instances that have Shielded VM support. For a list of Shielded VM public images, run the gcloud compute images list command with the following flags:\ngcloud compute images list --project gce-uefi-images --no-standard-images\n1. Stop the instance:\ngcloud compute instances stop INSTANCE_NAME\n2. Update the instance:\ngcloud compute instances update INSTANCE_NAME --shielded-vtpm --shielded-vmintegrity-monitoring\n3. Optionally, if you do not use any custom or unsigned drivers on the instance, turn on secure boot:\ngcloud compute instances update INSTANCE_NAME --shielded-vm-secure-boot\n4. Restart the instance:\ngcloud compute instances start INSTANCE_NAME","multiregional":true,"service":"Compute Engine"},"ecc-gcp-092":{"article":"Ensure that SSL policy for a Load Balancer is based on TLS 1.1 or TLS 1.2, but not on TLS 1.0.","impact":"TLS 1.0 is vulnerable to man-in-the-middle attacks, risking the integrity and authentication of data sent between a website and a browser.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the SSL policies page in the Google Cloud Platform Console.\n2. Click on Create policy or choose the created one. The Create policy page appears.\n3. Enter a Name.\n4. Select Minimum TLS Version. Choose 1.1 or 1.2.\n5. Under Profile, select Compatible, Modern, or Restricted. Enabled features and Disabled features for the profile are displayed on the right side of the page.\n6. If there is a Load balancer to which you want to attach the policy, click Add target and select a forwarding rule as the target of the SSL policy. If desired, add more targets.\n7. Click on Create. Refer https://cloud.google.com/load-balancing/docs/use-ssl-policies.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-067":{"article":"Ensure that all secrets in the Secret Manager have an expiration time set.","impact":"Secrets can be misused or exposed during their life cycle, which can lead to potential threats to data integrity and confidentiality.","report_fields":["name"],"remediation":"From Console:\n1. Go to Security, Secret Manager.\n2. Click on the Name of affected secret.\n3. Click Edit secret.\n4. Choose Set expiration date under Expiration.\n5. Set an appropriate expiration date.","multiregional":true,"service":"Secret Manager"},"ecc-gcp-101":{"article":"This insight will be triggered whenever we detect a Load balancer with no logging enabled. Logging is crucial in detecting intrusion attempts, monitoring access as well as in helping debug access errors and system failures during and after the fact. Almost every compliance framework will mandate logging be enabled.","impact":"Disabled access logs for Load Balancer Access makes it harder to analyze statistics, diagnose issues or detect different types of attacks, as well as retain data for regulatory or legal purposes.","report_fields":["selfLink"],"remediation":"From Console:\n1. From Google Cloud home open the Navigation Menu in the top left.\n2. Under the Networking heading select Network services.\n3. Select the HTTPS load-balancer you wish to audit.\n4. Select Edit then Backend Configuration.\n5. Select Edit on the corresponding backend service.\n6. Click Enable Logging.\n7. Set Sample Rate to a desired value. This is a percentage as a decimal point. 1.0 is 100%.\nFrom Google Cloud CLI\n1. Run the following command\ngcloud compute backend-services update <serviceName> --region=REGION --enable-logging --logging-sample-rate=<percentageAsADecimal>","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-036":{"article":"Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management.","impact":"Disabling osLogin makes it more difficult to centrally and automatically manage SSH key pairs, which is useful when handling cases such as responding to compromised SSH key pairs and/or revoking external/third-party/vendor users.","report_fields":["selfLink"],"remediation":"Set enable-oslogin in project-wide metadata so that it applies to all of the instances in your project:\nUsing Console:\n1. Go to the VM compute metadata page using https://console.cloud.google.com/compute/metadata.\n2. Click Edit.\n3. Add a metadata entry where the key is enable-oslogin and the value is TRUE.\n4. Click Save to apply the changes.\n5. For every instance that overrides the project setting, go to the VM Instances page at https://console.cloud.google.com/compute/instances.\n6. Click the name of the instance on which you want to remove the metadata value.\n7. At the top of the instance details page, click Edit to edit the instance settings.\n8. Under Custom metadata, remove any entry with key 'enable-oslogin' and value 'FALSE'.\n9. At the bottom of the instance details page, click Save to apply your changes to the instance.\nFrom CLI:\n1. Configure oslogin on the project:\ngcloud compute project-info add-metadata --metadata enable-oslogin=TRUE\n2. Remove instance metadata that overrides the project setting:\ngcloud compute instances remove-metadata INSTANCE_NAME --keys=enable-oslogin","multiregional":true,"service":"Compute Engine"},"ecc-gcp-153":{"article":"VM instance does not have any Labels. Labels can be used for easy identification and search.","impact":"VM instances without label information make identification and search difficult.","report_fields":["selfLink"],"remediation":"1. Login to GCP Console and, from 'Compute', select 'Compute Engine'.\n2. Select the identified VM instance.\n3. Click on the 'SHOW INFO PANEL'.\n4. Add labels with the appropriate KeyValue information.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-019":{"article":"Google Cloud IAM provides predefined roles that give granular access to specific Google Cloud Platform resources and prevent unwanted access to other resources. However, to cater to organization-specific needs, Cloud IAM also provides the ability to create custom roles. Project owners and administrators\nwith the Organization Role Administrator role or the IAM Role Administrator role can create custom roles.\nIt is recommended that a metric filter and alarm be established for changes to Identity and Access Management (IAM) role creation, deletion and updating activities.","impact":"Lack of monitoring and logging of Custom Role changes calls can lead to insufficient response time to detect accidental or intentional changes that may result in unauthorized access.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click 'CREATE METRIC'.\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nresource.type='iam_role'\nAND protoPayload.methodName = 'google.iam.admin.v1.CreateRole'\nOR protoPayload.methodName='google.iam.admin.v1.DeleteRole'\nOR protoPayload.methodName='google.iam.admin.v1.UpdateRole'\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the advanced logs query.\n6. Click Create Metric.\nCreate the prescribed Alert Policy:\n1. Identify the new metric that has just been created under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the metric and select Create alert from Metric. A new page displays.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero(0) for the most recent value ensures that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notification channels in the Notifications section.\n5. Name the policy and click Save.\nFrom Google Cloud CLI:\n1. Create the prescribed Log Metric:\ngcloud logging metrics create\n2. Create the prescribed Alert Policy:\ngcloud alpha monitoring policies create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-212":{"article":"Trace flags are frequently used to diagnose performance issues or to debug stored procedures or complex computer systems.","impact":"3625(trace log) Limits the amount of information returned to users who are not members of the sysadmin fixed server role by masking the parameters of some error messages using '******'. This can help prevent the disclosure of sensitive information.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the '3625' flag from the drop-down menu, and set its value to 'on'.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\n1. Configure the 3625 database flag for every Cloud SQL SQL Server database instance using the below command:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags \"3625=on\"\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-050":{"article":"Authorized networks are used to specify a restricted range of IP addresses that are permitted to access your container cluster's Kubernetes master endpoint. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your container cluster's Kubernetes master endpoint from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network.","impact":"Unauthorized networks give access to an unlimited range of IP addresses, which increases the level of outside attacks. It also increases the likelihood of leaking master certificates from your company premises.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to Kubernetes GCP Console using https://console.cloud.google.com/kubernetes/list?.\n2. Select the reported Kubernetes clusters for which Master authorized networks is disabled.\n3. Click on EDIT.\n4. Set 'Master authorized networks' to 'Enabled' and add authorize networks.\n5. Click SAVE.\nUsing Command Line:\nTo enable Master authorized networks for an existing cluster, run the following command:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --enable-master-authorized-networks\nAlong with this, you can list authorized networks using the --master-authorized-networks flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's Kubernetes master through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as 192.168.100.0/24).","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-213":{"article":"BigQuery by default encrypts the data at-rest by employing Envelope Encryption using Google managed cryptographic keys. The data is encrypted using data encryption keys, and the data encryption keys themselves are further encrypted using the key encryption keys. This is seamless and does not require any additional input from the user. However, if you want to have greater control, Customer-managed encryption keys (CMEK) can be used as the encryption key management solution for BigQuery Data Sets. The CMEK is used to encrypt the data encryption keys instead of using the google-managed encryption keys.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of your keys.","report_fields":["id"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nCurrently, there is no way to update the encryption of existing data in the table. The data  needs to be copied to either an original table or another table while specifying the customer managed encryption key (CMEK).\nFrom Command Line:\nUse the following command to copy the data.\nThe source and the destination needs to be same in case copying to the original table.\nbq cp --destination_kms_key <customer_managed_key>\nsource_dataset.source_table destination_dataset.destination_table","multiregional":true,"service":"BigQuery"},"ecc-gcp-229":{"article":"By default, the Auto-Delete rule is enabled for zonal persistent disks during virtual machine creation. When Auto-Delete is on, the persistent disks are deleted when the associated VM instance is deleted. However, for mission-critical Google Cloud VM instances and cloud environments where compliance and security requirements are more rigorous, you may need to retain the persistent disks after the instance termination. When Auto-Delete behavior rule is disabled, the zonal persistent disks attached to your VM instance are no longer removed when the instance is deleted.","impact":"If auto delete behavior rule is not disabled for persistent drives attached to your Google Cloud VM instances, VM data are not protected from being deleted.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances.\n2. In the navigation panel, select VM instances to access the list with all the Compute Engine instances provisioned forthe selected project.\n3. Click on the name of the virtual machine (VM) instance that you want to reconfigure.\n4. On the selected resource configuration page, click EDIT to enter the instance edit mode.\n5. In the Boot disk section, select Keep disk from When deleting instance dropdown list, to disable the Auto-Deletebehavior rule and keep the boot disk when the VM instance is terminated.\n6. In the Additional disks section, if the selected instance has additional disks attached, click on the disk box header,select Keep disk under Deletion rule, and click Done to close the configuration box. Repeat this step to disable theAuto-Delete behavior for all the required data disks attached.\n7. Click Save to apply the configuration changes.\nFrom Command Line:\n1. Run compute instances set-disk-auto-delete command using the name of the VM instance that you want to update asidentifier parameter and the name of the attached disk that you want to reconfigure as value for the --disk parameter, todisable the Auto-Delete behavior rule for the selected instance persistent disk:\ngcloud compute instances set-disk-auto-delete INSTANCE_NAME --zone ZONE --no-auto-delete --disk DISK\n2. If successful, the command output should return the compute instances set-disk-auto-delete request status:\nUpdated [https://www.googleapis.com/compute/v1/projects/PROJECT/zones/ZONE/instances/INSTANCE_NAME].","multiregional":true,"service":"Compute Engine"},"ecc-gcp-130":{"article":"This policy identifies Kubernetes Engine Clusters which are not configured with network traffic egress metering. When network traffic egress metering is enabled, a deployed DaemonSet pod meters network egress traffic by collecting data from the conntrack table and exports the metered metrics to the specified destination. It is recommended to use network egress metering to allow you to receive data and track the monitored network traffic.\nNOTE: Measuring network egress requires a network metering agent (NMA) running on each node. The NMA runs as a privileged pod, consumes some resources on the node (CPU, memory, and disk space), and enables the nf_conntrack_acct sysctl flag on the kernel (for connection tracking flow accounting). If you are comfortable with these caveats, you can enable network egress tracking for use with GKE usage metering.","impact":"When network traffic egress metering is disabled you can not track monitored network traffic to detect suspicious activity.","report_fields":["selfLink"],"remediation":"You cannot currently enable usage metering using Google Cloud Platform Console.\n1. To use usage metering for clusters in your Google Cloud Platform project, you first create the BigQuery dataset, and then configure clusters to use it.\n2. To enable usage metering on an existing cluster, run the following command.\nEnter the name of your cluster where you see a test-cluster and enter the name of your BigQuery dataset where you see test_usage_metering_dataset:\ngcloud beta container clusters update <CLUSTER_NAME> --resource-usage-bigquery-dataset <BIGQUERY_DATASET_NAME --enable-network-egress-metering","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-006":{"article":"Service Account keys consist of a key ID (Private_key_Id) and Private key which are used to sign programmatic requests you make to Google cloud services accessible to that particular Service account.\nIt is recommended that all Service Account keys are regularly rotated.","impact":"Not rotating Service Account keys periodically increases the probability of access keys being compromised, which could lead to unauthorized access to your GCP cloud resources.","report_fields":["name"],"remediation":"Delete any external (user-managed) Service Account Key older than 90 days:\n1. Go to APIs & Services/Credentials using https://console.cloud.google.com/apis/credentials\n2. In the Service Account Keys section, for every external (user-managed) service account key where creation date is greater than or equal to 90 days, click on the Delete Bin icon to delete Service Account key\nCreate a new external (user-managed) Service Account Key for a Service Account:\n1. Go to APIs & Services/Credentials using https://console.cloud.google.com/apis/credentials\n2. Click on Create Credentials and then on Select Service Account Key.\n3. Choose the service account in the drop-down list for which an External (user_x0002_managed) Service Account key needs to be created\n4. Select the desired key type format JSON or P12.\n5. Click on Create. It will download the private key. Keep it safe.\n6. Click on Close if prompted.\n7. The site will redirect you to the APIs & Services/Credentials page. Make a note of the  new ID displayed in the Service account keys section","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-012":{"article":"It is recommended to restrict API keys to use (call) only APIs required by an application.","impact":"Unrestricted keys are insecure because they can be viewed publicly, such as from within a browser, or they can be accessed on a device where the key resides.","report_fields":["displayName","name"],"remediation":"From Console:\n1. Go to APIs & Services\\Credentials using https://console.cloud.google.com/apis/credentials\n2. In the section API Keys, Click the API Key Name. The API Key properties display on a new page.\n3. In the Key restrictions section go to API restrictions.\n4. Click the Select API drop-down to choose an API.\n5. Click Save.\n6. Repeat steps 2,3,4,5 for every unrestricted API key\nNote: Do not set API restrictions to Google Cloud APIs, as this option allows access to all services offered by Google cloud.\nFrom Google Cloud CLI:\n1. List all API keys.\ngcloud services api-keys list\n2. Note the UID of the key to add restrictions to.\n3. Run the update command with the appropriate flags to add the required restrictions.\ngcloud alpha services api-keys update <UID> <restriction_flags>\nNote: Flags can be found by running - gcloud alpha services api-keys update --help\nor in this documentation https://cloud.google.com/sdk/gcloud/reference/alpha/services/api-keys/update","multiregional":true,"service":"Cloud APIs"},"ecc-gcp-335":{"article":"The Service account has an IAM policy containing 'iam.serviceAccounts.actAs' permissions that allow privilege escalation, at the resourse level. The existing permissions allow the Service account to impersonate a service account with higher permissions than their own. The Service account can then utilize that service account to perform API calls that the Service account may not be authorized to perform.","impact":"The permission allow the Service account to impersonate a service account with higher permissions than their own.","report_fields":["c7n:service-account.name","members"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/serviceaccounts.\n2. Click on reported servise account.\n3. Click permissions field.\n3. Click Edit for reported Service account.\n3. For all non-critical members of these roles, remove their membership by clicking the Trash icon on the right.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-307":{"article":"This rule detects when Artifact Registry repository is anonymously or publicly accessible.\nArtifact registry repositories can contain sensitive credentials that are baked into containers, personal data (like PII), or confidential data that you may not want publicly accessible. Repositories can be made anonymously or publicly accessible via IAM policies containing the IAM members allUsers or allAuthenticatedUsers.","impact":"Granting permissions to allUsers or allAuthenticatedUsers allows anyone to access the Artifact Registry repository. Such access might not be desirable if sensitive data is being stored in the repositories.","report_fields":["name"],"remediation":"From Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Repositories.\n3. Select the target Artifact Registry repository.\n4. Expand the Info Panel by selecting Show Info Panel.\n5. To remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Remove member.","multiregional":true,"service":"Dataproc"},"ecc-gcp-030":{"article":"GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions are met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances. Firewall rules are defined at the VPC network level, and are specific to the network in which they are defined. The rules themselves cannot be shared among networks.\nFirewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR notation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using SSH on Port 22 can be avoided.","impact":"Publicly exposed SSH access can increase opportunities for malicious activities such as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to VPC Network.\n2. Go to the Firewall Rules.\n3. Click the Firewall Rule you want to modify.\n4. Click Edit.\n5. Modify Source IP ranges to specific IP.\n6. Click Save.\nFrom CLI:\n1.Update Firewall rule with new SOURCE_RANGE from the below command:\ngcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...]","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-057":{"article":"Use Network Policy to restrict pod-to-pod traffic within a cluster and segregate workloads.","impact":"Disabling Network Policy can lead to unrestricted traffic between pods.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Select the cluster for which Network policy is disabled\n3. Click on EDIT.\n4. Set 'Network policy for master' to 'Enabled'.\n5. Click on SAVE.\n6. Once the cluster has updated, repeat steps 1-3.\n7. Set 'Network Policy for nodes' to 'Enabled'.\n8. Click on SAVE.\nUsing Command Line:\nThe Remediation script for this recommendation utilizes 2 variables $CLUSTER_NAME $COMPUTE_ZONE.\nPlease set these parameters on the system where you will be executing your gcloud audit script or command.\nTo enable Network Policy for an existing cluster, firstly enable the Network Policy add-on:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --update-addons NetworkPolicy=ENABLED\nThen, enable Network Policy:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --enable-network-policy","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-450":{"article":"If you have workloads that are fault tolerant, like HPC, big data, media transcoding, CI/CD pipelines or stateless web applications, using preemptible VMs to batch-process them can provide massive cost savings. \nUsing preemptible VMs in your architecture is a great way to scale compute at a discounted rate, but you need to be sure that the workload can handle the potential interruptions if the VM needs to be reclaimed.","impact":"Not using preemptible nodepool VMs in your architecture can result in missed opportunities to efficiently scale your compute resources while enjoying cost savings.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. In the list of clusters, click on the cluster requiring the update and click 'Add node pool'.\n3. Ensure that the 'Enable nodes on spot VMs' checkbox is checked.\n4. Click 'Save'.\nYou will also need to migrate workloads from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-015":{"article":"Log entries are held in Stackdriver Logging. It is recommended to create a sink which will export copies of all the log entries.","impact":"Without log aggregation, it may be hard to obtain logs in case of security incident investigation as logs may have been rotated or deleted.","report_fields":["projectId"],"remediation":"From Console:\n1. Go to Logs Router by visiting https://console.cloud.google.com/logs/router.\n2. Click on the arrow symbol with CREATE SINK text.\n3. Fill out the fields for Sink details.\n4. Choose Cloud Logging bucket in the Select sink destination drop down menu.\n5. Choose a log bucket in the next drop down menu.\n6. If an inclusion filter is not provided for this sink, all ingested logs will be routed to the destination provided above. This may result in higher than expected resource usage.\n7. Click Create Sink.\nFor more information, see https://cloud.google.com/logging/docs/export/configure_export_v2#dest-create.\nFrom Command Line:\nTo create a sink to export all log entries in a Google Cloud Storage bucket:\ngcloud logging sinks create <sink-name> storage.googleapis.com/DESTINATION_BUCKET_NAME\nSinks can be created for a folder or organization, which will include all projects.\ngcloud logging sinks create <sink-name> storage.googleapis.com/DESTINATION_BUCKET_NAME --include-children --folder=FOLDER_ID | --organization=ORGANIZATION_ID","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-266":{"article":"This rule detects when a service is configured with ingress allow all traffic. Restrict traffic from the internet and other resources to get better network-based access control and allow only VPC resources traffic to enter or traffic through the load balancer.","impact":"Allows all requests, including requests directly from the internet to the run.app URL that can can increase opportunities for malicious activities such as Distributed Denial of Service (DDoS) attacks.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. Select a service.\n3. Click on the Triggers tab.\n4. Under the Ingress label, select the ingress traffic you want to allow.\n5. Click on Save.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-178":{"article":"Ensure that the 'log_connections' setting causes each attempted connection to the server to be logged, along with successful completion of client authentication. This parameter cannot be changed after the session starts.","impact":"The 'log_connections' flag causes each attempted connection to the database instance to be logged, including successful client authentication requests. Not logging data generated by this configuration flag can lead to insufficient response time to identify, troubleshoot, and repair configuration errors and sub-optimal performance for your Google Cloud PostgreSQL database instances.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_connections' flag from the drop-down menu and set the value as 'on'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_connections' database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_connections=on\nNote: This command will overwrite all previously set database flags. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-293":{"article":"This rule detects when a Cloud Spanner backup is created with an expiration date of 29 days or less. Too short an expiration time reduces the benefit of backing up your data to protect against loss. A spanner backup should persist for at least 30 days or longer before it is replaced with a new backup, preferably on an automated cadence.","impact":"Too short an expiration time reduces the benefit of backing up your data to protect against loss.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Spanner page at Google Cloud Console: https://console.cloud.google.com/spanner/instances.\n2. Select an instance.\n3. Select a database for backup.\n4. Click Backup/Restore.\n5. Click on Create.\n6. Enter backup name.\n7. Enter backup expiration period, it should be with expiration time of at least 30 days\n8. Click Create.","multiregional":true,"service":"Cloud Spanner"},"ecc-gcp-291":{"article":"This policy identifies Secret Manager secrets that are not encrypted with customer managed encryption key.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of keys.","report_fields":["name"],"remediation":"From console:\n1. Go to the Secret Manager page in the Google Cloud Console.\n2. Make sure you have selected correct project at top.\n3. Select the violating secret from the list.\n4. On the secret details page, click Edit at top.\n5. For Encryption, check Use a customer-managed encryption key (CMEK) option and select an Encryption key.\n6. Click the Update Secret button.","multiregional":true,"service":"Secret Manager"},"ecc-gcp-043":{"article":"It is recommended to enforce all incoming connections to SQL database instance to use SSL.","impact":"Unencrypted traffic between the SQL database and client applications, if successfully intercepted (MITM), can expose sensitive data such as credentials, database queries, query outputs, etc.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to https://console.cloud.google.com/sql/instances.\n2. Click on an instance name to see its configuration overview.\n3. In the left-side panel, select Connections.\n4. In the SSL connections section, click Allow only SSL connections.\n5. Under Configure SSL server certificates, click Create new certificate.\n6. Under Configure SSL client certificates, click Create a client certificate.\n7. Follow the instructions shown to learn how to connect to your instance.\nFrom Command Line:\nTo enforce SSL encryption for an instance, run the command:\ngcloud sql instances patch INSTANCE_NAME --require-ssl\nNote:\nRESTART is required for type MySQL Generation 1 Instances (backendType FIRST_GEN) to get this configuration in effect.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-438":{"article":"It is recomended to enable Shielded GKE Nodes to provide verifiable integrity via secure boot, virtual trusted platform module (vTPM)-enabled measured boot, and integrity monitoring.","impact":"Clusters without GKE Shielded Nodes are potentially vulnerable to boot- or kernel-level malware or rootkits which persist beyond infected OS.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Navigate to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Select the cluster which you wish to enable Shielded GKE Nodes and Click EDIT.\n3. Locate the 'Shielded GKE Nodes' drop-down menu and select 'Enabled'.\n4. Click SAVE.\nUsing Command Line:\nTo migrate an existing cluster, you will need to specify the --enable-shielded-nodes flag on a cluster update command:\ngcloud beta container clusters update $CLUSTER_NAME \\\n--zone $CLUSTER_ZONE \\\n--enable-shielded-nodes","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-039":{"article":"Customer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. If you supply your own encryption keys, Google uses your key to protect the Google-generated keys used to encrypt and decrypt your data. By default, Google Compute Engine encrypts all data at rest. Compute Engine handles and manages this encryption for you without any additional actions on your part. However, if you want to control and manage this encryption yourself, you can provide your own encryption keys.","impact":"Not using CSEK instead of the standard Google Compute Engine encryption can cause problems if your project is compromised and all data will be disclosed. At least business-critical VMs should have VM disks encrypted with CSEK.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nCurrently there is no way to update the encryption of an existing disk. Therefore create a new disk with Encryption set to Customer supplied.\nFrom Console:\n1. Go to Compute Engine Disks by visiting https://console.cloud.google.com/compute/disks.\n2. Click CREATE DISK.\n3. Set Encryption type to Customer supplied.\n4. Provide the Key in the box.\n5. Select Wrapped key.\n6. Click Create.\nFrom CLI:\nIn the gcloud compute tool, encrypt a disk using the --csek-key-file flag during instance creation. If you are using an RSA-wrapped key, use the gcloud beta component:\ngcloud compute instances create INSTANCE_NAME --csek-key-file <example-file.json>\nTo encrypt a standalone persistent disk:\ngcloud compute disks create DISK_NAME --csek-key-file <examplefile.json>","multiregional":true,"service":"Compute Engine"},"ecc-gcp-187":{"article":"It is recommended to have all SQL database instances set to enable automated backups.","impact":"Backups provide a way to restore a Cloud SQL instance to recover lost data or recover from a problem with the instance. Automated backups need to be set for any instance containing data that should be protected from loss or damage.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the instance where the backups need to be configured.\n3. Click Edit.\n4. In the Backups section, check 'Enable automated backups', and choose a backup window.\n5. Click Save.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Enable Automated backups for every Cloud SQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --backup-start-time [HHMM]\nThe 'backup-start-time' parameter is specified in 24-hour time, in the UTC\u00b100 time zone, and specifies the start of a 4-hour backup window. Backups can start any time during the backup window.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-340":{"article":"This rule detects when Cloud Armor has packet size bypass.\nCloud Armor has a documented limitation of 8 KB as the maximum size of web request that it will inspect. The default behavior of Cloud Armor in this case can allow oversized malicious requests to bypass Cloud Armor and directly reach an underlying application. Morevoer, Cloud Armor does not warn users of this limitation during policy creation or when configuring rules from within the web UI, and can only find a reference to the 8 KB limit in the Cloud Armor documentation https://cloud.google.com/armor/docs/security-policy-overview.","impact":"Without an inbound rule where the Content-Length header value is equal to or greater than 8192, malicious oversized requests can bypass Cloud Armor and reach the application.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud Armor page in the Cloud Console https://console.cloud.google.com/net-security/securitypolicies/list.\n2. On the Security policies page, click reported policy.\n3. Click Add rules \n4. Set rule action: deny(403).\n5. Click Advanced mode and type: int(request.headers['content-length']) >= 8192.\n6. Set desired priority.\n6. Click Add.","multiregional":true,"service":"Cloud Armor"},"ecc-gcp-177":{"article":"Ensure that the 'log_checkpoints' database flag for the Cloud SQL PostgreSQL instance is set to 'on'.","impact":"In most cases, checkpoints are disrupting your Google Cloud PostgreSQL database performance and can cause connections to stall for up to a few seconds while they occur. By disabling the 'log_checkpoints' flag, you cannot get verbose logging of the checkpoint process for your PostgreSQL database instances to identify and troubleshoot sub-optimal PostgreSQL database performance.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the flag 'log_checkpoints' from the drop-down menu, and set its value.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_checkpoints' database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_checkpoints=on\nNote: This command will overwrite all previously set database flags. To keep those and add new ones, include the values for all flags to be set on the instance. Any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-058":{"article":"Disable Client Certificates that require certificate rotation for authentication. Instead, use another authentication method like OpenID Connect.","impact":"Client Certificate authentication requires manual key management and rotation. Such manual actions can be easily forgotten.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Click on CREATE CLUSTER.\n3. Configure as required and then click on the 'Availability, networking, security, and additional features' section.\n4. Ensure that the 'Issue a client certificate' checkbox is not ticked.\n5. Click on CREATE.\nUsing Command Line:\nCreate a new cluster without a Client Certificate:\ngcloud container clusters create [CLUSTER_NAME] --no-issue-client-certificate","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-302":{"article":"This rule detects when Cloud Armor policy does not prevent message lookup in Log4j2. \nUsing a vulnerable version of Apache Log4j library might enable attackers to exploit a Lookup mechanism that supports making requests using special syntax in a format string.\nSet your Cloud Armor to prevent executing such mechanism using remediation.","impact":"Not using security policy that prevents message lookup in Log4j2 can potentially lead to a risky code execution, data leakage and more.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud Armor page in the Cloud Console https://console.cloud.google.com/net-security/securitypolicies/list.\n2. On the Security policies page, click reported policy.\n3. Click Add rules \n4. Set rule action: deny(403).\n5. Click Advanced mode and type: evaluatePreconfiguredExpr('cve-canary').\n6. Set desired priority.\n6. Click Add.","multiregional":true,"service":"Cloud Armor"},"ecc-gcp-181":{"article":"The log_min_messages flag defines the minimum message severity level that is considered as an error statement. Messages for error statements are logged with the SQL statement. Valid values include DEBUG5, DEBUG4, DEBUG3, DEBUG2, DEBUG1, INFO, NOTICE, WARNING, ERROR, LOG, FATAL, and PANIC. Each severity level includes the subsequent levels mentioned above.\nNote: To effectively turn off logging failing statements, set this parameter to PANIC.\nERROR is considered the best practice setting. Changes should only be made in accordance with the organization's logging policy.\nBy default log_min_error_statement is ERROR.","impact":"If log_min_messages is not set to the correct value, messages may not be classified as error messages appropriately. Considering general log messages as error messages would make it difficult to find actual errors.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console by visiting https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the flag log_min_messages from the drop-down menu and set appropriate value.\n6. Click Save to save the changes.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database Instances using the following command:\ngcloud sql instances list\n2. Configure the log_min_messages database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_min_messages=<DEBUG5|DEBUG4|DEBUG3|DEBUG2|DEBUG1|INFO|NOTICE|WARNING|ERROR|LOG|FATAL|PANIC>\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-264":{"article":"This rule detects when a revision doesn't contain a VPC connector. VPC connectors helps Cloud Run (fully managed) service access Compute Engine VM instances, Memorystore instances, and any other resources with an internal IP address.","impact":"Not using a VPC connector can increase the risk of compromising resources from the Internet.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. Select a service.\n3. Click on Edit and Deploy New Revision.\n4. Click on Connections, click the VPC Connector dropdown and select a connector to use.\n5. Select Serve this revision immediately.\n6. Click on Deploy.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-082":{"article":"SQL instance should have Retention Policies for Backups configured to retain at least 7 days of backups.","impact":"A retention period set for database instances less than 7 days could result in data loss and inability to recover it in the event of failure.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to SQL.\n2. Click the instance.\n3. Click the Backups tab.\n4. Click Create backup\nor\n1. Go to SQL.\n2. Select the instance for which you want to configure backups.\n3. Click Edit.\n4. In the Auto backups section, select Automate backups, and choose the backup window.\n5. Click Save. Link https://cloud.google.com/sql/docs/mysql/backup-recovery/backing-up.\nVia Command Line:\n1. Scheduling automated backups:\ngcloud sql instances patch [INSTANCE_NAME] --backup-start-time [HHMM]","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-209":{"article":"It is recommended to check the user connections for a Cloud SQL SQL Server instance to ensure that it is not artificially limiting connections.\nThe user connections option specifies the maximum number of simultaneous user connections that are allowed on an instance of SQL Server. The actual number of user connections allowed also depends on the version of SQL Server that is used, and also the limits of your application or applications and hardware. It is recommended to set 'user connections' database flag for Cloud SQL SQL Server instance according the organization-defined value.","impact":"artificially limiting the connection can lead to problems with connecting to the database for some users","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'user connections' flag from the drop-down menu, and set its value to your organization-recommended value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. Configure the user connections database flag for every Cloud SQL SQL Server database instance using the below command:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags \"user connections=[0-32,767]\"\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-162":{"article":"The information life cycle includes information creation, collection, use, processing, storage, maintenance, dissemination, disclosure, disposition. Many datasets contain information about individuals that can be used to distinguish or trace an individual\u2019s identity, such as name, social security number, date and place of birth, mother\u2019s maiden name, or biometric records. Datasets may also contain other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information. Personally identifiable information is removed from datasets by trained individuals when such information is not (or no longer) necessary to satisfy the requirements envisioned for the data. For example, if the dataset is only used to produce aggregate statistics, the identifiers that are not needed for producing those statistics are removed. Removing identifiers improves privacy protection, since information that is removed cannot be inadvertently disclosed or improperly used.","impact":"Without the lifecycle configuration, you are missing out on the opportunity to manage objects so that they are stored cost-effectively throughout their lifecycle by moving data to more economical storage classes over time or expiring data based on the object age.","report_fields":["selfLink"],"remediation":"From GCP Console:\n1. Open the Cloud Storage browser in Google Cloud Console.\n2. In the bucket list, find the bucket you want to enable and click 'None' in the 'Lifecycle' column. The lifecycle rules page appears.\n3. Click 'Add rule'.\n4. In the page that appears, specify a configuration:\na) Select the condition(s) under which an action is taken.\nb) Click 'Continue'.\nc) Select the action to take when an object meets the condition(s).\nd) Click 'Continue'.\ne) Click 'Save'.\nGSUTIL:\n1. Create a .json file with the lifecycle configuration rules you would like to apply (see examples).\n2. Use the lifecycle set command to apply the configuration:\ngsutil lifecycle set [LIFECYCLE_CONFIG_FILE] 'gs://[BUCKET_NAME]'\nWhere:\n[LIFECYCLE_CONFIG_FILE] is the name of the file you created in Step 1.\n[BUCKET_NAME] is the name of the relevant bucket. For example, my-bucket.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-323":{"article":"The Service account has an IAM policy containing 'iam.serviceAccounts.actAs' permissions that allow privilege escalation, at the project level. The existing permissions allow the service account to impersonate another service account with higher permissions than their own. The service account can then utilize that service account to perform API calls that the user may not be authorized to perform.","impact":"The permission allow the service account to impersonate another service account with higher permissions than their own.","report_fields":["member","roles"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/iam.\n2. Click Edit for reported service account.\n3. For all non-critical members of these roles, remove their membership by clicking the Trash icon on the right.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-013":{"article":"It is recommended to rotate API keys every 90 days.","impact":"Once a key is stolen, it has no expiration, meaning it may be used indefinitely unless the project owner revokes or regenerates the key.","report_fields":["displayName","name"],"remediation":"From Console:\n1. Go to APIs & Services\\Credentials using https://console.cloud.google.com/apis/credentials\n2. In the section API Keys, Click the API Key Name. The API Key properties display on a new page.\n3. Click REGENERATE KEY to rotate API key.\n4. Click Save.\n5. Repeat steps 2,3,4 for every API key that has not been rotated in the last 90 days.\nNote: Do not set HTTP referrers to wild-cards (* or *.[TLD] or .[TLD]/) allowing access to any/wide HTTP referrer(s). Do not set IP addresses and referrer to any host (0.0.0.0 or 0.0.0.0/0 or ::0)\nFrom Google Cloud CLI:\nThere is not currently a way to regenerate and API key using gcloud commands. To 'regenerate' a key you will need to create a new one, duplicate the restrictions from the key being rotated, and delete the old key.\n1. List existing keys\ngcloud services api-keys list\n2. Note the UID and restrictions of the key to regenerate.\n3. Run this command to create a new API key.\n<key_name> is the display name of the new key.\ngcloud alpha services api-keys create --display-name=\"<key_name>\"\nNote the UID of the newly created key.\n4. Run the update command to add required restrictions.\nNote: The restriction may vary for each key. Refer to this documentation for the appropriate flags: https://cloud.google.com/sdk/gcloud/reference/alpha/services/api-keys/update\ngcloud alpha services api-keys update <UID of new key>\n5. Delete the old key\ngcloud alpha services api-keys delete <UID of old key>","multiregional":true,"service":"Cloud APIs"},"ecc-gcp-386":{"article":"Confidential Computing is a breakthrough technology which encrypts data in-use while it is being processed. Confidential Computing environments keep data encrypted in memory and elsewhere outside the central processing unit (CPU).\nConfidential VMs leverage the Secure Encrypted Virtualization (SEV) feature of AMD EPYC CPUs. Customer data will stay encrypted while it is used, indexed, queried, or trained on. Encryption keys are generated in hardware, per VM, and are not exportable. Thanks to built-in hardware optimizations of both performance and security, there is no significant performance penalty to Confidential Computing workloads.","impact":"Confidential VM can help alleviate concerns about risk related to either dependency on Google infrastructure or open access by Google Insiders to customer data.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nWe can't edit an existing Instance Template. Delete the existing one and create a new one with Confidential Computing enabled using the link https://cloud.google.com/compute/docs/instance-templates/create-instance-templates","multiregional":true,"service":"Compute Engine"},"ecc-gcp-077":{"article":"You can add a user account to your created SQL instance. During the account creation, it is recommended that you avoid using names like 'admin' or 'administrator', which are potential targets for brute-force dictionary attacks.","impact":"Names like 'admin' or 'administrator' are targeted brute force dictionary attacks.","report_fields":["instance"],"remediation":"From Console:\n1. Go to SQL.\n2. Click the instance you want to modify.\n3. Click the USERS tab.\n4. Don't use name (admin, Admin, administrator, Administrator) for user account.\nFrom Command Line:\n1. Creating a user:\ngcloud sql users create [USER_NAME] --host=[HOST] --instance=[INSTANCE_NAME] --password=[PASSWORD]","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-194":{"article":"Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management.","impact":"Enabling osLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to an IAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and automated SSH key pair management, which is useful in handling cases like response to compromised SSH key pairs and/or revocation of external/third-party/Vendor users.","report_fields":["selfLink"],"remediation":"Set 'enable-oslogin' in project-wide metadata so that it applies to all of the instances in your project\nFrom Console:\n1. Go to the VM compute metadata page using https://console.cloud.google.com/compute/metadata.\n2. Click Edit.\n3. Add a metadata entry where the key is 'enable-oslogin' and the value is 'TRUE'.\n4. Click Save to apply the changes.\n5. For every instances that overrides the project setting, go to the VM Instances page at https://console.cloud.google.com/compute/instances.\n6. Click the name of the instance on which you want to remove the metadata value.\n7. At the top of the instance details page, click Edit to edit the instance settings.\n8. Under Custom metadata, remove any entry where the key is 'enable-oslogin' and the value is 'FALSE'.\n9. At the bottom of the instance details page, click Save to apply your changes to the instance.\nFrom Command Line:\n1. Configure oslogin on the project:\ngcloud compute project-info add-metadata --metadata enable-oslogin=TRUE\n2. Remove instance metadata that overrides the project setting:\ngcloud compute instances remove-metadata INSTANCE_NAME --keys=enable-oslogin","multiregional":true,"service":"Compute Engine"},"ecc-gcp-066":{"article":"Accidental unavailability of a key vault can cause immediate data loss or loss of security functions (authentication, validation, verification, non-repudiation, etc.) supported by the key vault objects.\nIt is recommended the key vault be made recoverable by enabling the 'Do Not Purge' and 'Soft Delete' functions. This is in order to prevent the loss of encrypted data including storage accounts, SQL databases, and/or dependent services provided by key vault objects (Keys, Secrets, Certificates) etc., as may happen in the case of accidental deletion by a user or from disruptive activity by a malicious user.","impact":"Accidental unavailability of a key vault can cause immediate data loss or loss of security functions supported by the key vault objects.","report_fields":["name"],"remediation":"From Console:\n1. Go to Security.\n2. Go to Cryptographic Keys.\n3. Check status Not avaliable.\n4. Create a new key to replace the unavailable one using the following link https://cloud.google.com/kms/docs/creating-keys#kms-create-keyring-console","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-007":{"article":"Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be able to complete a malicious action. No user should have Service Account Admin and Service Account User roles assigned at the same time.\nIt is recommended that the principle of 'Separation of Duties' is enforced while assigning service account related roles to users.","impact":"Without Separation of duties, any individual can have all the necessary permissions to perform a malicious action, such as, for example, using a key to access and decrypt data.","report_fields":["member","roles"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/iam.\n2. For any member having both Service Account Admin and Service account User rolesgranted/assigned, click on the Delete Bin icon to remove either role from the member.\nRemoval of a role should be done based on the business requirements.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-143":{"article":"Primitive roles are the roles that existed prior to Cloud IAM. Primitive roles (owner, editor) are built-in and provide a broader access to resources making them prone to attacks and privilege escalation. Predefined roles provide more granular controls than primitive roles. Therefore, predefined roles should be used.","impact":"Primitive IAM roles grant over-privileged cloud identities in your Google Cloud projects, which can lead to unwanted or unauthorized access to your GCP cloud resources.","report_fields":["member","roles"],"remediation":"Review the projects/resources that have Primitive roles assigned to them and replace them with equivalent Predefined roles.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-129":{"article":"This policy identifies Firewall rules attached to the cluster network which allows inbound traffic on all protocols from the public internet. Doing so may allow a bad actor to brute-force their way into the system and potentially get access to the entire cluster network.","impact":"Allowing unrestricted ingress access to uncommon ports can increase opportunities for malicious activities such as hacking and various types of attacks (brute-force attacks, Denial of Service (DoS) attacks, etc.) that may lead to data loss.","report_fields":["selfLink"],"remediation":"1. Login to GCP Portal\n2. Go to VPC network (Left Panel)\n3. Select Firewall rules\n4. Click on the reported firewall rule\n5. Click on the 'EDIT' button\n6. Change the 'Source IP ranges' other than '0.0.0.0/0'\n7. Click on 'Save'","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-248":{"article":"This rule detects when a function is not configured with event trigger. Events are things that happen within your cloud environment that you might want to take action on. These might be changes to data in a database, files added to a storage system, or a new virtual machine instance being created. Creating a response to an event is done with a trigger. A trigger is a declaration that you are interested in a certain event or set of events. Binding a function to a trigger allows you to capture and act on events.","impact":"Without an event trigger, you won't be able to instantly capture and respond to suspicious events within your cloud environment.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions Overview page. https://console.cloud.google.com/functions\n2. Click on 'create function'\n3. In Trigger section, choose a trigger.\n4. For each trigger fill the required information.\n5. Click save","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-046":{"article":"To minimize attack surface root access can explicitly allowed from only trusted IPs (Hosts) to support database related administrative tasks.\nIt is recommended that root access to a MySql Database Instance be allowed only through specific white-listed trusted IPs.","impact":"When root access is allowed for any host, any host from authorized networks can attempt to authenticate to a MySql Database Instance using administrative privileges.","report_fields":["instance"],"remediation":"From Command Line:\nNote: We haven't come across any setting provided by Google cloud console or gcloud utility to update host for a root user. Below remediation uses myMySql-client binary to set the host for root user. Similarly, for PostgreMySql instance,\n1. Login to MySql database instance from authorized network\nmysql connect -u root [INSTANCE_ADRESS]\n2. Set host for root user\nUPDATE MySql.user SET Host=[Host_name/IP] WHERE User='root';","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-091":{"article":"Ensure that CDN is enabled only on a Load Balancer with an HTTPS protocol on backend services.","impact":"The unencrypted traffic between the edge servers and the custom origin can be disclosed by malicious users in case they are able to capture packets sent across CDN.","report_fields":["selfLink"],"remediation":"From Console:\n1. Ensure that CDN is enabled on the Load balancer.\n2. On the Network Security page, from the Console, configure the SSL policy.\n3. Go to the Load balancing page in the Google Cloud Platform Console.\n4. Find the target load balancer name.\n5. Click on the Edit pencil.\n6. Configure Backend.\n7. Configure Frontend.\n8. Attach policy to the Frontend HTTPS proxy.\n9. Configure Certificate.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-088":{"article":"On Google's Global HTTP Load Balancer, each HTTPS target proxy is linked to a certificate. Ensure that SSL/TLS certificates are renewed one month before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and a GCP resource that implements the certificates is no longer secure.","report_fields":["selfLink"],"remediation":"In the Google Cloud Platform Console, you create a new SSL certificate resource when you create or edit a frontend for an HTTPS or SSL proxy load balancer.\nTo create a new SSL certificate resource for a frontend:\n1. Go to the Load balancing page in the Google Cloud Platform Console.\n2.Click the name of an HTTPS or SSL proxy load balancer.\n3.Click the Edit pencil.\n4.Select Frontend Configuration.\n-For an existing frontend, click the Edit pencil. In the Certificate drop-down menu, select a visible certificate, then click Create a new certificate.\n-For a new frontend, in the Certificate drop-down menu, click Select a certificate, and then click Create a new certificate.\n5.In the Create a new certificate dialog box, enter a Name for your certificate resource, and optionally click Add a description and enter a description.\n6.Choose Upload my certificate.\n7.In the Public key certificate field, click the Upload button to upload your .crt file or paste the entire contents of your .key file into the field, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- that enclose the file contents.\n8.In the Certificate chain field, click the Upload button to upload your .csr file or paste the entire contents of the .csr file into the field, including the -----BEGIN CERTIFICATE  REQUEST----- and -----END CERTIFICATE REQUEST-----  that enclose the file contents.\n9.In the Private key certificate field, click the Upload button to upload your private key, using the .key file generated previously. This file uses, for example, -----BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- to enclose the file contents.\n10.Click Create.\n11.To create additional certificates, click the additional certificates link, click Select a Certificate, and then click Create a new certificate. Follow the previous steps to upload or paste the appropriate files.\nRefer to: https://cloud.google.com/load-balancing/docs/ssl-certificates.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-205":{"article":"The PostgreSQL executor is responsible for executing the plan handed over by the PostgreSQL planner. The executor processes the plan recursively to extract the required set of rows. The 'log_executor_stats' flag controls the inclusion of PostgreSQL executor performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_executor_stats' flag enables a crude profiling method for logging PostgreSQL executor performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_executor_stats' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_executor_stats database flag for every Cloud SQL PosgreSQL database instance using the below command.\ngcloud sql instances patch INSTANCE_NAME --database-flags log_executor_stats=off\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-027":{"article":"Cloud DNS is a fast, reliable and cost-effective Domain Name System that powers millions of domains on the internet. DNSSEC in Cloud DNS enables domain owners to take easy steps to protect their domains against DNS hijacking and man-in-the-middle and other attacks.","impact":"Attackers can hijack the process of domain/IP lookup and redirect users to a malicious site using DNS hijacking and man-in-the-middle attacks. DNSSEC helps mitigate the risk of such attacks by cryptographically signing DNS records.","report_fields":["id","name"],"remediation":"From Console:\n1. Go to Cloud DNS by visiting https://console.cloud..google.com/net-services/dns/zones.\n2. For each zone of Type Public, set DNSSEC to On.\nFrom Command Line:\nUse the below command to enable DNSSEC for Cloud DNS Zone Name.\ngcloud dns managed-zones update ZONE_NAME --dnssec-state on","multiregional":true,"service":"Cloud DNS"},"ecc-gcp-038":{"article":"Compute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP address of the instance. Similarly, GCP won't deliver a packet whose destination IP address is different than the IP address of the instance receiving the packet. However, both capabilities are required if you want to use instances to help route packets. Forwarding of data packets should be disabled to prevent data loss or information disclosure.\nException:\nInstances created by GKE should be excluded because they need to have IP forwarding enabled and cannot be changed. Instances created by GKE have names that start with 'gke-'.","impact":"When IP forwarding is enabled, Google Cloud does not enforce packet source and destination verification, and the security of this redirect is potentially downgraded as this source and destination may refer to the attacker's ones.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue when canIpForward is set to false.\nFrom Console:\n1. Go to the VM Instances page using https://console.cloud.google.com/compute/instances.\n2. Select the VM Instance you want to remediate.\n3. Click the Delete button.\n4. On the 'VM Instances' page, click 'CREATE INSTANCE'.\n5. Create a new instance with the desired configuration. By default, the instance is configured to not allow IP forwarding.\nFrom CLI:\n1. Delete the instance:\ngcloud compute instances delete INSTANCE_NAME.\n2. Create a new instance to replace it. Set IP forwarding to Off:\ngcloud compute instances create","multiregional":true,"service":"Compute Engine"},"ecc-gcp-138":{"article":"This policy identifies Load Balancer HTTPS target proxies that are not configured with QUIC protocol. Enabling QUIC protocol on Load balancer target HTTPS proxies adds such advantages as faster connections establishing, stream-based multiplexing, improved loss recovery, and elimination of head-of-line blocking.","impact":"If QUIC's key features are disabled you can not include establishing connections faster, stream-based multiplexing, improved loss recovery, and no head-of-line blocking. With QUIC protocol disabled, such advantages as faster connection establishing, stream-based multiplexing, improved loss recovery, and elimination of head-of-line blocking are not available.","report_fields":["selfLink"],"remediation":"1. Login to GCP Portal.\n2. Go to Network services (Left Panel).\n3. Select Load balancing.\n4. Click on the 'Advanced menu' hyperlink to view target proxies.\n5. Click on the 'Target proxies' tab.\n6. Click on the reported HTTPS target proxy.\n7. Click on the hyperlink under 'URL map'.\n8. Click on the 'EDIT' button.\n9. Select 'Frontend configuration', click on HTTPS protocol rule.\n10. For 'QUIC negotiation', select 'Enabled' from the drop-down list.\n11. Click on 'Done'.\n12. Click on 'Update'.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-292":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned an IAM role in the Cloud Spanner backup resource. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Spanner page at Google Cloud Console: https://console.cloud.google.com/spanner/instances.\n2. Select an instance from Resources.\n3. Click Backup.\n4. Select a backup.\n5. Review each role and find Members having editor or owner access.\n6. Click Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Spanner"},"ecc-gcp-295":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned an instance-level IAM role in the Cloud Spanner service. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Spanner page at Google Cloud Console: https://console.cloud.google.com/spanner/instances.\n2. Select an instance.\n3. Review each role and find Members having editor or owner access.\n4. Click Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Spanner"},"ecc-gcp-241":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned a function-level IAM role in the Cloud Functions service. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a function.\n3. Click on Permission.\n4. Review each role and find Members with Owner or Editor access.\n5. Click the Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-010":{"article":"Ensure API keys are not created for a project.","impact":"API Keys are insecure because they can be viewed publicly, such as from within a browser, or they can be accessed on a device where the key resides. Keys should only be used for services in cases where other authentication methods are unavailable.","report_fields":["displayName","name"],"remediation":"From Console:\n1. Go to APIs & Services\\Credentials using https://console.cloud.google.com/apis/credentials\n2. In the section API Keys, to delete API Keys: Click the Delete Bin Icon in front of every API Key Name.\nFrom Google Cloud Command Line:\n1. Run the following from within the project you wish to audit\ngcloud services api-keys list --filter\n2. **Pipe the results into**\ngcloud alpha services api-keys delete","multiregional":true,"service":"Cloud APIs"},"ecc-gcp-257":{"article":"This policy identifies GCP App Engine applications for which Identity-Aware Proxy(IAP) is disabled.  IAP is used to enforce access control policies for applications and resources. It works with signed headers or the App Engine standard environment Users API to secure your app. It is recommended to enable Identity-Aware Proxy for securing the App engine.\nReference: https://cloud.google.com/iap/docs/concepts-overview","impact":"When you don't use IAP, users are not subject to the fine-grained access controls implemented by the product in use and when they try to access a resource, IAP not perform authentication and authorization checks.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Identity-Aware Proxy page https://console.cloud.google.com/security/iap.\n2. If you don't already have an active project, you'll be prompted to select the project you want to secure with IAP. Select the project to which you deployed the sample application.\nIf you haven't configured your project's OAuth consent screen, you'll be prompted to do so. An email address and product name are required for the OAuth consent screen.\nConfiguring the OAuth consent screen:\n1. Go to the OAuth consent screen https://console.cloud.google.com/apis/credentials/consent.\n2. Under Support email, select the email address you want to display as a public contact. This email address must be your email address, or a Google Group you own.\n3. Enter the Application name you want to display.\n4. Add any optional details you'd like.\n5. Click Save.\nTo change information on the OAuth consent screen later, such as the product name or email address, repeat the preceding steps to configure the consent screen.\nSetting up IAP access:\n1. Go to the Identity-Aware Proxy page https://console.cloud.google.com/security/iap.\n2. Select the resource you wish to modify by checking the box to its left. On the right side panel, click Add Member.\n3. In the Add members dialog, add the email addresses of groups or individuals to whom you want to grant the IAP-secured Web App User role for the project.\nThe following kinds of accounts can be members:\n  - Google Account: user@gmail.com\n  - Google Group: admins@googlegroups.com\n  - Service account: server@example.gserviceaccount.com\n  - G Suite domain: example.com\nMake sure to add a Google Account that you have access to.\nIn order to make a resource publicly-accessible (while sibling resources are restricted), grant the IAP-secured Web App User role to `allUsers` or `allAuthenticatedUsers`. The difference between these two is explained in the Public access section https://cloud.google.com/iap/docs/managing-access#public_access.\n4. When you're finished adding members, click Add.\nTurning on IAP:\n1. On the Identity-Aware Proxy page, under HTTPS Resources, find the App Engine app you want to restrict access to. The Published column shows the URL of the app. To turn on IAP for the app, toggle the on/off switch in the IAP column.\n2. To confirm that you want IAP to secure the application, click Turn On in the Turn on IAP window that appears. After you turn it on, IAP requires login credentials for all connections to your application.\nFor more information https://cloud.google.com/iap/docs/app-engine-quickstart.","multiregional":true,"service":"App Engine"},"ecc-gcp-221":{"article":"The default Compute Engine service account has Editor privileges to the product, and shouldn't be associated with a instance template.","impact":"Using default Compute Engine service account associated with a instance template could allow an attacker to use extended account permissions.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nDelete affected instance template and then create a new one with non-default service account.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-005":{"article":"It is recommended to assign the Service Account User (iam.serviceAccountUser) and Service Account Token Creator (iam.serviceAccountTokenCreator) roles to a user for a specific service account rather than assigning the role to a user at project level.","impact":"Users with IAM roles to update the App Engine and Compute Engine instances can effectively run code as the service accounts used to run these instances, and indirectly gain access to all the resources for which the service accounts have access.","report_fields":["role","members"],"remediation":"From Console:\n1. Go to the IAM page in the GCP Console by visiting  https://console.cloud.google.com/iam-admin/iam\n2. Click on the filter table text bar. Type Role Service Account User\n3. Click the Delete Bin icon in front of the role Service Account User for every user  listed as a result of a filter\n4. Click on the filter table text bar. Type Role Service Account Token Creator\n5. Click the Delete Bin icon in front of the role Service Account Token Creator for  every user listed as a result of a filter\nFrom Command Line:\n1. Using a text editor, remove the bindings with the roles/iam.serviceAccountUser\nFor example, you can use the iam.json file shown below as follows:\n{\n  \"bindings\": [\n    {\n      \"members\": [\n        \"serviceAccount:our-project-123@appspot.gserviceaccount.com\",\n      ],\n      \"role\": \"roles/appengine.appViewer\"\n    },\n    {\n      \"members\": [\n        \"user:email1@gmail.com\"\n      ],\n      \"role\": \"roles/owner\"\n    },\n    {\n      \"members\": [\n        \"serviceAccount:our-project-123@appspot.gserviceaccount.com\",\n        \"serviceAccount:123456789012-compute@developer.gserviceaccount.com\"\n      ],\n        \"role\": \"roles/editor\"\n    }\n  ],\n    \"etag\": \"BwUjMhCsNvY=\"\n}\nor roles/iam.serviceAccountTokenCreator.\n2. Update the project's IAM policy:\ngcloud projects set-iam-policy PROJECT_ID iam.json","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-120":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the SMTP port (25) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 25 (SMTP) using VPC network firewall rules can increase opportunities for malicious activities such as hacking, spamming, Shellshock and Distributed Denial-of-Service (DDoS) attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-300":{"article":"This rule detects when redis instance has AUTH disabled.\nAUTH is an optional security feature on Memorystore for Redis that requires incoming connections to authenticate with an AUTH string. Every AUTH string is a Universally Unique Identifier (UUID), and each Redis instance with AUTH enabled has a unique AUTH string.","impact":"Without AUTH you can not ensure that known entities in your organization do not unintentionally access and modify your Redis instance.","report_fields":["name"],"remediation":"From Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Memorystore for Redis.\n3. View your instance's Instance details page by clicking on reported Instance ID.\n4. Select the EDIT button.\n5. Scroll to the Security section and select the checkbox for Enable AUTH.","multiregional":true,"service":"Cloud Memorystore"},"ecc-gcp-283":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the SQL server port (1433) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 1433 (SQL server) via VPC network firewall rules can increase opportunities for malicious activities such as hacking, brute-force attacks, and SQL injection attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-011":{"article":"It is recommended to restrict API key usage to trusted hosts, HTTP referrers and apps.","impact":"Unrestricted keys are insecure because they can be viewed publicly, such as from within a browser, or they can be accessed on a device where the key resides.","report_fields":["displayName","name"],"remediation":"From Console:\nLeaving Keys in Place\n1. Go to APIs & Services\\Credentials using https://console.cloud.google.com/apis/credentials\n2. In the section API Keys, Click the API Key Name. The API Key properties display on a new page.\n3. In the Key restrictions section, set the application restrictions to any of HTTP referrers, IP Adresses, Android Apps, iOs Apps.\n4. Click Save.\n5. Repeat steps 2,3,4 for every unrestricted API key.\nNote:\nDo not set HTTP referrers to wild-cards (* or *.[TLD] or .[TLD]/) allowing access to any/wide HTTP referrer(s).\nDo not set IP addresses and referrer to any host (0.0.0.0 or 0.0.0.0/0 or ::0)\nRemoving Keys\nAnother option is to remove the keys entirely.\n1. Go to APIs & Services\\Credentials using https://console.cloud.google.com/apis/credentials\n2. In the section API Keys, select the checkbox next to each key you wish to remove.\n3. Select Delete and confirm.","multiregional":true,"service":"Cloud APIs"},"ecc-gcp-265":{"article":"This rule detects when a service account with elevated privileges (editor or owner status) is assigned a service-level IAM role in the Cloud Run service. An elevated service account has more access than necessary and doesn't meet least privilege standards. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"An elevated service account has more access than necessary and it increases the level of compromise in the event of a security breach.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. Select a service.\n3. Click on Show Info Panel in the top right corner to show the Permissions tab.\n4. Click on the delete button to remove the service account or Edit the service acocunt role.\n5. Click on Save.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-121":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the Telnet port (23) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted Telnet access can increase opportunities for malicious activities such as IP address spoofing, man-in-the-middle attacks (MITM), and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-245":{"article":"This rule detects when a function is configured with a default service account, function should be configured with service account with least privileges according to requirement. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"Using the default App Engine service account can lead to privilege escalations. If your Cloud Function is compromised, it allows attackers broad access to many Google Cloud services.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a Function.\n3. Click on Edit.\n4. Change the default service account.\n5. Click on next.\n6. Click on Deploy.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-144":{"article":"Check to verify if the configuration for automated backup of Binary logs is enabled. Restoring from a backup reverts your MySQL instance to its state at the backup creation time. Enabling automated backups creates backup during the scheduled backup window.","impact":"If a backup is not enabled, there is a risk of data loss after accidental or targeted deletion beyond recovery.","report_fields":["selfLink"],"remediation":"1. Login to GCP Console and select 'SQL' from 'Storage'.\n2. Select the identified MySQL instance.\n3. Click on the 'BACKUPS' tab.\n4. Click on 'Setting Edit', enable the 'Automate backups' and 'Enable point-in-time-recovery' option and click on 'Save'.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-009":{"article":"Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be able to complete a malicious action. No user should have Service Account Admin and Service Account User roles assigned at the same time.\nIt is recommended that the principle of 'Separation of Duties' is enforced while assigning KMS related roles to users.","impact":"Without Separation of duties, any individual can have all the necessary permissions to perform a malicious action, such as, for example, using a key to access and decrypt data.","report_fields":["member","roles"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/iam.\n2. For any member having Cloud KMS Admin and any of the Cloud KMS CryptoKey Encrypter/Decrypter, Cloud KMS CryptoKey Encrypter, Cloud KMS CryptoKey Decrypter roles granted/assigned, click the Delete Bin icon to remove the role from the member.\nNote: Removing a role should be done based on the business requirement.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-016":{"article":"It is recommended to enable object versioning on log-buckets as the feature allows you to preserve, retrieve, and restore versions of objects.","impact":"With the Object Versioning feature disabled, Google Cloud Storage buckets cannot recover from both unintended user actions and application failures.","report_fields":["selfLink"],"remediation":"From Console:\n1. If sinks are not configured, first follow the instructions in the recommendation:\nEnsure that sinks are configured for all Log entries.\n2. For each storage bucket configured as a sink, go to the Cloud Storage browser at https://console.cloud.google.com/storage/browser/<BUCKET_NAME>.\n3. Select the Bucket Lock tab near the top of the page.\n4. In the Retention policy entry, click the Add Duration link. The Set a retention policy dialog box appears.\n5. Enter the desired length of time for the retention period and click Save policy.\n6. Set the Lock status for this retention policy to Locked.\nFrom Command Line:\n1. To list all sinks destined to storage buckets:\ngcloud logging sinks list --folder=FOLDER_ID | --organization=ORGANIZATION_ID | --project=PROJECT_ID\n2. For each storage bucket listed above, set a retention policy and lock it:\ngsutil retention set [TIME_DURATION] gs://[BUCKET_NAME]\ngsutil retention lock gs://[BUCKET_NAME]","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-059":{"article":"Google Cloud Platform Alias IP Ranges allow you to assign ranges of internal IP addresses as aliases to a virtual machine's network interfaces. This is useful if you have multiple services running on a VM and you want to assign each service a different IP address.","impact":"Without Alias IPs the networking layer cannot perform anti-spoofing checks, firewall controls for Pods cannot be applied separately from their nodes, Pods cannot directly access hosted services without using a NAT gateway, Pod IPs may conflict with other compute resources.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Consol:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list.\n2. Click CREATE CLUSTER.\n3. In the Networking section, check the 'Enable VPC-native traffic routing (using alias IP)' option.\n4.  Configure other settings as required.\n5. Click on 'Create'\nUsing Command Line:\nTo enable Alias IP on a new cluster, run the following command:\ngcloud container clusters create [CLUSTER_NAME] --zone [COMPUTE_ZONE] --enable-ip-alias","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-337":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the VNC-Server port (5900) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 5900 (VNC-Server) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-111":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the FTP port (21) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted FTP access to your Google Cloud virtual machine (VM) instances via VPC network firewall rules can increase opportunities for malicious activities such as brute-force attacks, FTP bounce attacks, spoofing, and packet capture attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-128":{"article":"Disable the legacy GCE instance metadata APIs for GKE nodes. Under some circumstances, these can be used from within a pod to extract the node's credentials.","impact":"Without requiring a custom HTTP header when accessing the legacy GCE metadata endpoint, a flaw in an application that allows an attacker to trick the code into retrieving the contents of an attacker-specified web URL could provide a simple method for enumeration and potential credential exfiltration.","report_fields":["selfLink"],"remediation":"The legacy GCE metadata endpoint must be disabled upon creation of a cluster or a node-pool. For GKE versions 1.12 and newer, the legacy GCE metadata endpoint is disabled by default.\nUsing Google Cloud Console:\nTo update the existing cluster, create a new Node pool with the legacy GCE metadata endpoint disabled:\n1. Go to Kubernetes Engine using 'https://console.cloud.google.com/kubernetes/list'.\n2. Click on the name of the cluster to be upgraded and click ADD NODE POOL.\n3. Ensure that GCE instance metadata is set to the key-value pair 'disable-legacy-endpoints true'.\n4. Click SAVE.\nYou will need to migrate workloads from any existing non-conforming Node pools to the new Node pool, then delete non-conforming Node pools to complete the remediation.\nUsing Command Line:\nTo update the existing cluster, create a new Node pool with the legacy GCE metadata endpoint disabled:\ngcloud container node-pools create [POOL_NAME] --metadata disable-legacy-endpoints=true --cluster [CLUSTER_NAME] --zone [COMPUTE_ZONE]\nYou will need to migrate workloads from any existing non-conforming Node pools to the new Node pool, then delete non-conforming Node pools to complete the remediation.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-263":{"article":"This rule detects when a Cloud Run revision is configured with the default service account. The revision should be configured with a service account with least privileges according to requirements. You can resolve this by assigning custom roles or providing access according to the requirements in the provider documentation.","impact":"Using the default Compute Engine service account can lead to privilege escalations.","report_fields":["metadata.selfLink"],"remediation":"From Console:\n1. Go to the Cloud Run page at Google Cloud Console https://console.cloud.google.com/run.\n2. Select a service.\n3. Click on Edit and Deploy New Revision.\n4. Under Container, click the Service account dropdown and select the desired service account.\n5. Select Serve this revision immediately.\n6. Click on Deploy.","multiregional":true,"service":"Cloud Run"},"ecc-gcp-031":{"article":"GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions are met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or destination of the traffic, including IP addresses, subnets, and instances.\nFirewall rules are defined at the VPC network level, and are specific to the network in which they are defined. The rules themselves cannot be shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a destination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR notation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using RDP on Port 3389 can be avoided.","impact":"Publicly exposed RDP access can increase opportunities for malicious activities such as hacking, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to VPC Network.\n2. Go to the Firewall Rules.\n3. Click the Firewall Rule you want to modify.\n4. Click Edit.\n5. Modify Source IP ranges to specific IP.\n6. Click Save.\nFrom CLI:\n1.Update RDP Firewall rule with new SOURCE_RANGE from below command:\ngcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...]","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-313":{"article":"This rule detects when 'Access Transparency' is not Enabled in an organization.\nGCP Access Transparency provides audit logs for all actions that Google personnel take in your Google Cloud resources.\nControlling access to your information is one of the foundations of information security. Given that Google Employees do have access to your organizations' projects for support reasons, you should have logging in place to view who, when, and why your information is being accessed. By default Access Transparency is not enabled.\nTo enable Access Transparency for your Google Cloud organization, your Google Cloud organization must have one of the following customer support levels: Premium, Enterprise, Platinum, or Gold.","impact":"Lack of Access Transparency of logs record the actions taken by Google personnel can lead to insufficient response time to detect accidental or intentional changes.","report_fields":["projectId"],"remediation":"Add privileges to enable Access Transparency\n1. From the Google Cloud Home, within the project you wish to check, click on the Navigation hamburger menu in the top left. Hover over the 'IAM and Admin'. Select IAM in the top of the column that opens.\n2. Click the blue button the says +add at the top of the screen.\n3. In the principals field, select a user or group by typing in their associated email address.\n4. Click on the role field to expand it. In the filter field enter Access Transparency Admin and select it.\n5. Click save.\nVerify that the Google Cloud project is associated with a billing account\n1. From the Google Cloud Home, click on the Navigation hamburger menu in the top left. Select Billing.\n2. If you see This project is not associated with a billing account you will need to enter billing information or switch to a project with a billing account.\nEnable Access Transparency\n1. From the Google Cloud Home, click on the Navigation hamburger menu in the top left. Hover over the IAM & Admin Menu. Select settings in the middle of the column that opens.\n2. Click the blue button labeled Enable Access Transparency for Organization","multiregional":true,"service":"Access Transparency"},"ecc-gcp-163":{"article":"Security marking refers to the application or use of human-readable security attributes. Security labeling refers to the application or use of security attributes regarding internal data structures within systems. System media includes both digital and non-digital media. Digital media includes, for example, diskettes, magnetic tapes, external or removable disk drives, flash drives, compact disks, and digital video disks. Non-digital media includes, for example, paper and microfilm. Security marking is generally not required for media containing information determined by organizations to be in the public domain or to be publicly releasable. However, some organizations may require marking for public information indicating that the information is publicly releasable. Marking of system media reflects applicable laws, Executive Orders, directives, policies, regulations, standards, and guidelines.","impact":"An unlabeled bucket makes it difficult to read security attributes.","report_fields":["selfLink"],"remediation":"From GCP Console:\n1. Open the Cloud Storage browser in Google Cloud Console.\n2. In the bucket list, find the bucket you want to apply a label to, and click the 'More options' button (three vertical dots).\n3 .Click 'Edit labels'.\n4. In the side panel that appears, click the '+ Add label' button.\n5. Specify a key and value for your label.\n6. Click Save.\nGSUTIL:\nUse the -l flag in a label ch command. For example,\ngsutil label ch -l [KEY_1][VALUE_1] 'gs://[BUCKET_NAME]/'\nWhere:\n[KEY_1] is the key name for your label. For example, pet.\n[VALUE_1] is the value for your label. For example, dog.\n[BUCKET_NAME] is the name of the bucket that the label applies to. For example, my-bucket.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-442":{"article":"While the VM is suspended, you will be charged for the instance memory, device state and all of its attached resources (persistent disk usage, static IPs). To stop being charged for the instance memory and the device state, you can stop the VM instance. In case the attached resources aren't necessary, you can reconfigure a stopped VM to detach and then delete its resources.","impact":"While the VM is suspended, you will be billed for instance memory, device state, and all connected resources (persistent disk usage, static IP addresses)","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VM Instances page using https://console.cloud.google.com/compute/instances.\n2. Select the VM Instance you want to remediate.\n3. Click the Stop or Delete button.\nFrom CLI:\n1. Stop the instance:\ngcloud compute instances stop INSTANCE_NAME","multiregional":true,"service":"Compute Engine"},"ecc-gcp-334":{"article":"The User has an IAM policy containing 'iam.serviceAccounts.actAs' permissions that allow privilege escalation, at the resourse level. The existing permissions allow the user to impersonate a service account with higher permissions than their own. The user can then utilize that service account to perform API calls that the user may not be authorized to perform.","impact":"The permission allow the user to impersonate a service account with higher permissions than their own.","report_fields":["c7n:service-account.name","members"],"remediation":"From Console:\n1. Go to IAM & Admin/IAM using https://console.cloud.google.com/iam-admin/serviceaccounts.\n2. Click on reported servise account.\n3. Click permissions field.\n3. Click Edit for reported user.\n3. For all non-critical members of these roles, remove their membership by clicking the Trash icon on the right.","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-060":{"article":"Pod Security Policy should be used to prevent privileged containers where possible and enforce namespace and workload configurations.","impact":"Disabled and not set as appropriate, the Pod Security Policy may accept requests to create or update a pod that does not comply with the pod's security policy.","report_fields":["selfLink"],"remediation":"Using\tGoogle\tCloud\tConsole:\nThere is no means of enabling the Pod Security Policy Admission controller on an existing or new cluster from the console.\nUsing Command Line:\nThe Remediation script for this recommendation utilizes 2 variables $CLUSTER_NAME $COMPUTE_ZONE\nPlease set these parameters on the system where you will be executing your gcloud audit script or command.\nTo enable Pod Security Policy for an existing cluster, run the following command:\ngcloud beta container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --enable-pod-security-policy","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-191":{"article":"Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.","impact":"Lack of monitoring of  Shielded GKE Nodes can lead to insufficient response time to respond to integrity failures and prevent compromised nodes from being deployed into the cluster.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nOnce a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. You must create new Node pools within the cluster with Integrity Monitoring enabled.\nFrom Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. In the list of clusters, click on the cluster requiring the update and click 'Add node pool'.\n3. Ensure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' heading.\n4. Click 'Save'.\nYou will also need to migrate workloads from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\nFrom Command Line:\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the following command:\ngcloud beta container node-pools create [NODEPOOL_NAME] \\ --cluster [CLUSTER_NAME] --zone [COMPUTE_ZONE] \\ --shielded-integrity-monitoring\nYou will also need to migrate workloads from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-204":{"article":"The same SQL query can be excuted in multiple ways and still produce different results. The PostgreSQL planner/optimizer is responsible for creating an optimal execution plan for each query. The 'log_planner_stats' flag controls the inclusion of PostgreSQL planner performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_planner_stats' flag enables a crude profiling method for logging PostgreSQL planner performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_planner_stats' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the log_planner_stats database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_planner_stats=off\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-316":{"article":"This rule detects when Cloud Asset Inventory is not Enabled.\nGCP Cloud Asset Inventory is services that provides a historical view of GCP resources and IAM policies through a time-series database. The information recorded includes metadata on Google Cloud resources, metadata on policies set on Google Cloud projects or resources, and runtime information gathered within a Google Cloud resource.\nThe Cloud Asset Inventory API is disabled by default in each project.","impact":"Lack of history of GCP asset metadata can lead to insufficient response time to detect accidental or intentional changes.","report_fields":["name"],"remediation":"From Console:\nEnable the Cloud Asset API:\n1. Go to API & Services/Library by visiting https://console.cloud.google.com/apis/library.\n2. Search for Cloud Asset API and select the result for Cloud Asset API.\n3. Click the ENABLE button.\nFrom Command Line:\nEnable the Cloud Asset API:\n1. Enable the Cloud Asset API through the services interface:\ngcloud services enable cloudasset.googleapis.com","multiregional":true,"service":"Cloud Asset Inventory"},"ecc-gcp-198":{"article":"It is recommended to set the 'skip_show_database' database flag for Cloud SQL Mysql instance to 'on'","impact":"The 'Skip_show_database' database flag prevents people from using the SHOW DATABASES statement if they do not have the SHOW DATABASES privilege. This can improve security if you have concerns about users being able to see databases belonging to other users.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the Mysql instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'skip_show_database' flag from the drop-down menu, and set its value to 'on'.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the skip_show_database database flag for every Cloud SQL Mysql database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags skip_show_database=on\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-029":{"article":"DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing should be recommended one and it should not be weak.","impact":"The use of RSASHA1 results in a low level of security (weak algorithm). You should not use it unless required for compatibility reasons.","report_fields":["id","name"],"remediation":"From Command Line:\n1. If the need exists to change the settings for a managed zone where it has been enabled, DNSSEC must be turned off and then re-enabled with different settings. To turn off DNSSEC, run following command:\ngcloud dns managed-zones update ZONE_NAME --dnssec-state off\n2. To update zone-signing for a reported managed DNS Zone, run the following command:\ngcloud dns managed-zones update ZONE_NAME --dnssec-state on --ksk-algorithm KSK_ALGORITHM --ksk-key-length KSK_KEY_LENGTH --zsk-algorithm ZSK_ALGORITHM --zsk-key-length ZSK_KEY_LENGTH --denial-of-existence DENIAL_OF_EXISTENCE\nSupported algorithm options and key lengths are as follows.\nAlgorithm                 KSK Length            ZSK Length\n---------                 ----------            ----------\nRSASHA1                   1024,2048             1024,2048\nRSASHA256                 1024,2048             1024,2048\nRSASHA512                 1024,2048             1024,2048\nECDSAP256SHA256           256                   384\nECDSAP384SHA384           384                   384","multiregional":true,"service":"Cloud DNS"},"ecc-gcp-028":{"article":"DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing should be a recommended one and it should not be weak.","impact":"The use of RSASHA1 results in a low level of security (weak algorithm). You should not use it unless required for compatibility reasons.","report_fields":["id","name"],"remediation":"From Command Line:\n1. If it is necessary to change the settings for a managed zone where it has been  enabled, NSSEC must be turned off and re-enabled with different settings. To turn off DNSSEC, run the following command:\ngcloud dns managed-zones update ZONE_NAME --dnssec-state off\n2. To update key-signing for a reported managed DNS Zone, run the following command:\ngcloud dns managed-zones update ZONE_NAME --dnssec-state on --ksk-algorithm KSK_ALGORITHM --ksk-key-length KSK_KEY_LENGTH --zsk-algorithm ZSK_ALGORITHM --zsk-key-length ZSK_KEY_LENGTH --denial-of-existence DENIAL_OF_EXISTENCE\nSupported algorithm options and key lengths are as follows.\nAlgorithm                        KSK Length               ZSK Length\n---------                        ----------               ----------\nRSASHA1                          1024,2048                1024,2048\nRSASHA256                        1024,2048                1024,2048\nRSASHA512                        1024,2048                1024,2048\nECDSAP256SHA256                  256                      256\nECDSAP384SHA384                  384                      384","multiregional":true,"service":"Cloud DNS"},"ecc-gcp-047":{"article":"Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.","impact":"Lack of Stackdriver Kubernetes Logging can result in insufficient response time to detect issues due to inaccessibility of audit data after a cluster security event and lack of a centralized place to analyze log data and metrics collected from multiple sources.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Select Kubernetes clusters for which logging is disabled.\n3. Click on EDIT.\n4. Set 'Stackdriver Logging' to 'Enabled'.\n5. Click SAVE.\nUsing Command Line:\nTo enable Stackdriver Logging for an existing cluster, run the following command:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --logging-service logging.googleapis.com","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-276":{"article":"Security Policy rules help identify web requests that we want to allow or block. You should add rules to the Security Policy, other than the default rule.","impact":"The lack of rules can increase the scope for attacks on web applications such as XSS, SQL injection, CSRF, and can eventually lead to data loss.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud Armor page in the Cloud Console.\n2. On the Security policies page, click the name of the security policy. The Policy details page is displayed.\n3. In the middle of the page, click the Rules tab.\n4. Click Add rule.\n5. (Optional) Enter a description of the rule.\n6. Select the mode:\n   Basic mode: allow or deny traffic based on IP addresses or IP ranges.\n   Advanced mode: allow or deny traffic based on rule expressions.\n7. In the Match field, specify the conditions under which the rule applies. Follow this link for more details\n8. For Action, select Allow or Deny.\n9. If you are creating a deny rule, select a Deny status message.\n10. If you want to enable preview mode for the rule, select the Enable checkbox.\n11. In the Priority field, enter a positive integer.\n12. Click Add.","multiregional":true,"service":"Cloud Armor"},"ecc-gcp-444":{"article":"VM instance have disk which contain snapshot that were created more than 90 days ago. The most recent snapshot is usally used to restore data, if something goes wrong.","impact":"Keeping unnecessary snapshots will increase your monthly bill.","report_fields":["selfLink","sourceDisk"],"remediation":"From Console:\n1. Go to the Snapshots page in the Google Cloud Platform Console.\n2. Click on reported snapshot and Delete it.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-232":{"article":"Ensure that the OS Login feature enabled at the virtual machine instance level is configured with Two-Factor Authentication (2FA) in order to help protect the access to your Google Cloud VM instances. Two-Factor Authentication (also known as Multi-Factor Authentication - MFA) provides an additional layer of security on top of the existing credentials.","impact":"Without an 2FA/MFA-protected instance does not represent an efficient way to safeguard your production and business-critical applications against malicious actors, as attackers would have to compromise less authentication methods in order to gain access to your VM instance, and this increase the risk of attack.","report_fields":["selfLink"],"remediation":"Set 'enable-oslogin-2fa' in project-wide metadata so that it applies to all of the instances in your project\nFrom Console:\n1. Go to the VM compute metadata page using https://console.cloud.google.com/compute/metadata.\n2. Click Edit.\n3. Add a metadata entry where the key is 'enable-oslogin-2fa' and the value is 'TRUE'.\n4. Click Save to apply the changes.\n5. For every instances that overrides the project setting, go to the VM Instances page at https://console.cloud.google.com/compute/instances.\n6. Click the name of the instance on which you want to remove the metadata value.\n7. At the top of the instance details page, click Edit to edit the instance settings.\n8. Under Custom metadata, remove any entry where the key is 'enable-oslogin-2fa' and the value is 'FALSE'.\n9. At the bottom of the instance details page, click Save to apply your changes to the instance.\nFrom Command Line:\n1. Configure oslogin on the project:\ngcloud compute project-info add-metadata --metadata enable-oslogin-2fa=TRUE\n2. Remove instance metadata that overrides the project setting:\ngcloud compute instances remove-metadata INSTANCE_NAME --keys=enable-oslogin-2fa","multiregional":true,"service":"Compute Engine"},"ecc-gcp-318":{"article":"This rule detects when GCP cloud function is running with an outdated runtime.\nWe recommend to update the function to use up to date runtime. for more information, see https://cloud.google.com/functions/docs/concepts/execution-environment.","impact":"Outdated runtimes have bugs and security vulnerabilities.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select the desired function by clicking on its name.\n3. At the top of the page, choose Edit.\n4. At the bottom of the page, choose NEXT.\n5. From the Runtime drop-down list, choose the desired supported runtime.\n6. At the bottom of the page, choose DEPLOY.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-287":{"article":"Ensure that each Google Cloud Pub/Sub subscription is configured to use a dead-letter topic, also known as dead-letter queue, in order to capture undeliverable messages. Your Pub/Sub subscriptions are configured with a maximum number of delivery attempts. When a message cannot be delivered, it is republished to the specified dead-letter topic.","impact":"Disabling dead-letter topics for your Pub/Sub subscriptions reduces cloud applications resilient and durable when messages can not be delivered due to client errors or server errors.","report_fields":["name"],"remediation":"From Console:\n1. Navigate to Pub/Sub console at https://console.cloud.google.com/cloudpubsub and click on the CREATE TOPIC button from the dashboard top menu to initiate the dead-letter topic setup process.\n2. Within Create a topic configuration box, perform the following actions:\n  A.\tProvide a unique identifier for the new topic in the Topic ID box.\n  B.\tIn the Encryption section, choose Customer-managed key, and select your own CMK from the Select a customer-managed key dropdown list.\n  C.\tInside \"The service-<project-number>@gcp-sa-pubsub.iam.gserviceaccount.com service account does not have permissions to encrypt/decrypt with the selected key.\" box, click Grant to grant the specified service account the required IAM role on the selected CMK.\n  D.\tClick CREATE TOPIC to deploy your new Pub/Sub dead-letter topic.\n3. The dead-letter topic should have at least one subscription so that dead-lettered messages will not be lost. Click on the newly created Pub/Sub topic, select the SUBSCRIPTIONS tab, then click CREATE SUBSCRIPTION to create the required subscription.\n4. In the navigation panel, select Subscriptions to access the Pub/Sub subscriptions available for the selected GCP project.\n5. Click on the ID of the subscription that you want to reconfigure, and choose the EDIT button from the dashboard top menu.\n6. On the Edit subscription page, in the Dead lettering section, select Enable dead lettering checkbox, and choose the dead-letter topic created at the previous steps from the Select a dead letter topic dropdown list. (Optional) In the Maximum delivery attempts field, specify an integer between 5 and 100, based on your application needs. Click UPDATE to apply the changes.\n7. Once the configuration changes are applied to the selected subscription, select the DEAD LETTERING tab, and click on the GRANT PUBLISHER ROLE and GRANT SUBSCRIBER ROLE to assign the Publisher and Subscriber roles. The Pub/Sub service account for the selected GCP project needs the Publisher role to publish dead-lettered messages to the specified topic and the Subscriber role to forward messages from this subscription to the dead-letter topic. The selected subscription can dead letter messages to the new dead-letter topic once it exceeds the specified delivery attempt count.","multiregional":true,"service":"Pub/Sub"},"ecc-gcp-195":{"article":"Cloud DNS logging records the queries from the name servers within your VPC to Stackdriver. Logged queries can come from Compute Engine VMs, GKE containers, or other GCP resources provisioned within the VPC.","impact":"Monitoring of Cloud DNS logs provides visibility to DNS names requested by the clients within the VPC. These logs can be monitored for anomalous domain names and other suspicious activity.","report_fields":["selfLink"],"remediation":"From Command Line:\n1. Add New DNS Policy With Logging Enabled.\nFor each VPC network that needs a DNS policy with logging enabled:\ngcloud dns policies create enable-dns-logging --enable-logging --description=''Enable DNS Logging'' --networks=VPC_NETWORK_NAME\nThe VPC_NETWORK_NAME can be one or more networks in comma-separated list.\n2. Enable Logging for Existing DNS Policy.\nFor each VPC network that has an existing DNS policy that needs logging enabled:\ngcloud dns policies update POLICY_NAME --enable-logging --networks=VPC_NETWORK_NAME\nThe VPC_NETWORK_NAME can be one or more networks in comma-separated list.\nCloud DNS logging is disabled by default on each network.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-183":{"article":"The 'log_min_duration_statement' flag defines the minimum amount of execution time of a statement in milliseconds where the total duration of the statement is logged. Ensure that 'log_min_duration_statement' is disabled, i.e., a value of '-1' is set.","impact":"The 'log_min_duration_statement' configuration flag causes the duration of each completed SQL statement to be logged if the statement executes for at least the specified number of milliseconds. Setting this flag to 0 logs all statement durations, whereas setting it to -1 disables logging statement durations. Logging SQL statements may include sensitive information that should not be recorded in log files.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_min_duration_statement' flag from the drop-down menu and set a value to '-1'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_min_duration_statement' flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_min_duration_statement=-1\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-165":{"article":"Container-native Load balancing enables HTTP(S) Load balancers to target Pods directly and evenly distribute their traffic to Pods. Container-native load balancing leverages a data model called network endpoint groups (NEGs), and collections of network endpoints represented by IP-port pairs.","impact":"When Load Balancer is disabled, the Kubernetes Engine can't terminate unauthorized HTTP/HTTPS requests and make worse context-aware load balancing decisions, which may have a negative effect on the availability of resources.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Google Kubernetes Engine menu in GCP Console.\n2. Click Edit 'HTTP Load Balancing'.\n3. Enable HTTP load balancing.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-037":{"article":"A serial port is often referred to as a serial console. Interaction with it is similar to using a terminal window where the input and output are performed entirely in a text mode, and no graphical interface or mouse support is provided.\nIf you enable an interactive serial console on an instance, clients can attempt to connect to that instance from any IP address. Therefore, the interactive serial console support should be disabled.","impact":"If you enable the interactive serial console on an instance, clients can attempt to connect to that instance from any IP address. This allows anybody to connect to that instance if they know the correct SSH key, username, project ID, zone, and instance name.","report_fields":["selfLink"],"remediation":"From Console:\n1. Login to the Google Cloud console.\n2. Go to Computer Engine.\n3. Go to VM instances.\n4. Click on the Specific VM.\n5. Click on EDIT.\n6. Unselect Enable connecting to serial ports below Remote access block.\n7. Click on Save.\nFrom CLI:\nUse the below command to disable:\ngcloud compute instances add-metadata INSTANCE_NAME --zone=ZONE -- metadata=serial-port-enable=false\nor\ngcloud compute instances add-metadata INSTANCE_NAME --zone=ZONE -- metadata=serial-port-enable=0","multiregional":true,"service":"Compute Engine"},"ecc-gcp-299":{"article":"Ensure that the objects stored within your Google Cloud Storage buckets have a sufficient data retention period configured for security and compliance purposes. A retention period indicates the amount of time the objects in the bucket must be retained. The retention period can be configured by editing the bucket retention policy. A retention policy prevents the deletion or modification of the bucket's objects for the specified duration of time. You can set a maximum retention period of 3155760000 seconds (i.e. 100 years).\nNote: if the retention policy associated with your bucket is locked, the policy can not be edited or removed.","impact":"Without an optimal data retention period set for Google Cloud Storage objects will not allow you recover your data more efficiently in the event of a failure or deliberate  deletion by an attacker.","report_fields":["selfLink"],"remediation":"From Console:\n1. Sign in to Google Cloud Management Console.\n2. Select the Google Cloud Platform project that you want to examine from the console top navigation bar.\n3. Navigate to Cloud Storage dashboard at https://console.cloud.google.com/storage.\n4. Click on the name of the storage bucket that you want to examine.\n5. Select the Protection tab near the top of the page.\n6. In the Retention policy section, set your retention policy:\n   a. If no retention policy currently applies to the bucket, click the + Set Retention Policy link. Choose a unit of time and a length of time for your retention period.\n   b. If a retention policy currently applies to a bucket, it appears in the section. Click Edit to modify the retention time.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-387":{"article":"Autoscaling capabilities let you automatically add or delete virtual machine (VM) instances from a MIG based on increases or decreases in load. Autoscaling helps your apps gracefully handle increases in traffic and reduce costs when the need for resources is lower.","impact":"Not using Autoscaling capabilities will increase your monthly bill.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Instance groups page in the Google Cloud console: https://console.cloud.google.com/compute/instanceGroups/list.\n2. Select reported instance group and click Edit.\n3. If no autoscaling configuration exists, under Autoscaling, click Configure autoscaling.\n4. Under Autoscaling mode, select On: add and remove instances to the group to enable autoscaling.\n5. Specify the minimum and maximum numbers of instances that you want the autoscaler to create in this group.\n6. In the Autoscaling metrics section add desired metrics.\n7. Click Save.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-151":{"article":"Check to verify if any VM instance is initiated with the 'Pre-Emptible termination' flag set to 'True'. Setting this instance to 'True' implies that this VM instance will shut down within 24 hours or can also be terminated by a Service Engine when high demand is encountered.","impact":"Setting an instance to True implies that it can lead to unexpected loss of service when the VM instance is terminated.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the VM Instances page using https://console.cloud.google.com/compute/instances.\n2. Select the VM Instance you want to remediate.\n3. Click the Delete button.\n4. On the 'VM Instances' page, click 'CREATE INSTANCE'.\n5. Create a new instance with necessary services, processes, and updates not using Pre-Emptible termination.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-451":{"article":"Compute disk without label info","impact":"Compute disk without label information make identification and search difficult.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Disks page in the Google Cloud Platform Console https://console.cloud.google.com/compute/disks.\n2. Select reported Disk.\n3. Click \"Labels\" -> \"Add label\" and add proper key-value pair.\n4. Save changes.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-201":{"article":"The value of the 'log_statement' flag determines the SQL statements that are logged. Valid values are - none, ddl, mod, all. The value 'ddl' logs all data definition statements. The value 'mod' logs all ddl statements, plus data-modifying statements. The statements are logged after a basic parsing is done and statement type is determined. Thus, it does not log statements with errors. When using extended query protocol, logging occurs after the Execute message is received and values of the Bind parameters are included.","impact":"Auditing helps in troubleshooting operational problems and also permits forensic analysis. If log_statement is not set to the correct value, too many statements may be logged leading to issues in finding the relevant information from the logs, or too few statements may be logged with relevant information missing from the logs.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_statement' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\n1. Configure the log_statement database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags log_statement=<ddl|mod|all|none>\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-385":{"article":"The default network in GCP region is generally considered insecure and should be avoided. The default firewall and routes are generic in nature and do not lock down sensitive ports or traffic routing.","impact":"Automatically created firewall rules of the default network have a number of open ports, including port 22. This may increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nWe can't edit an existing Instance Template. Delete the existing one and create a new one with a non-default network using the link https://cloud.google.com/compute/docs/instance-templates/create-instance-templates","multiregional":true,"service":"Compute Engine"},"ecc-gcp-142":{"article":"Check to ensure that logging is enabled on a Stackdriver exported Bucket. Enabled logging provides information about all the requests made on the bucket.","impact":"Without logging enabled, malicious requests to the bucket may slip under the radar.","report_fields":["selfLink"],"remediation":"Make a note of the reported Storage buckets which have Stackdriver Logs.\nFrom Command line:\n1. Create a bucket to store your logs using the following command:\ngsutil mb gs://example-logs-bucket\n2. Assign Cloud Storage the roles/storage.legacyBucketWriter role for the bucket:\ngsutil iam ch group:cloud-storage-analytics@google.com:legacyBucketWriter gs://example-logs-bucket\n3. Enable logging for your bucket using the logging command:\ngsutil logging set on -b gs://example-logs-bucket [-o log_object_prefix ] gs://reported-storage-bucket\nGCP documentation here 'https://cloud.google.com/storage/docs/access-logs'.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-113":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the Microsoft-DS port (445) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound  access over TCP port 445 (Microsoft-DS) through VPC network firewall rules can increase opportunities for malicious acts such as hacking, brute force attacks, and interception of network traffic.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-024":{"article":"It is recommended that a metric filter and alarm be established for SQL Instance configuration changes.","impact":"Lack of monitoring and logging of SQL instance configuration changes can lead to insufficient response time to detect and correct misconfigurations done on the SQL server.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed Log Metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click \"CREATE METRIC\".\n2. Click the down arrow symbol on Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nprotoPayload.methodName=\"cloudsql.instances.update\"\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This ensures that the log metric counts the number of log entries matching the user's advanced logs query.\n6. Click Create Metric.\nCreate the prescribed alert policy:\n1. Identify the newly created metric under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page appears.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the user's organization. For example, a threshold of zero (0) for the most recent value will ensure that a notification is triggered for every owner change in the user's project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notification channels in the Notifications section.\n5. Name the policy and click Save.\nFrom Google Cloud CLI:\nCreate the prescribed log metric:\ngcloud logging metrics create\nCreate the prescribed alert policy:\ngcloud alpha monitoring policies create\nReference for command usage https://cloud.google.com/sdk/gcloud/reference/alpha/monitoring/policies/create","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-040":{"article":"It is recommended that IAM policy on Cloud Storage bucket do not allow anonymous and/or public access.","impact":"Allowing anonymous or public access grants permissions to anyone to access bucket content. Such access might not be desired if you are storing any sensitive data.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Storage browser using https://console.cloud.google.com/storage/browser.\n2. Click on the bucket name to go to the Bucket details page.\n3. Click on the Permissions tab.\n4. Click on the Delete button in front of allUsers and allAuthenticatedUsers to remove a particular role assignment.\nFrom CLI:\nRemove allUsers and allAuthenticatedUsers access.\ngsutil iam ch -d allUsers gs//BUCKET_NAME\ngsutil iam ch -d allAuthenticatedUsers gs//BUCKET_NAME","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-412":{"article":"This rule detects when Datafusion instance has public ip.\nIn a public instance, network communication happens over the open Internet regardless of the source location, which is not recommended for critical environments.","impact":"An data fusion instance with a public IP address could potentially be compromised, which may lead to malicious activity with sensitive data.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Create a VPC network or a shared VPC network.\n2. Enable Private Google Access and allocate an IP range (If you're not using a shared VPC network, Cloud Data Fusion allocates an IP range by default when you create an data-fusion instance).\n3. Create a private instance (If you're using a shared VPC network, go to step 4).\n3.1 Go to the Create Data Fusion instance page.\n3.2 Enter an instance name and description for your instance.\n3.3 Select the Region in which to create the instance. The region must have Private Google Access enabled.\n3.4 Select a Cloud Data Fusion Version and Edition.\n3.5 Specify the Dataproc service account to use for running your Cloud Data Fusion pipeline in Dataproc.\n3.6 Expand the Advanced Options menu and click Enable Private IP.\n3.7 In the Network field, choose a network in which to create the instance.\n3.8 Click Create. It takes up to 30 minutes for the instance creation process to complete.\n4. If you're using a shared VPC network, use this curl command:\ncurl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" https://$DATA_FUSION_API_NAME/v1/projects/$PROJECT/locations/$LOCATION/instances?instanceId=INSTANCE_ID -X POST -d '{\"description\": \"Private CDF instance created through REST.\", \"type\": \"ENTERPRISE\", \"privateInstance\": true, \"networkConfig\": {\"network\": \"projects/SHARED_VPC_HOST_PROJECT_ID/global/networks/NETWORK_NAME\", \"ipAllocation\": \"IP_RANGE\"}}'\n5. If you need to set up VPC Network Peering to connect with the source and sink that you use in your pipeline than follow this link https://cloud.google.com/data-fusion/docs/how-to/create-private-ip#set-up-vpc-peering\n6. If you create your Cloud Data Fusion instance in a shared VPC network, you must grant the Compute Network User role to the following service accounts:\nCloud Data Fusion service account: service-PROJECT_NUMBER@gcp-sa-datafusion.iam.gserviceaccount.com\nDataproc service account: service-PROJECT_NUMBER@dataproc-accounts.iam.gserviceaccount.com\n7. Create a firewall rule on your VPC network that allows for incoming SSH connections from the IP range you specified when you created your private Cloud Data Fusion instance:\ngcloud compute firewall-rules create FIREWALL_NAME-allow-ssh --allow=tcp:22 --source-ranges=IP_RANGE --network=NETWORK_NAME --project=PROJECT_ID","multiregional":true,"service":"Cloud Data Fusion"},"ecc-gcp-107":{"article":"Identify the volumes that have not had a backup or snapshot in the past 14 days. If a snapshot is not recent, it could be missing crucial patches and software updates.","impact":"If a snapshot is not recent, it could be missing crucial patches and software updates.","report_fields":["selfLink","sourceDisk"],"remediation":"From Console:\n1. Go to the Snapshots page in the Google Cloud Platform Console.\n2. Enter a snapshot Name.\n3. Optionally, enter a Description of the snapshot.\n4. Under Source disk, select an existing disk from which you want to create a snapshot.\n5. Optionally, you can specify a custom storage location.\n6. Click Create to create the snapshot.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-225":{"article":"VM instances with public IP addresses are part of the network perimeter and are susceptible to various attack vectors over the exposed services. Unless essential to the provided service, avoid providing such instances with permissions to access GCS buckets.","impact":"An instance with a public IP address and access to GCS buckets could potentially be compromised, which may lead to malicious activity with sensitive data.","report_fields":["selfLink"],"remediation":"From Console:\nEither:\n1. In the Cloud Console, go to https://console.cloud.google.com/compute/instances.\n2. Click on the relevant VM instance name.\n3. Click on the Edit button in the toolbar.\n4.Scroll down to the Network Interfaces section and click on the Edit (pen) icon next to the network interface with public IP.\n5. In the \"External IP\" field, select \"None\".\n6. Click on the Done button to save your changes.\nOr remove the role assignment that permits bucket access from the VM instance as follows:\n1. In the Cloud Console, go to the IAM & Admin page at https://console.cloud.google.com/iam-admin/iam.\n2. Select Edit service account that is assigned to the VM instance.\n3. Remove the redundant role.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-217":{"article":"Interacting with a serial port is often referred to as the serial console, which is similar to using a terminal window, in that input and output is entirely in text mode and there is no graphical interface or mouse support. If you enable the interactive serial console on an instance, clients can attempt to connect to that instance from any IP address. Therefore interactive serial console support should be disabled.","impact":"if an instance based on template with enabled interactive serial console, clients can attempt to connect to that instance from any IP address. This allows anybody to connect to that instance if they know the correct SSH key, username, project ID, zone, and instance name.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the instance template page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. To block serial ports access, create a new template and set serial-port-enable=false in instance template metadata.\n3. Repeat the above steps for every impacted Instance template.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-169":{"article":"Ensure that Cloud KMS cryptokeys are not anonymously or publicly accessible","impact":"Misconfigured access permissions is a common security vulnerability that involves KMS resources. Granting permissions to 'allUsers' and 'allAuthenticatedUsers' members can allow anyone to access your KMS keys and the data encrypted with these keys.","report_fields":["name"],"remediation":"From Command Line:\n1. List all Cloud KMS Cryptokeys:\ngcloud kms keys list --keyring=[key_ring_name] --location=global -- format=json | jq '.[].name'\n2.Using the below command, remove IAM policy binding for a KMS key to deny access to 'allUsers' and 'allAuthenticatedUsers':\ngcloud kms keys remove-iam-policy-binding [key_name] -- keyring=[key_ring_name] --location=global --member='allAuthenticatedUsers' -- role='[role]'\ngcloud kms keys remove-iam-policy-binding [key_name] -- keyring=[key_ring_name] --location=global --member='allUsers' --role='[role]'","multiregional":true,"service":"Cloud KMS"},"ecc-gcp-231":{"article":"It is recommended to configure Google Cloud Managed Instance Groups (MIGs) with Autohealing feature.","impact":"Without autohealing virtual machine instances that become unresponsive can't be automaticaly re-created.","report_fields":["selfLink"],"remediation":"From Console:\n1. Navigate to Google Compute Engine dashboard at https://console.cloud.google.com/compute.\n2. In the navigation panel, select Instance groups to access the list with the VM instance groups created for the selected project.\n3. Click on the name of the MIG resource that you want to reconfigure, and choose EDIT GROUP to access the instance group editing page.\n4. Under Autohealing, select Create a health check from the Heath check dropdown list to initiate the setup process.\n5. Click Save to apply the configuration changes.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-192":{"article":"Enable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.","impact":"An attacker may seek to alter boot components to persist malware or root kits during system initialization. Secure Boot helps ensure that the system only runs authentic software byverifying the digital signature of all boot components, and halting the boot process if signature verification fails.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nOnce a Node pool is provisioned, it cannot be updated to enable Secure Boot. You must create new Node pools within the cluster with Secure Boot enabled.\nFrom Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2.In the list of clusters, click on the cluster requiring the update and click 'Add node pool'.\n3. Ensure that the 'Secure boot' checkbox is checked under the 'Shielded options' heading.\n4. Click 'Save'.  You will also need to migrate workloads from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\nFrom Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following command:\ngcloud beta container node-pools create [NODEPOOL_NAME] \\ --cluster [CLUSTER_NAME] --zone [COMPUTE_ZONE] \\ --shielded-secure-boot.\nYou will also need to migrate workloads from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-280":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the kibana port (5601) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 5601 (kibana) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-003":{"article":"User-managed service account should not have user-managed keys. Anyone who has access to the keys will be able to access resources through the service account.\nGCP-managed keys are used by Cloud Platform services such as App Engine and Compute Engine. These keys cannot be downloaded. Google will keep the keys and automatically rotate them on an approximately weekly basis.","impact":"Managing your own service account keys will increase the risk of key exposure.","report_fields":["keys[].name"],"remediation":"From Console:\n1. Go to the IAM page from the GCP Console using  https://console.cloud.google.com/iam-admin/iam.\n2. In the left navigation pane, click on Service accounts. All service accounts and their  corresponding keys are listed.\n3. Click on the service account.\n4. Click on Edit and delete the keys.\nFrom Command Line:\nTo delete a user-managed Service Account Key, run the following command,\ngcloud iam service-accounts keys delete --iam-account=<user-managed-service_x0002_account-EMAIL> <KEY-ID>","multiregional":true,"service":"Cloud IAM"},"ecc-gcp-315":{"article":"This rule detects when the 'cloudsql.enable_pgaudit' Database Flag for Cloud Sql Postgresql Instance is not Set to 'on'. By default cloudsql.enable_pgaudit database flag is set to off and the extension is not enabled.\nYour organization will need a way to manage logs. You may have a solution already in place. If you do not, consider installing and enabling the open source pgaudit extension within PostgreSQL and enabling its corresponding flag of cloudsql.enable_pgaudit. This flag and installing the extension enables database auditing in ostgreSQL through the open-source pgAudit extension. This extension provides detailed session and object logging to comply with government, financial, & ISO standards and provides auditing capabilities to mitigate threats by monitoring security events on the instance. Enabling the flag and settings later in this recommendation will send these logs to Google Logs Explorer so that you can access them in a central location.","impact":"pgAudit extension helps configure many of the logs often required to comply with government, financial, and ISO certifications and provides auditing capabilities to mitigate threats by monitoring security events on the instance.","report_fields":["selfLink"],"remediation":"Initialize the pgAudit flag\nNote: RESTART is required to get this configuration in effect\nFrom Console:\n1. Go to https://console.cloud.google.com/sql/instances.\n2. Select the instance to open its Overview page.\n3. Click Edit.\n4. Scroll down and expand Flags.\n5. To set a flag that has not been set on the instance before, click Add item.\n6. Enter cloudsql.enable_pgaudit for the flag name and set the flag to on.\n7. Click Done.\n8. Click Save to update the configuration.\n9. Confirm your changes under Flags on the Overview page\nFrom Command Line:\nRun the below command by providing <INSTANCE_NAME> to enable cloudsql.enable_pgaudit flag.\ngcloud sql instances patch <INSTANCE_NAME> --database-flags=cloudsql.enable_pgaudit=on\nCreating the extension\n1. Connect to the the server running PostgreSQL or through a SQL client of your choice.\n2. If SSHing to the server in the command line open the PostgreSQL shell by typing psql\n3. Run the following command as a superuser.\nCREATE EXTENSION pgaudit;\nUpdating the previously created pgaudit.log flag for your Logging Needs\nNote: there are multiple options here. This command will enable logging for all databases on a server. Please see the customizing database audit logging reference for more flag\noptions.\nFrom Console:\n1. Go to https://console.cloud.google.com/sql/instances.\n2. Select the instance to open its Overview page.\n3. Click Edit.\n4. Scroll down and expand Flags.\n5. To set a flag that has not been set on the instance before, click Add item.\n6. Enter pgaudit.log=all for the flag name and set the flag to on.\n7. Click Done.\n8. Click Save to update the configuration.\n9. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\nRun the command\ngcloud sql instances patch <INSTANCE_NAME> --database-flags cloudsql.enable_pgaudit=on,pgaudit.log=all\nDetermine if logs are being sent to Logs Explorer\n1. From the Google Console home page, open the hamburger menu in the top left.\n2. In the menu that pops open, scroll down to Logs Explorer under Operations.\n3. In the query box, paste the following and search\nresource.type=\"cloudsql_database\"\nlogName=\"projects//logs/cloudaudit.googleapis.com%2Fdata_access\"\nprotoPayload.request.@type=\"type.googleapis.com/google.cloud.sql.audit.v1.PgAuditEntry\"\nIf it returns any log sources, they are correctly setup.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-033":{"article":"Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC Subnets. After you've created a flow log, you can view and retrieve its data in Stackdriver Logging.\nIt is recommended that Flow Logs be enabled for every business-critical VPC subnet.","impact":"Without enabled VPC Flow Logs, you miss out on the opportunity to detect security and access issues like overly permissive security groups or network ACLs and the opportunity to receive alerts about abnormal activities triggered within your VPC networks, such as rejected connection requests or unusual levels of data transfer.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the VPC network GCP Console visiting https://console.cloud.google.com/networking/networks/list\n2. Click the name of a subnet, The Subnet details page displays.\n3. Click the EDIT button.\n4. Set Flow Logs to On.\n5. Expand the Configure Logs section.\n6. Set Aggregation Interval to 5 SEC.\n7. Check the box beside Include metadata.\n8. Set Sample rate to 100 %.\n9. Click Save.\nNote: It is not possible to configure a Log filter from the console.\nFrom Command Line:\nTo enable VPC Flow Logs for a network subnet, run the following command:\ngcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-flow-logs --logging-aggregation-interval=interval-5-sec --logging-flow-sampling=1 --logging-metadata=include-all","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-452":{"article":"Cloud Dataproc cluster without label information make identification and search difficult.","impact":"Cloud Dataproc clusters without label information makes it difficult to identify, search and view usage of GCP resource and spending.","report_fields":["clusterName","projectId"],"remediation":"From Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Clusters.\n3. Select the target Dataproc cluster.\n4. Expand the Info Panel by selecting Show Info Panel.\n5. Select the Labels button and add desired labels.","multiregional":true,"service":"Dataproc"},"ecc-gcp-278":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the elastic search ports (9200 or 9300) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP ports 9200 or 9300 (elastic search) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-014":{"article":"It is recommended that Cloud Audit Logging is configured to track all Admin activities and read/write access to user data.\nObject versioning acts as an extra layer of data protection and can be used for retention scenarios such as recovering objects that have been accidentally or intentionally deleted, or overwritten by Cloud IAM users or cloud applications.","impact":"Without a customized log for all Admin actions and read/write access to user data, you miss out on logging any changes / tampering with user data.","report_fields":["projectId"],"remediation":"From Console:\n1. Go to Audit Logs using https://console.cloud.google.com/iam-admin/audit.\n2. Follow the steps at https://cloud.google.com/logging/docs/audit/configure-data-access to enable audit logs for all Google Cloud services.\nFrom Command Line:\n1. To read the project's IAM policy and store it in a file, run a command:\ngcloud projects get-iam-policy PROJECT_ID > /tmp/project_policy.yaml\nAlternatively, the policy can be set at the organization or folder level. When setting the policy at the organization level, it is not necessary to set it for each folder or project:\ngcloud organizations get-iam-policy ORGANIZATION_ID > /tmp/org_policy.yaml\ngcloud resource-manager folders get-iam-policy FOLDER_ID >/tmp/folder_policy.yaml\n2. Edit the policy in /tmp/policy.yaml adding or changing only the audit logs configuration to:\nauditConfigs:\n- auditLogConfigs:\n  - logType: DATA_WRITE\n  - logType: DATA_READ\n  service: allServices\n3. To write a new IAM policy, run the command:\ngcloud organizations set-iam-policy ORGANIZATION_ID /tmp/org_policy.yaml\ngcloud resource-manager folders set-iam-policy FOLDER_ID /tmp/folder_policy.yaml\ngcloud projects set-iam-policy PROJECT_ID /tmp/project_policy.yaml","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-099":{"article":"A good cloud provisioning strategy should include tagging of resources. Tags may be used to associate the function, owner, environment, or other attributes for application instances, and a consistent tagging strategy is a recommended best practice.","impact":"Instance Without Any Tags does not allow you to categorize GCP resources in different ways, such as by purpose, owner, or environment, which is useful when you have many resources of the same type.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to Compute engine https://console.cloud.google.com/compute/instances.\n2. Click an instance name.\n3. Click an instance name.\n4. In the Network tags section, specify one or more tags, separated by commas.\n5. Click Save.\nFrom Command Line:\n1. To assign new tags to an instance, use the following gcloud command:\ngcloud compute instances add-tags INSTANCE_NAME --zone ZONE --tags TAGS","multiregional":true,"service":"Compute Engine"},"ecc-gcp-254":{"article":"There are one or more Cloud AppEngine applications with an SSL certificate that expires in less than 30 days. To prevent security incidents, renew the SSL certificate before it expires. Configure GCP AppEngine custom domain SSL certificate with an expiration time of at least 30 days.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and a GCP resource that implements the certificates is no longer secure.","report_fields":["name"],"remediation":"Get a new certificate for your domain from the certificate authority (CA) of your choice.\nConvert your private key and SSL certificate files into formats that are supported by App Engine.\nFrom Console:\n1. Go to the App Engine page at Google Cloud Console.\n2. From the sidebar, navigate to Settings -> SSL certificates and click Upload a new certificate.\n3. Upload your concatenated SSL certificate under PEM encoded X.509 public key certificate, for example concat.crt, and then upload your RSA private key under Unencrypted PEM encoded RSA private key, for example  myserver.key.pem. Click Upload.\n4. Select the new certificate you just added from the certificate list, then select the domain being served by the old certificate.\n5. Click Save to transfer the mappings from the old certificate to the new one.","multiregional":true,"service":"App Engine"},"ecc-gcp-079":{"article":"Ensure that there are no static websites that you are not aware of being hosted on buckets.","impact":"There is a potential risk of exposure when you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go Cloud Storage Browser.\n2. Choose bucket.\n3. Click the More Actions icon and select Edit website configuration.\nLink https://cloud.google.com/storage/docs/hosting-static-website","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-434":{"article":"Use Customer-Managed Encryption Keys (CMEK) to encrypt node boot and dynamically-provisioned attached Google Compute Engine persistent Disks (PDs) using keys managed within Cloud Key Management Service (Cloud KMS).","impact":"Not using CSEK instead of the standard Google Compute Engine encryption can cause problems if your project is compromised and all data will be disclosed.","report_fields":["selfLink"],"remediation":"This cannot be remediated by updating an existing cluster. You must either recreate the desired node pool or create a new cluster.\nUsing Google Cloud Console:\nTo create a new node pool,\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Select Kubernetes clusters for which node boot disk CMEK is disabled\n3. Click ADD NODE POOL\n4. Ensure Boot disk type is 'Standard persistent disk' or 'SSD persistent disk'\n5. Select 'Enable customer-managed encryption for Boot Disk' and select the Cloud KMS encryption key you desire\n6. Click SAVE.\nTo create a new cluster,\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Click CREATE CLUSTER\n3. Under the 'default-pool' heading, click 'More options'\n4. In the Node pool edit window, select 'Standard persistent disk' or 'SSD Persistent Disk' as the Boot disk type\n5. Select 'Enable customer-managed encryption for Boot Disk' check box and choose the Cloud KMS encryption key you desire\n6. Configure the rest of the cluster settings as desired\n7. Click CREATE.\nUsing Command Line:\nCreate a new node pool using customer-managed encryption keys for the node boot disk, of [DISK_TYPE] either pd-standard or pd-ssd:\ngcloud beta container node-pools create [CLUSTER_NAME] --disk-type [DISK_TYPE] --boot-disk-kms-key projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]\nCreate a cluster using customer-managed encryption keys for the node boot disk, of [DISK_TYPE] either pd-standard or pd-ssd:\ngcloud beta container clusters create [CLUSTER_NAME] --disk-type [DISK_TYPE] --boot-disk-kms-key projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-240":{"article":"The slow query log feature designed for MySQL databases enables you to log queries that exceed a predefined time limit. By enabling the 'slow_query_log' flag, you can keep an eye on your MySQL database performance, allowing you to identify which queries need optimization.\nNote: By default, the 'slow_query_log' database flag is not enabled for Google Cloud MySQL instances.","impact":"By disabling the 'slow_query_log' flag, you make it difficult  to find inefficient or time-consuming SQL queries for your MySQL database instances to identify and troubleshoot sub-optimal database performance.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the MySQL instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'slow_query_log' flag from the drop-down menu and set the appropriate value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-089":{"article":"On Google's Global HTTP Load Balancer, each HTTPS target proxy is linked to a certificate. Ensure that SSL/TLS certificates are renewed one week before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and a GCP resource that implements the certificates is no longer secure.","report_fields":["selfLink"],"remediation":"In the Google Cloud Platform Console, you create a new SSL certificate resource when you create or edit a frontend for an HTTPS or SSL proxy load balancer.\nTo create a new SSL certificate resource for a frontend:\n1. Go to the Load balancing page in the Google Cloud Platform Console.\n2.Click the name of an HTTPS or SSL proxy load balancer.\n3.Click the Edit pencil.\n4.Select Frontend Configuration.\n-For an existing frontend, click the Edit pencil for that frontend. In the Certificate drop-down menu, select a visible certificate, then click Create a new certificate.\n-For a new frontend, in the Certificate drop-down menu, click Select a certificate, and then click Create a new certificate.\n5.In the Create a new certificate dialog box, enter a Name for your certificate resource, and optionally click Add a description and enter a description.\n6.Choose Upload my certificate.\n7.In the Public key certificate field, click the Upload button to upload your .crt file or paste the entire contents of your .key file into the field, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- that enclose the file contents.\n8.In the Certificate chain field, click the Upload button to upload your .csr file or paste the entire contents of the .csr file into the field, including the -----BEGIN CERTIFICATE REQUEST----- and -----END CERTIFICATE REQUEST----- that enclose the file contents.\n9.In the Private key certificate field, click the Upload button to upload your private key, using the .key file generated previously. This file uses, for example, -----BEGIN RSA PRIVATE KEY----- and -----END RSA PRIVATE KEY----- to enclose the file contents.\n10.Click Create.\n11.To create additional certificates, click the additional certificates link, click Select a Certificate, and then click Create a new certificate. Follow the previous steps to upload or paste the appropriate files.\nRefer to: https://cloud.google.com/load-balancing/docs/ssl-certificates","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-133":{"article":"With Intranode Visibility, all network traffic in your cluster is seen by the Google Cloud Platform network. This means you can see flow logs for all traffic between Pods, including traffic between Pods on the same node. And you can create firewall rules that apply to all traffic between Pods. This policy checks your cluster's intra-node visibility feature and generates an alert if it's disabled.","impact":"When you disable Intranode visibility can not ensure that packets sent between Pods are always processed by the VPC network, which ensures that firewall rules, routes, flow logs, and packet mirroring configurations apply to the packets. It can lead to insufficient response time to detecting any suspicious traffic between Pods. With Intranode visibility disabled, packets sent between Pods are not processed by the VPC network, which means that firewall rules, routes, flow logs, and packet mirroring configurations won't apply to the packets. It can lead to insufficient response time to detect any suspicious traffic between Pods.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Select Kubernetes clusters for which intranode visibility is disabled.\n3. Click on EDIT.\n4. Set 'Intranode visibility' to 'Enabled'.\n5. Click SAVE.\nUsing Command Line:\nTo enable intranode visibility on an existing cluster, run the following command:\ngcloud beta container clusters update [CLUSTER_NAME] \\\n--enable-intra-node-visibility","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-090":{"article":"Enforce the use of secure protocols TLS v1.1 and TLS v1.2 in an HTTP(S) Load balancer with CDN.","impact":"Using outdated and insecure ciphers for the SSL policies associated with your HTTPS/SSL Proxy load balancers could make the SSL connection between CDN and load balancers vulnerable to exploits.","report_fields":["selfLink"],"remediation":"From Console:\n1. Ensure that CDN is enabled on Load Balancer.\n2. Go to the Load balancing page in the Google Cloud Platform Console.\n3. Find the target Load Balancer name.\n4. Click the Edit pencil.\n5. Configure Backend.\n6. Choose HTTPS as protocol at the Backend configuration.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-109":{"article":"Ensure that an access is restricted from all ports and protocols.","impact":"Allowing unrestricted ingress access to uncommon ports can increase opportunities for malicious activities such as hacking and various types of attacks (brute-force attacks, Denial of Service (DoS) attacks, etc.) that may lead to data loss.","report_fields":["selfLink"],"remediation":"From Console:\n1. Select 'VPC network' from the side menu.\n2. Select 'Firewall rules'.\n3. Delete or Update all the firewall rules containing '0-65535' or 'all' in  the Protocols/ports column that allow ingress.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-436":{"article":"Instance addresses can be public IP or private IP. Public IP means that the instance is accessible through the public internet. In contrast, instances using only private IP are not accessible through the public internet, but are accessible through a Virtual Private Cloud (VPC). Limiting network access to the database will limit potential attacks.","impact":"An instance with a public IP address could potentially be compromised, and an attacker could gain anonymous access to it and other resources connected to this instance, which may lead to malicious activity with sensitive data.","report_fields":["selfLink"],"remediation":"From Google Cloud Console:\n1. In the Google Cloud console, go to the Cloud SQL Instances page.\n2. Open the Overview page of an instance by clicking the instance name.\n3. Select Connections from the SQL navigation menu.\n4. Check the Private IP checkbox. A drop-down list shows the available networks in your project.\n5. Select the VPC network you want to use:\n  If you see Private service connection required:\n  1. Click Set up connection.\n  2. In the Allocate an IP range section, choose one of the following options:\n  3. Select one or more existing IP ranges or create a new one from the dropdown. The dropdown includes previously allocated ranges, if there are any, or you can select Allocate a new IP range and enter a new range and name.\n  4. Use an automatically allocated IP range in your network.\n  Note: You can specify an address range only for a primary instance, not for a read replica or clone.\n  5. Click Continue.\n  6. Click Create connection.\n  7. Verify that you see the Private service connection for network VPC_NETWORK_NAME has been successfully created status.\n6. [Optional step for Private Services Access - review reference links to VPC documents for additional detail] If you want to allow other Google Cloud services such as BigQuery to access data in Cloud SQL and make queries against this data over a private IP connection, then select the Private path for Google Cloud services check box.\n7. Click Save\nFrom Google Cloud CLI:\n1. List cloud SQL instances\ngcloud sql instances list --format=\"json\" | jq '.[] | .connectionName,.ipAddresses'\nNote the project name of the instance you want to set to a private IP, this will be <PROJECT_ID>\nNote the instance name of the instance you want to set to a private IP, this will be <INSTANCE_ID>\nExample public instance output:\n\"my-project-123456:us-central1:my-instance\"\n[\n  {\n    \"ipAddress\": \"0.0.0.0\",\n    \"type\": \"PRIMARY\"\n  },\n  {\n  \"ipAddress\": \"0.0.0.0\",\n  \"type\": \"OUTGOING\"\n  }\n2. Run the following command to list the available VPCs\ngcloud compute networks list --format=\"json\" | jq '.[].name'\nNote the name of the VPC to use for the instance private IP, this will be <VPC_NETWORK_NAME>\n3. Run the following to set instance to a private IP\ngcloud beta sql instances patch <INSTANCE_ID> \\\n--project=<PROJECT_ID> \\\n--network=projects/<PROJECT_ID>/global/networks/<VPC_NETWORK_NAME> \\\n--no-assign-ip","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-210":{"article":"The user options option specifies global defaults for all users. It is recommended that 'user options' database flag for Cloud SQL SQL Server instance should not be configured.","impact":"The 'user options' option allows changing the default values of the SET options (if the server's default settings are not appropriate). A user can override these defaults by using the SET statement.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. Check the box next to the 'user options' flag.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Clear the user options database flag for every Cloud SQL SQL Server database instance using either of the below commands.\na. Clearing all flags to their default value:\ngcloud sql instances patch <INSTANCE_NAME> --clear-database-flags\nb. To clear only `user options` database flag, configure the database flag by overriding the `user options`. Exclude `user options` flag and its value, and keep all other flags you want to configure:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags [FLAG1=VALUE1,FLAG2=VALUE2]\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-170":{"article":"Secure Sockets Layer (SSL) policies determine what port Transport Layer Security (TLS) features clients are permitted to use when connecting to load balancers. To prevent usage of insecure features, SSL policies should use (a) at least TLS 1.2 with the MODERN profile; or (b) the RESTRICTED profile, because it effectively requires clients to use TLS 1.2 regardless of the chosen minimum TLS version; or (3) a CUSTOM profile that does not support any of the following features: TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA.","impact":"GCP default SSL policy uses a minimum TLS version of 1.0 and a Compatible profile, which allows the widest range of insecure cipher suites. As a result, it is easy for customers to configure a load balancer without even knowing that they are permitting outdated cipher suites.","report_fields":["selfLink"],"remediation":"From Console:\nIf the TargetSSLProxy or TargetHttpsProxy does not have an SSL policy configured, create a new SSL policy. Otherwise, modify the existing insecure policy.\n2. Navigate to the SSL Policies page by visiting 'https://console.cloud.google.com/net-security/sslpolicies'\n3. Click on the name of the insecure policy to go to its SSL policy details page.\n4. Click EDIT.\n5. Set Minimum TLS version to TLS 1.2.\n6. Set Profile to Modern or Restricted.\n7. Alternatively, if teh user selects the profile Custom, make sure that the following features are disabled: TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA.\nFrom Command Line:\n1. For each insecure SSL policy, update it to use secure cyphers:\ngcloud compute ssl-policies update NAME [--profile COMPATIBLE|MODERN|RESTRICTED|CUSTOM] --min-tls-version 1.2 [--custom-features FEATURES]\n2. If the target proxy has a GCP default SSL policy, use the following command corresponding to the proxy type to update it:\ngcloud compute target-ssl-proxies update TARGET_SSL_PROXY_NAME --ssl-policy SSL_POLICY_NAME\ngcloud compute target-https-proxies update TARGET_HTTPS_POLICY_NAME --ssl-policy SSL_POLICY_NAME","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-400":{"article":"It is recommended to set the 'password_history' database flag for Cloud SQL Mysql instance.","impact":"Using of 'password_history' database flag allow to establish password-reuse policy and prevent using the old passwords.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the Mysql instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'password_history' flag from the drop-down menu, and set its value.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database Instances\ngcloud sql instances list\n2. Configure the 'password_history' database flag for every Cloud SQL Mysql database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags password_history={number of subsequent account password changes that must occur before the password can be reused}\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-134":{"article":"Istio is an open service mesh that provides a uniform way to connect, manage, and secure microservices. It supports managing traffic flows between services, enforcing access policies, and aggregating telemetry data, all without requiring changes to the microservice code. This policy checks your cluster for the Istio  add-on feature and alerts if it is not enabled.","impact":"istioConfig provides automatic load balancing that helps against potential DDoS attacks. In addition, it provides secure communication between services in the cluster with strong authentication and identity-based authorization, which is a uniform way to connect, manage, and secure microservices instead of different unsafe accesses.","report_fields":["selfLink"],"remediation":"Add Istio to your existing cluster.\nIf you want to update a cluster with the add-on, you may need first to resize your cluster to ensure that you have enough resources for Istio. As for creation of a new cluster, we suggest at least a 4 node cluster with the 2 vCPU machine type. Your cluster must also be running a supported cluster master version to use the add-on.\n1. Go to the Kubernetes clusters page in GCP Console and select the cluster you want to update.\n2. Select Edit.\n3. Select Add-ons to display possible add-ons, including Istio on GKE.\n4. Select Enabled under Istio.\n5. From the drop-down list, select the mTLS security mode you want to use for your cluster.\n6. Click Save to update your cluster.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-233":{"article":"Ensure that your virtual machine disk images are not publicly shared with all other Google Cloud Platform (GCP) accounts in order to avoid exposing sensitive or confidential data. If required, you can share your disk images with specific GCP accounts only, without making them public.","impact":"Granting permissions to 'allUsers' and 'allAuthenticatedUsers' members can allow anyone to access your compute image that breaches data privacy.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Compute Image page at Google Cloud Console.\n2. Select an image from Resources.\n3. Review each role and find Members having allUsers or allAuthenticatedUsers.\n4. Click Delete icon and confirm by clicking on REMOVE.\n5. Click Done.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-279":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the FTP port (20) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted FTP access to your Google Cloud virtual machine (VM) instances via VPC network firewall rules can increase opportunities for malicious activities such as brute-force attacks, FTP bounce attacks, spoofing, and packet capture attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-114":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the MongoDB port (27017) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 27017 (MongoDB) via VPC network firewall rules can increase opportunities for malicious activities such as hacking, brute-force attacks, and SQL injection attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-116":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the NetBIOS-SSN port (139) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound access on port 139 (NetBIOS-SSN) via VPC network firewall rules can increase opportunities for malicious activities such as DDoS attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-305":{"article":"This rule detects when Dataproc cluster is anonymously or publicly accessible.\nA Dataproc cluster contains at least one \"management\" VM and one \"compute\" VM. Access to Dataproc clusters is controlled via IAM policies. These IAM policies can be set for public access via the allUsers and allAuthenticatedUsers IAM principals which can inadvertently expose your data to the public.","impact":"Granting permissions to allUsers or allAuthenticatedUsers allows anyone to access the Dataproc cluster. Such access might not be desirable if sensitive data is in the Dataproc cluster.","report_fields":["clusterName","projectId"],"remediation":"From Console:\n1. Log in to the GCP Console at https://console.cloud.google.com.\n2. Navigate to Clusters.\n3. Select the target Dataproc cluster.\n4. Expand the Info Panel by selecting Show Info Panel.\n5. To remove a specific role assignment, select allUsers or allAuthenticatedUsers, and then click Remove member.","multiregional":true,"service":"Dataproc"},"ecc-gcp-286":{"article":"Policy identifies GCP Pub/Sub topics that are not encrypted using a customer-managed encryption key. It is a best practice to use customer-managed KMS Keys to encrypt your Pub/Sub topic.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of keys.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nOnce the Pub/Sub topic is created it is not modifiable.\nTo remediate you need to create one new topic with encrypted using customer-managed encryption key following below link and then delete the alerted topic.\nhttps://cloud.google.com/pubsub/docs/encryption","multiregional":true,"service":"Pub/Sub"},"ecc-gcp-447":{"article":"VM disk is not attached to any instance.","impact":"Keeping unused VM disk will increase your monthly bill.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Disks page in the Google Cloud Platform Console https://console.cloud.google.com/compute/disks.\n2. Click on reported disk and Delete it or Create Instance.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-180":{"article":"Enabling the 'log_lock_waits' flag for a PostgreSQL instance creates a log for any session waits that take longer than the allotted deadlock_timeout time to acquire a lock.","impact":"Not logging such waits on locks by enabling the 'log_lock_waits' database flag can lead to insufficient response time to identify poor performance due to locking delays.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the flag 'log_lock_waits' from the drop-down menu and set the value as 'on'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_lock_waits' database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_lock_waits=on\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('='). Default Value By default log_lock_waits is off.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-171":{"article":"Ensure that your instance is not configured to use the default Compute Engine service account as it is associated with the Editor role on the project.","impact":"Using the default Compute Engine service account can lead to privilege escalations. If your VM is compromised, it allows attackers to gain access to all your projects with the Editor role.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to the VM instances page using 'https://console.cloud.google.com/compute/instances'.\n2. Click on the instance name to go to its 'VM instance details' page.\n3. Click on 'Stop' and then click on 'Edit'\n4. Under the 'Service Account' section, select a service account other that the default Compute Engine service account. You may first need to create a new service account.\n5. Click on 'Save' and then click on 'Start'.\nFrom Command Line:\n1. Stop the instance:\ngcloud compute instances stop INSTANCE_NAME\n2. Update the instance:\ngcloud compute instances set-service-account INSTANCE_NAME --serviceaccount=SERVICE_ACCOUNT\n3. Restart the instance:\ngcloud compute instances start INSTANCE_NAME","multiregional":true,"service":"Compute Engine"},"ecc-gcp-062":{"article":"Private Google Access enables your cluster hosts, which have only private IP addresses, to communicate with Google APIs and services using an internal IP address rather than an external IP address. External IP addresses are routable and reachable over the Internet. Internal (private) IP addresses are internal to Google Cloud Platform and are not routable or reachable over the Internet. You can use Private Google Access to allow VMs without Internet access to reach Google APIs, services, and properties that are accessible over HTTP/HTTPS.","impact":"Without Private Google Access, VM instances that need to reach the Google Cloud and Developer APIs and services require an external IP. Having an external IP exposes VM instances to the public Internet, which increases the attack surface.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to Kubernetes GCP Console using https://console.cloud.google.com/kubernetes/list.\n2. From the list of clusters, for each cluster, note the Subnet name.\n3. Go to VPC network GCP Console using https://console.cloud.google.com/networking/networks/list.\n4. Click the noted subnet, the Subnet details page is displayed.\n5. Click on Edit.\n6. Set Private Google access to On.\n7. Click on Save.\nUsing Command Line:\nTo set Private Google access for a network subnet, run the following command:\ngcloud compute networks subnets update [SUBNET_NAME] --region [REGION] --enable-private-ip-google-access","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-132":{"article":"Application-layer Secrets Encryption provides an additional layer of security for sensitive data, such as Secrets, stored in etcd. Using this functionality, you can use a key, that you manage in Cloud KMS, to encrypt data at the application layer. This protects against the attackers gaining access to an offline copy of etcd. This policy checks your cluster for the Application-layer Secrets Encryption security feature and alerts if it is not enabled. Enabling application-layer secrets encryption for your GKE clusters is considered a security best practice for applications that store sensitive and confidential data.","impact":"Without this feature, you can not use an encryption key managed with Cloud KMS to encrypt data at the application layer and protect against attackers that gain access to an offline copy of etcd.","report_fields":["selfLink"],"remediation":"To enable Application-layer Secrets Encryption, several configuration items are required. These include:\n- A key ring\n- A key\n- A GKE service account with Cloud KMS CryptoKey Encrypter/Decrypter role\nOnce these are created, Application-layer Secrets Encryption can be enabled on an existing or new cluster.\nUsing Google Cloud Console:\nTo create a key:\n1. Go to Cloud KMS by visiting https://console.cloud.google.com/security/kms\n2. Select CREATE KEY RING.\n3. Enter a Key ring name and the region where the keys will be stored.\n4. Click CREATE.\n5. Enter a Key name and appropriate rotation period within the Create key pane.\n6. Click CREATE.\nTo enable on a new cluster:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Click CREATE CLUSTER.\n3. Expand the template by clicking 'Availability, networking, security, and additional features' and check the 'Enable Application-layer Secrets Encryption' checkbox.\n4. Select the desired Key as the customer-managed key and if prompted grant permissions to the GKE Service account.\n5. Click CREATE.\nTo enable on an existing cluster:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Click to edit cluster you want to modify.\n3. Enable Application-layer Secrets Encryption and choose the desired Key.\n4. Click SAVE.\nUsing Command Line:\nTo create a key:\n  Create a key ring:\n  gcloud kms keyrings create [RING_NAME] \\\n  --location [LOCATION] \\\n  --project [KEY_PROJECT_ID]\n  Create a key:\n  gcloud kms keys create [KEY_NAME] \\\n  --location [LOCATION] \\\n  --keyring [RING_NAME] \\\n  --purpose encryption \\\n  --project [KEY_PROJECT_ID]\nGrant the Kubernetes Engine Service Agent service account the Cloud KMS CryptoKey Encrypter/Decrypter role:\ngcloud kms keys add-iam-policy-binding [KEY_NAME] \\\n--location [LOCATION] \\\n--keyring [RING_NAME] \\\n--member serviceAccount:[SERVICE_ACCOUNT_NAME] \\\n--role roles/cloudkms.cryptoKeyEncrypterDecrypter \\\n--project [KEY_PROJECT_ID]\nTo create a new cluster with Application-layer Secrets Encryption:\ngcloud container clusters create [CLUSTER_NAME] \\\n--cluster-version=latest \\\n--zone [ZONE] \\\n--database-encryption-key\nprojects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME] \\\n--project [CLUSTER_PROJECT_ID]\nTo enable on an existing cluster:\ngcloud container clusters update [CLUSTER_NAME] \\\n--zone [ZONE] \\\n--database-encryption-key\nprojects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME] \\\n--project [CLUSTER_PROJECT_ID]","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-110":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the DNS port (53) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted DNS access to your Google Cloud virtual machines (VMs) through VPC network firewall rules can increase opportunities for malicious activities such as Denial of Service (DoS) attacks and Distributed Denial of Service (DDoS) attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to  GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-223":{"article":"The default network in GCP region is generally considered insecure and should be avoided. The default firewall and routes are generic in nature and do not lock down sensitive ports or traffic routing.","impact":"Automatically created firewall rules of the default network have a number of open ports, including port 22. This may increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromise.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to VM instances page https://console.cloud.google.com/compute/instances.\n2. Select the VM instance whose configuration you want to change.\n3. Click STOP.\n4. Click EDIT.\n5. In Network interfaces section, click the edit icon.\n6. Select Network and Subnetwork that are not default.\n7. Click Save.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-189":{"article":"It is recommended to store objects in a multi-region or dual-region locations to ensure data availability in the event of large-scale disruptions.","impact":"Geo-redundancy ensures maximum availability of your data, even in the event of large-scale disruptions such as natural disasters.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Open the Cloud Storage browser in the Google Cloud Console.\n2. Click Create bucket to open the bucket creation form.\n3. Enter your bucket information and click Continue to complete each step:\n  3.1 Specify a Name, subject to the bucket name requirements.\n  3.2 Select the Multi-region Location type.\n  3.3 Select the Default storage class for the bucket. The default storage class is assigned by default to all objects uploaded to the bucket.\n  3.4. Select the Access control model to determine how you control access to the bucket's objects. Optionally, you can add bucket labels, set a retention policy, and choose an encryption method.\n4. Click Create.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-190":{"article":"Enabling retention policies on log buckets will protect logs stored in cloud storage buckets from being overwritten or accidentally deleted. It is recommended to set up retention policies and configure Bucket Lock on all storage buckets that are used as log sinks.","impact":"Sinks can be configured to export logs in storage buckets. It is recommended to configure a data retention policy for these cloud storage buckets and to lock the data retention policy, thus permanently preventing the policy from being reduced or removed. This way, if the system is ever compromised by an attacker or a malicious insider who wants to cover their tracks, the activity logs are definitely preserved for forensics and security investigations.","report_fields":["selfLink"],"remediation":"From Console:\n1. If sinks are not configured, first follow the instructions in the recommendation:\nEnsure that sinks are configured for all Log entries.\n2. For each storage bucket configured as a sink, go to the Cloud Storage browser at https://console.cloud.google.com/storage/browser/<BUCKET_NAME>\n3. Select the Bucket Lock tab near the top of the page.\n4. In the Retention policy entry, click the Add Duration link. The 'Set a retention policy' dialog box appears.\n5. Enter the desired length of time for the retention period and click 'Save policy'.\n6. Set the 'Lock status' for this retention policy to 'Locked'.\nFrom Command Line:\n1. To list all sinks destined to storage buckets:\ngcloud logging sinks list --folder=FOLDER_ID | --organization=ORGANIZATION_ID | --project=PROJECT_ID\n2. For each storage bucket listed above, set a retention policy and lock it:\ngsutil retention set [TIME_DURATION] gs://[BUCKET_NAME]\ngsutil retention lock gs://[BUCKET_NAME]\nFor more information, visit https://cloud.google.com/storage/docs/using-bucket-lock#set-policy","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-115":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the MySQL DB port (3306) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 3306 (MySQL DB) via VPC network firewall rules can increase opportunities for malicious activities such as hacking, brute-force attacks, and SQL injection attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-176":{"article":"It is recommended to set the 'local_infile' database flag for a Cloud SQL MySQL instance to 'off'.","impact":"Enabling the 'local_infile' database flag controls server-side LOCAL capability, and the server permits local data loading by the clients requesting it.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the MySQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'local_infile' flag from the drop-down menu, and set its value to 'off'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'local_infile' database flag for every Cloud SQL MySQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags local_infile=off\nNote: This command will overwrite all database flags that were previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-261":{"article":"Ensure that your Google Cloud Dataproc clusters are encrypted with Customer-Managed Keys (CMKs) in order to have a fine control over the cluster data encryption/decryption process. You can create and manage your own Customer-Managed Keys (CMKs) with Cloud Key Management Service (Cloud KMS). Cloud KMS provides secure and efficient encryption key management, controlled key rotation, and revocation mechanisms.\nBy default, the Dataproc service encrypts all data at rest using Google-managed encryption keys. The Dataproc cluster data is encrypted using a Google-generated Data Encryption Key (DEK) and a Key Encryption Key (KEK). If you need to control and manage your cluster data encryption yourself, you can use your own Customer-Managed Keys (CMKs). Cloud KMS Customer-Managed Keys can be implemented as an additional security layer on top of existing data encryption, and are often used in the enterprise world, where compliance and security controls are very strict.","impact":"Not using Customer-Managed Keys results in less control over aspects of your keys' lifecycle and management.","report_fields":["clusterName","projectId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Login to the GCP Console and navigate to the Dataproc Cluster page by visiting https://console.cloud.google.com/dataproc/clusters.\n2. Select the project from the projects dropdown list.\n3. On the Dataproc Cluster page, click on the Create Cluster to create a new cluster with Customer managed encryption keys.\n4. On Create a cluster page, perform below steps:\n  \u2022 Inside Set up cluster section perform below steps:\n  -In the Name textbox, provide a name for your cluster.\n    o From Location select the location in which you want to deploy a cluster.\n    o Configure other configurations as per your requirements.\n  \u2022 Inside Configure Nodes and Customize cluster section configure the settings as per your requirements.\n  \u2022 Inside Manage security section, perform below steps:\n    o From Encryption, select Customer-managed key.\n    o Select a customer-managed key from dropdown list.\n    o Ensure that the selected KMS Key have Cloud KMS CryptoKey Encrypter/Decrypter role assign to Dataproc Cluster service account (\"serviceAccount:service-<project_number>@compute-system.iam.gserviceaccount.com\").\n    o Click on Create to create a cluster.\n  \u2022 Once the cluster is created migrate all your workloads from the older cluster to the new cluster and delete the old cluster by performing the below steps:\n    o On the Clusters page, select the old cluster and click on Delete cluster.\n    o On the Confirm deletion window, click on Confirm to delete the cluster.\n    o Repeat step above for other Dataproc clusters available in the selected project.\n  \u2022 Change the project from the project dropdown list and repeat the remediation procedure for other Dataproc clusters available in other projects.","multiregional":true,"service":"Dataproc"},"ecc-gcp-053":{"article":"A node in a degraded state is an unknown quantity and so may pose a security risk.","impact":"Lack of automatic recovery of Kubernetes Engine nodes, you can lead to insufficient response time to initiate a recovery process for a cluster node that fails sequential health checks within the specified threshold time.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list.\n2. Select Kubernetes clusters for which node auto-repair is disabled.\n3. Click on the name of the Node pool that requires node auto-repair to be enabled.\n4. Within the Node pool details pane, click EDIT.\n5. Under the 'Management' heading, ensure the 'Enable Auto-repair' box is checked.\n6. Click on SAVE.\nUsing Command Line:\nTo enable node auto-repair for an existing cluster with Node pool, run the following command:\ngcloud container node-pools update $POOL_NAME --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --enable-autorepair","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-303":{"article":"A Dataflow job consists of at least one management node and one compute node (both are GCE VMs). By default, these nodes are configured with public IPs that allow them to communicate with the public internet","impact":"Public IP increases your potential attack surface by being publicly accessible.","report_fields":["projectId","id"],"remediation":"From Console:\n1. Go to Dataflow jobs https://console.cloud.google.com/dataflow/jobs.\n2. Drain or cancel your job and then re-create with the Private IP in the Worker IP Address Configuration.","multiregional":true,"service":"Dataflow"},"ecc-gcp-236":{"article":"Ensure that MySQL database instances are using the latest major version of MySQL database in order to receive the latest database features and benefit from enhanced performance and security.","impact":"If your MySQL database instances are not using the latest major version of the MySQL database, you cannot benefit from security updates.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nMySQL database version cannot be automatically upgraded within Google Cloud Platform (GCP).\nTo upgrade your Google Cloud MySQL instances to the latest major version of the MySQL database, you have to re-create the existing instance, export data from the existing (source) instance, and importing that data into a new (target) instance running the latest major version of MySQL.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-247":{"article":"This rule detects when a function are not in 'ACTIVE' node. 'ACTIVE' mode means that function has been successfully deployed and is serving.","impact":"When your function is not in \"ACTIVE\" mode, it means that it cannot trigger when an event being watched is fired.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select the relevant function.\n3. Click DELETE.\n4. In the confirmation box click Delete.\nFrom Command Line:\ngcloud functions delete FUNCTION_NAME --region=REGION","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-086":{"article":"Ensure that SSL/TLS certificates stored in Cloud SQL are renewed one month before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid, and the communication between a client and a GCP resource that implements the certificates is no longer secure.","report_fields":["selfLink"],"remediation":"You must take the following steps to complete the rotation:\n1.Download the new server certificate information.\n2.Update your clients to use the new server certificate information.\n3.Complete the rotation, which moves the currently active certificate into the \"previous\" slot and updates the newly added certificate to be the active certificate.\nDownload the new server certificate information:\n1.Go to the Cloud SQL Instances page in the Google Cloud Platform Console.\n2.Click the instance name to open its Instance details page.\n3.Select the CONNECTIONS tab.\n4.Scroll down to the Configure SSL server certificates section.\n5.Click Create new certificate.\n6.Scroll down to Download SSL server certificates section.\n7.Click Download.\nThe server certificate information, encoded as a PEM file, is displayed and can be downloaded to your local environment.\n8.Update all of your clients to use the new information.\nAfter you have updated your clients, complete the rotation:\n1.Return to the Configure SSL server certificates section.\n2.Click Rotate certificate.\n3.Confirm that your clients are connecting properly.\nIf any clients are not connecting using the newly rotated certificate, you can click Rollback certificate to roll back to the previous configuration.\nLink: https://cloud.google.com/sql/docs/mysql/configure-ssl-instance","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-222":{"article":"To support principle of least privileges and prevent potential privilege escalation instance templates should not be assigned to the default Compute Engine service account, or a service account that provides full access to all Cloud APIs.","impact":"Using the default service account on your instance template can increase malicious activity to the point that an attacker gains full control of the project.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nWe can't Edit an exisitng Instance Template\nFrom Console:\n1. In the Google Cloud Console, go to the Instance Template page https://console.cloud.google.com/compute/instanceTemplates.\n2. Click on Create Instance Template\n3. Scroll down to the Service Account section.\n4. Select a different service account or ensure that Allow full access to all Cloud APIs is not selected.\n5. Click on Create and Delete old template.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-197":{"article":"Google Cloud encrypts data at-rest and in-transit, but customer data must be decrypted for processing. Confidential Computing is a breakthrough technology which encrypts data in-use while it is being processed. Confidential Computing environments keep data encrypted in memory and elsewhere outside the central processing unit (CPU).\nConfidential VMs leverage the Secure Encrypted Virtualization (SEV) feature of AMD EPYC CPUs. Customer data will stay encrypted while it is used, indexed, queried, or trained on. Encryption keys are generated in hardware, per VM, and are not exportable. Thanks to built-in hardware optimizations of both performance and security, there is no significant performance penalty to Confidential Computing workloads.","impact":"Confidential Computing enables customers' sensitive code and other data encrypted in memory during processing. Confidential VM can help alleviate concerns about risk related to either dependency on Google infrastructure or Google insiders' access to customer data in the clear.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Console:\n1. Go to the VM instances page using https://console.cloud.google.com/compute/instances.\n2. Click CREATE INSTANCE.\n3. Fill out the desired configuration for your instance.\n4. Under the 'Confidential VM service' section, check the option Enable the Confidential Computing service on this VM instance.\n5. Click Create.\nFrom Command Line:\nCreate a new instance with Confidential Compute enabled.\ngcloud compute instances create <INSTANCE_NAME> --zone <ZONE> --confidential-compute --maintenance-policy=TERMINATE","multiregional":true,"service":"Compute Engine"},"ecc-gcp-048":{"article":"Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.","impact":"Lack of Stackdriver Kubernetes monitoring can result in insufficient response time to detect issues due to inaccessibility of audit data after a cluster security event and lack of a centralized place to analyze log data and metrics collected from multiple sources.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\n2. Select Kubernetes clusters for which monitoring is disabled.\n3. Click on EDIT.\n4. Set 'Stackdriver Monitoring' to 'Enabled'.\n5. Click SAVE.\nUsing Command Line:\nTo enable Stackdriver Monitoring for an existing cluster, run the following command:\ngcloud container clusters update [CLUSTER_NAME] --zone [COMPUTE_ZONE] --monitoring-service monitoring.googleapis.com","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-274":{"article":"By default, for each policy you start with one rule that allows/denies all traffic with the lowest priority. To conform with least privilege access principles, set the default action as deny and then add allow/deny rules with higher priority as necessary.","impact":"Not setting the default action as deny with higher priority for security policy can increase opportunities for malicious activities such as Denial-of-Service (DoS) attacks.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud Armor page in the Cloud Console https://console.cloud.google.com/net-security/securitypolicies/list.\n2. On the Security policies page, click the name of the security policy. The Policy details page is displayed.\n3. Click the Edit button.\n4. Under Default rule action select Deny.\n5. Click Update.","multiregional":true,"service":"Cloud Armor"},"ecc-gcp-150":{"article":"Check to ensure that there are no public access to the Stackdriver logs storing on the Storage Buckets. Stackdriver Logging allows to store, search, investigate, monitor and alert on log information/events from Google Cloud Platform. The permissions needs to be set only for authorized users. Public access to Stackdriver Logs will enable anyone with a web association to retrieve sensitive information critical to business.","impact":"Public access to Stackdriver Logs will enable anyone with a web association to retrieve sensitive information critical to business.","report_fields":["selfLink"],"remediation":"1. Login to the GCP Portal.\n2. Go to Storage (Left Panel) and click Browser.\n3. Choose the identified Storage Bucket whose ACL needs to be modified.\n4. Click on the SHOW INFO PANEL button.\n5. Check all the ACL groups and make sure that none of them are set to 'allUsers' or 'allAuthenticatedUsers'.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-243":{"article":"This rule detects when a function is configured with ingress allow all traffic. Restrict the traffic from the Internet and other resources, to get better network-based access control and allow only vpc resources traffic to enter or traffic through load balancer.","impact":"Allowing all traffic for functions allows all incoming requests from both the Internet and resources within the project and can increase opportunities for malicious activities such as hacking and various types of attacks (brute-force attacks, Denial of Service (DoS) attacks, etc.).","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a function.\n3. Click on edit.\n4. Click on Runtime, build, connections and security settings.\n5. Click on Connections.\n6. On Ingress Settings select either Allow internal trafffic only or Allow internal trafffic and from Cloud load balancing.\n7. Click on next.\n8. Click on Deploy.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-282":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the redis port (6379) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 6379 (redis) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-140":{"article":"This policy identifies a SQL DB instance that does not have any Labels. Labels can be used for easy identification and search.","impact":"SQL Instances without labels make it difficult to organize resources available within your GCP environment. As your GCP environment is becoming more and more complex, it requires better management strategies.","report_fields":["selfLink"],"remediation":"1. Login to GCP Console.\n2. In the left Navigation pane, click on SQL.\n3. Select the reported SQL instance.\n4. Click on EDIT and add labels with the appropriate KeyValue information.\n5. Click on Save.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-055":{"article":"Use Container-Optimized OS (cos_containerd) as a managed, optimized and enhanced base OS that limits the host's attack surface.","impact":"Not using COS can increase the attack surface of your instance so that instances may not include a locked-down firewall and other security settings by default.","report_fields":["selfLink"],"remediation":"Using Google Cloud Console:\n1. Go to Kubernetes Engine using https://console.cloud.google.com/kubernetes/list.\n2. Select the Kubernetes cluster that does not use COS.\n3. Under the Node pools heading, select the Node Pool that requires alteration.\n4. Click EDIT.\n5. Under the Image Type heading, click CHANGE.\n6. From the pop-up menu, select Container-Optimized OS (cos_containerd) and click CHANGE.\n7. Repeat for all non-compliant Node pools.\nUsing Command Line:\nTo set the node image to COS for an existing cluster's Node pool:\ngcloud container clusters upgrade [CLUSTER_NAME] --image-type cos_containerd --zone [COMPUTE_ZONE] --node-pool [POOL_NAME]","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-gcp-239":{"article":"When automatic storage increases is enable, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity. If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 64 TB.\nNote: You can increase storage size, but you cannot decrease it; the storage increases are permanent for the life of the instance. When you enable this setting, a spike in storage requirements can permanently increase storage costs (incrementally) for your instance. If an instance runs out of available space, it can cause the instance to go offline, and the Cloud SQL SLA might not cover the outage.\nFor more information https://cloud.google.com/sql/docs/sqlserver/instance-settings.","impact":"If a database instance runs out of available space, it can drop existing connections and cause downtime, and Google Cloud SQL Service Level Agreement (SLA) might not cover the outage.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL instance for which you want to enable automatic storage increases.\n3. Click Edit.\n4. Scroll down to the Storage section.\n5. Check the box next to the 'Enable automatic storage increases'.\n6. Click Save to save your changes.\n7. Confirm your changes in the Configuration block on the Overview page.","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-285":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the WinRM port (5985 or 5986) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound/ingress access on TCP port 5985 or 5986 (WinRM) via VPC network firewall rules can increase opportunities for malicious activities such as hacking and brute-force attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-312":{"article":"This rule detects when Datafusion instance has stackdriver monitoring disabled.\nStackdriver monitoring collects metrics, events, and metadata. Google Cloud's operations suite ingests that data and generates insights via dashboards, charts, and alerts.","impact":"Lack of Stackdriver Datafusion monitoring can result in insufficient response time to detect issues.","report_fields":["name"],"remediation":"From Command Line:\n1. After you create your instance, you cannot enable Cloud Monitoring in the console. Instead, run this gcloud CLI command:\ngcloud beta data-fusion instances update INSTANCE_NAME --project=PROJECT_ID --location=LOCATION --enable_stackdriver_monitoring","multiregional":true,"service":"Cloud Data Fusion"},"ecc-gcp-249":{"article":"This policy identifies GCP Cloud Functions for which the HTTP trigger is not secured. When you configure HTTP functions to be triggered only with HTTPS, user requests will be redirected to use the HTTPS protocol, which is more secure. It is recommended to set the 'Require HTTPS' for configuring HTTP triggers while deploying your function.","impact":"Requests for a URL that match HTTP are not automatically redirected to the HTTPS URL with the same path, which can lead to a breach of confidentiality.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions Overview page https://console.cloud.google.com/functions.\n2. Click on the alerting function.\n3. Click on 'EDIT'.\n4. Under section 'Trigger', click on 'EDIT'.\n5. Select the checkbox against the field 'Require HTTPS'.\n6. Click on 'SAVE'.\n7. Click on 'NEXT'.\n8. Click on 'DEPLOY'.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-219":{"article":"To defend against advanced threats and ensure that the boot loader and firmware on your VMs are signed and untampered, it is recommended that Compute instances are launched with Shielded VM enabled. If vTPM or EnableIntegrityMonitoring is not set to true or not present then it means instances are not launched with shielded VM enabled.","impact":"Without Shilded VM protection it may be possible to compromise the integrity of boot loader and firmware of the VM. This can allow an attacker to inject malicious code there.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nUsing Console:\n1. Go to the instance template page using https://console.cloud.google.com/compute/instanceTemplates/list.\n2. Replace all affected templates with new ones with turned on vTPM and Integrity monitoring in the Security section.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-018":{"article":"Google Cloud Platform services write audit log entries to Admin Activity and Data Access logs to help answer the questions of \"who did what, where, and when?\" within Google Cloud Platform projects. Cloud Audit logging records information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the GCP services. Cloud Audit logging provides a history of GCP API calls for an account, including API calls made via the Console, SDKs, command line tools, and other GCP services.","impact":"Lack of monitoring and logging of Audit Configuration Changes can lead to insufficient response time to detect accidental or intentional changes that may result in unauthorized access.","report_fields":["projectId"],"remediation":"From Console:\nCreate the prescribed log metric:\n1. Go to Logging/Logs-based Metrics using https://console.cloud.google.com/logs/metrics and click \"CREATE METRIC\".\n2. Click the down arrow symbol on the Filter Bar at the rightmost corner and select Convert to Advanced Filter.\n3. Clear any text and add:\nprotoPayload.methodName=\"SetIamPolicy\" AND\nprotoPayload.serviceData.policyDelta.auditConfigDeltas:*\n4. Click Submit Filter. Display logs appear based on the filter text entered by the user.\n5. In the Metric Editor menu on the right, fill out the name field. Set Units to 1 (default) and Type to Counter. This will ensure that the log metric counts the number of log entries matching the user's advanced logs query.\n6. Click Create Metric.\nCreate the prescribed Alert Policy:\n1. Identify the new metric the user has just created under the User-defined Metrics section at https://console.cloud.google.com/logs/metrics.\n2. Click the 3-dot icon in the rightmost column for the new metric and select Create alert from Metric. A new page opens.\n3. Fill out the alert policy configuration and click Save. Choose the alerting threshold and configuration that makes sense for the organization. For example, a threshold of zero(0) for the most recent value will ensure that a notification is triggered for every owner change in the project:\nSet 'Aggregator' to 'Count'\nSet 'Configuration':\n- Condition: above\n- Threshold: 0\n- For: most recent value\n4. Configure the desired notifications channels in the section Notifications.\n5. Name the policy and click Save.","multiregional":true,"service":"Cloud Logging"},"ecc-gcp-188":{"article":"It is recommended that the IAM policy on BigQuery datasets do not allow anonymous and/or public access","impact":"Granting permissions to allUsers or allAuthenticatedUsers allows anyone to access the dataset. Such access might not be desirable if sensitive data is being stored in the dataset.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to 'BigQuery' using https://console.cloud.google.com/bigquery.\n2. Select the dataset from 'Resources'.\n3. Click SHARING near the right side of the window and select Permissions\n4. Review each attached role.\n5. Click the 'Delete' icon for each 'allUsers' or 'allAuthenticatedUsers' member. In the pop-up menu, click 'Remove'.\nFrom Command Line:\n1. Retrieve the data set information:\nbq show --format=prettyjson PROJECT_ID:DATASET_NAME > PATH_TO_FILE\n2. In the access section of a JSON file, update the dataset information to remove all roles containing 'allUsers' or 'allAuthenticatedUsers'.\n3. Update the dataset:\nbq update --source PATH_TO_FILE PROJECT_ID:DATASET_NAME","multiregional":true,"service":"BigQuery"},"ecc-gcp-179":{"article":"Enabling the 'log_disconnections' setting logs the end of each session, including the session duration.","impact":"Disabled 'log_disconnections' can lead to insufficient response time to identify, troubleshoot, and repair configuration errors and sub-optimal performance for your Cloud PostgreSQL database instances.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_disconnections' flag from the drop-down menu and set the value as 'on'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nFrom Command Line:\n1. List all Cloud SQL database Instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_disconnections' database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_disconnections=on\nNote: This command will overwrite all previously set database flags. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-042":{"article":"Storage Access Logging generates a log that contains access records for each request made to the Storage bucket. Cloud Storage offers access logs and storage logs in the form of CSV files that can be downloaded and used for analysis/incident response. Access logs provide information for all of the requests made on a specified bucket and are created hourly, while the daily storage logs provide information about the storage consumption of that bucket for the last day. The access logs and storage logs are automatically created as new objects in a bucket that you specify. An access log record contains details about the request, such as the request type, the resources specified in the request worked, and the time and date the request was processed. While storage Logs helps to keep track of the amount of data stored in the bucket.\nIt is recommended that storage Access Logs and Storage logs are enabled for every Storage Bucket.","impact":"Lack of logging can lead to failure to detect the events which may affect objects within target buckets.","report_fields":["selfLink"],"remediation":"Using Gsutils:\nTo set Storage Access Logs and Storage logs for a bucket run:\ngsutil logging set on -b gs//<bucketName for a bucket used to store logs> gs//<your bucket name>","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-182":{"article":"PostgreSQL can create a temporary file for actions such as sorting, hashing and temporary query results when these operations exceed 'work_mem'. The 'log_temp_files' flag controls logging names and the file size when it is deleted. Configuring 'log_temp_files' to '0' causes all temporary file information to be logged, while positive values log only files whose size is greater than or equal to the specified number of kilobytes. A value of '-1' disables temporary file information logging.","impact":"If all temporary files are not logged, it can be more difficult to identify potential performance issues. Those may be caused either by poor application coding or by deliberate resource starvation attempts.","report_fields":["selfLink"],"remediation":"Using Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the PostgreSQL instance where the database flag needs to be enabled.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'log_temp_files' flag from the drop-down menu and set the value as '0'.\n6. Click Save.\n7. Confirm the changes under Flags on the Overview page.\nUsing Command Line:\n1. List all Cloud SQL database instances using the following command:\ngcloud sql instances list\n2. Configure the 'log_temp_files' database flag for every Cloud SQL PosgreSQL database instance using the below command:\ngcloud sql instances patch INSTANCE_NAME --database-flags log_temp_files='0'\nNote: This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags to be set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign ('=').","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-071":{"article":"Firewall rules provide stateful filtering of ingress/egress network traffic to GCP resources. It is recommended that no security group allow unrestricted ingress access","impact":"Allowing unrestricted ingress access to uncommon IP addresses increases opportunities for malicious activities such as hacking and various types of attacks (brute-force attacks, Denial of Service (DoS) attacks, etc.) that may lead to data loss.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to VPC Network.\n2. Go to Firewall Rules.\n3. Click the Firewall Rule you want to modify.\n4. Click Edit.\n5. Modify Source IP ranges to specific IP.\n6. Click Save.\nFrom Command Line:\n1.Update Firewall rule with new SOURCE_RANGE from below command:\ngcloud compute firewall-rules update FirewallName --allow=[PROTOCOL[PORT[-PORT]],...] --source-ranges=[CIDR_RANGE,...]","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-137":{"article":"This policy identifies Load balancer HTTPS target proxies which are configured with a default SSL Policy instead of a custom SSL policy. The best practice is to use a custom SSL policy to access Load balancers. It gives you closer control over SSL/TLS versions and ciphers.","impact":"Without custom SSL certificates, you will not be able to give closer control over SSL/TLS versions and ciphers.","report_fields":["selfLink"],"remediation":"1. Login to GCP Portal.\n2. Go to Network services (Left Panel).\n3. Select Load balancing.\n4. Click on the 'Advanced menu' hyperlink to view target proxies.\n5. Click on the 'Target proxies' tab.\n6. Click on the reported HTTPS target proxy.\n7. Click on the hyperlink under 'URL map'.\n8. Click on the 'EDIT' button.\n9. Select 'Frontend configuration', click on HTTPS protocol rule.\n10. For 'SSL policy', choose any custom SSL policy other than 'GCP default'.\n11. Click on 'Done'.\n12. Click on 'Update'.","multiregional":true,"service":"Cloud Load Balancing"},"ecc-gcp-298":{"article":"This policy identifies GCP storage buckets that are not configured with default Event-Based Hold. An event-based hold resets the object's time in the bucket for the purposes of the retention period. This behavior is useful when you want an object to persist in your bucket for a certain length of time after a certain event occurs. It is recommended to enable this feature to protect individual objects from deletion.\nReference: https://cloud.google.com/storage/docs/object-holds.","impact":"Without the default event-based hold configuration, you can not ensure that information will remain stored and unchangeable until it fulfills the retention period set in your retention policy when you release the hold.","report_fields":["selfLink"],"remediation":"From Console:\n1. Open the Cloud Storage browser in Google Cloud Console.\n2. In the list of buckets, click on the name of the bucket that you want to check the default event-based status for.\n3. Select the Protection tab near the top of the page.\n4. Click on the  'DISABLED' button beside 'Default event-based hold option' to configure it to enabled.\n5. Click Save.","multiregional":true,"service":"Cloud Storage"},"ecc-gcp-112":{"article":"This policy identifies GCP Firewall rules that allow inbound traffic to the HTTP port (80) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Allowing unrestricted inbound access on TCP port 80 (HTTP) via VPC network firewall rules can increase opportunities for malicious activities such as hacking, brute-force attacks, DDoS attacks.","report_fields":["selfLink"],"remediation":"To restrict all traffic, edit the reported Firewall rule as follows:\n1. Login to GCP Console.\n2. Go to VPC Network.\n3. Go to Firewall rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify Source IP ranges to specific IP.\n7. Click on Save.","multiregional":true,"service":"Virtual Private Cloud"},"ecc-gcp-317":{"article":"This rule detects when Bigtable instance cluster encryption not using CMEK.\nIt is recommended to use Customer-Managed Keys to fully control and manage data encryption.\nFeatures:\n- CMEK provides the same level of security as Google's default encryption but provides more administrative control.\n- Administrators can rotate, manage access to, and disable or destroy the key used to protect data at rest in Bigtable.\n- All actions on your CMEK keys are logged and viewable in Cloud Logging. Cloud EKM keys support Key Access Justification, which adds a justification field to all key requests. With select external key management partners, you can automatically approve or deny these requests, based on the justification.\n- Bigtable CMEK-protected instances offer comparable performance to Bigtable instances that use Google default encryption.\n- You can use the same CMEK key in multiple projects, instances, or clusters, or you can use separate keys, depending on your business needs.\n- You can use the same CMEK key in multiple projects, instances, or clusters, or you can use separate keys, depending on your business needs.","impact":"Not using CMEK results in less control over aspects of the lifecycle and management of your keys.","report_fields":["name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nFrom Command Line:\nCreate a Bigtable service agent: (https://cloud.google.com/bigtable/docs/use-cmek#gcloud)\n1. Run the gcloud services identity create command to view the service agent that Bigtable uses to access the CMEK key on your behalf. This command creates the service account if it does not already exist, then displays it.\ngcloud beta services identity create --service=bigtableadmin.googleapis.com --project PROJECT\nCreate a key:\n1.In the Google Cloud project where you want to manage your keys:\na) Enable the Cloud KMS API.\nb) Create a key ring and a key using one of the following options:\n- Create the key ring and key directly in Cloud KMS.\n- Use an externally-managed key. Create the external key and then create an Cloud EKM key to make the key available through Cloud KMS.\nThe Cloud KMS key location must be the same as the Bigtable cluster that it will be used with. For example, if you create a key ring and key in us-central1 (Iowa), then clusters in us-central1-a, us-central1-b, and us-central1-c can be protected by keys from that key ring.\nConfigure IAM settings for the key:\n1. Grant the cloudkms.cryptoKeyEncrypterDecrypter role to your service agent:\ngcloud kms keys add-iam-policy-binding KMS_KEY --keyring KMS_KEYRING --location KMS_LOCATION --member serviceAccount:SERVICE_ACCOUNT_EMAIL --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project PROJECT\nCreate a CMEK-enabled instance:\n1. Use the bigtable instances create command to create an instance:\ngcloud bigtable instances create INSTANCE_ID --display-name=DISPLAY_NAME [--cluster-storage-type=CLUSTER_STORAGE_TYPE] [--cluster-config=id=CLUSTER_ID,zone=CLUSTER_ZONE, nodes=CLUSTER_NUM_NODES] [--cluster-config=id=CLUSTER_ID,zone=CLUSTER_ZONE, autoscaling-min-nodes=AUTOSCALING_MIN_NODES, autoscaling-max-nodes=AUTOSCALING_MAX_NODES, autoscaling-cpu-target=AUTOSCALING_CPU_TARGET autoscaling-storage-target=AUTOSCALING_STORAGE_TARGET] kms-key=KMS_KEY","multiregional":true,"service":"Cloud Bigtable"},"ecc-gcp-242":{"article":"This rule detects when a function is publicly accessible in the Cloud Functions service.","impact":"Granting permissions to 'allUsers' and 'allAuthenticatedUsers' members can allow anyone to access your Cloud Function.","report_fields":["name"],"remediation":"From Console:\n1. Go to the Cloud Functions page at Google Cloud Console https://console.cloud.google.com/functions.\n2. Select a function.\n3. Click on Permission.\n4. Review each role and find Members with allUsers and allAuthenticatedUsers access.\n5. Click the Delete icon and confirm by clicking on REMOVE.","multiregional":true,"service":"Cloud Functions"},"ecc-gcp-034":{"article":"The default Compute Engine service account has the Editor role on the project, which allows read and write access to most Google Cloud Services. To support the principle of least privileges and prevent potential privilege escalation.\nIt is recommended that instances are not assigned to default service account Compute Engine default service account with Scope Allow full access to all Cloud APIs.","impact":"Using the default service account on your instances can increase malicious activity to the point that an attacker gains full control of the project.","report_fields":["selfLink"],"remediation":"From Console:\n1.Go to the VM instances page using https://console.cloud.google.com/compute/instances.\n2. Click on the impacted VM instance.\n3. If the instance is not stopped, click the Stop button. Wait for the instance to be stopped.\n4. Next, click on the Edit button.\n5. Scroll down to the Service Account section.\n6. Select a different service account or ensure that Allow full access to all Cloud APIs is not selected.\n7. Click on the Save button to save your changes and then click on START.\nFrom CLI:\n1. Stop the instance:\ngcloud compute instances stop INSTANCE_NAME\n2. Update the instance:\ngcloud compute instances set-service-account INSTANCE_NAME --serviceaccount=SERVICE_ACCOUNT --scopes [SCOPE1, SCOPE2...]\n3. Restart the instance:\ngcloud compute instances start INSTANCE_NAME","multiregional":true,"service":"Compute Engine"},"ecc-gcp-216":{"article":"Compute Engine instance templates cannot forward a packet unless the source IP address of the packet matches the IP address of the instance. Similarly, GCP won't deliver a packet whose destination IP address is different than the IP address of the instance receiving the packet. However, both capabilities are required if you want to use instances to help route packets. Forwarding of data packets should be disabled to prevent data loss or information disclosure.","impact":"Creating an instance based on template with enabled IP forwarding, Google Cloud does not enforce packet source and destination verification, and the security of this redirect is potentially downgraded as this source and destination may refer to the attacker's ones.","report_fields":["selfLink"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue\nWe can't Edit an exisitng Instance Template.\nUsing Console:\n1. In the Google Cloud Console, go to the Instance Template page https://console.cloud.google.com/compute/instanceTemplates/list.\n2. Click on Create Instance Template\n3. Click on NETWORKING,DISKS,SECURITY,MANAGEMENT,SOLE-TENANCY\n4. Under Networking, navigate to IP Forwarding and Disable it.\n5. Click on Create and Delete old template.","multiregional":true,"service":"Compute Engine"},"ecc-gcp-453":{"article":"Redis instance without label information make identification and search difficult.","impact":"Redis instance without label information makes it difficult to identify, search and view usage of GCP resource and spending.","report_fields":["name"],"remediation":"From Console:\n1. Log in to the GCP Console and navigate to Memorystore for Redis at https://console.cloud.google.com/memorystore/redis/instances.\n2. View your instance's Instance details page by clicking on reported Instance ID.\n3. Expand the Info Panel by selecting Show Info Panel.\n3. Select the Labels button and add desired labels.","multiregional":true,"service":"Cloud Memorystore"},"ecc-gcp-211":{"article":"The 'remote access' option controls the execution of stored procedures from local or remote servers on which instances of a SQL Server are running. This default value for this option is '1'. This grants permission to run local stored procedures from remote servers or remote stored procedures from the local server. To prevent local stored procedures from being run from a remote server or remote stored procedures from being run from the local server, this must be disabled. The 'Remote Access' option controls the execution of local stored procedures from remote servers or remote stored procedures from the local server. 'Remote access' functionality can be abused to launch a Denial-of-Service (DoS) attack on remote servers by off-loading query processing to a target, hence this should be disabled. This recommendation is applicable to SQL Server database instances.","impact":"The 'Remote access' functionality can be abused to launch a Denial-of-Service (DoS) attack on remote servers by off-loading query processing to a target.","report_fields":["selfLink"],"remediation":"From Console:\n1. Go to the Cloud SQL Instances page in the Google Cloud Console using https://console.cloud.google.com/sql/instances.\n2. Select the SQL Server instance for which you want to enable the database flag.\n3. Click Edit.\n4. Scroll down to the Flags section.\n5. To set a flag that has not been set on the instance before, click Add item, choose the 'remote access' flag from the drop-down menu, and set its value to 'off'.\n6. Click Save to save your changes.\n7. Confirm your changes under Flags on the Overview page.\nFrom Command Line:\n1. Configure the remote access database flag for every Cloud SQL SQL Server database instance using the below command:\ngcloud sql instances patch <INSTANCE_NAME> --database-flags \"remote access=off\"\nNote : This command will overwrite all database flags previously set. To keep those and add new ones, include the values for all flags you want set on the instance; any flag not specifically included is set to its default value. For flags that do not take a value, specify the flag name followed by an equals sign (\"=\").","multiregional":true,"service":"Cloud SQL"},"ecc-gcp-135":{"article":"Putting resources in different zones in a region provides protection against many types of infrastructure, hardware, and software failures. This policy alerts you if your cluster is not located in at least 3 zones.","impact":"GKE nodes not in redundant zones are not protected against infrastructure, hardware, and software failures.","report_fields":["selfLink"],"remediation":"Add zones to your zonal cluster.\n1. Visit the Google Kubernetes Engine menu in GCP Console.\n2. Click on the cluster's Edit button, which looks like a pencil.\n3. From the Additional zones section, select the desired zones.\n4. Click on Save.","multiregional":true,"service":"Google Kubernetes Engine"},"ecc-aws-235":{"article":"The debug_print_plan setting enables printing the execution plan for each executed query. These messages are emitted at the LOG message level. Unless directed otherwise by your  organization's logging policy, it is recommended this setting be disabled by setting it to off.","impact":"Enabling any of the DEBUG printing variables may cause the logging of sensitive information  that would otherwise be omitted based on the configuration of the other logging settings.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"debug_print_plan\".\n6. Choose 0 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-511":{"article":"Check whether Amazon Classic Load Balancers are not internet facing. The rule is NON_COMPLIANT if the Scheme field is 'internet-facing' in the configuration.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Sign in to the Amazon EC2 console and access the Amazon EC2 at https://console.aws.amazon.com/ec2/v2.\n2. Under the 'Load Balancing' click on the 'Load Balancers'.\n3. Create Classic load balancer.\n4. Under the 'Scheme' choose 'Internal'.\n5. Create load balancer.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-190":{"article":"It is not secure to have task definitions with host network mode and container definitions where privileged=false or is empty and user=root or is empty. \nIf a task definition has elevated privileges, it is because the customer has specifically opted in to that configuration. This policy checks for unexpected privilege escalation when a task definition has host networking enabled but the customer has not opted in to elevated privileges.","impact":"Without this policy, you fail to check for unexpected privilege escalation when a task definition has host networking enabled, but the customer has not opted into elevated privileges.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. From the navigation bar, choose the Region that contains your task definition.\n3. In the navigation bar, choose task definitions. \n4. On the task definitions page, select the box to the left of the task definition to revise and choose Create new revision. \n5. On the Create new revision of task definition page, make changes. Change the existing container definitions, select the container, make the changes (in the User field enter required user name except root and tick Privileged box), and then choose Update. \n6. Verify the information and choose Create. \n7. If your task definition is used in a service, update your service with the updated task definition.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-091":{"article":"You can use Compliance, a capability of AWS Systems Manager, to scan your fleet of managed nodes for patch compliance and configuration inconsistencies.","impact":"If patch compliance was not finished correctly, then the vulnerabilities are still not fixed.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Open the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager/.\n2. In the navigation pane, under Instances & Nodes, choose Run Command.\n3. Choose the radio button next to AWS-RunPatchBaseline and then change the Operation to Install.\n4. Choose instances manually and then choose the non-compliant instance(s).\n5. Scroll to the bottom and then choose Run.\n6. After the command has been completed, choose Compliance in the Navigation pane to monitor the new compliance status of your patched instances.\nFor more information about using Systems Manager documents to patch a managed instance, see in the AWS Systems Manager User Guide:\n  - About SSM documents for patching instances: https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html\n  - Running commands using Systems Manager Run command: https://docs.aws.amazon.com/systems-manager/latest/userguide/run-command.html","multiregional":false,"service":"Amazon EC2"},"ecc-aws-295":{"article":"The greater enhancement in encryption of TLS 1.2 allows it to use more secure hash algorithms such as SHA-256 as well as advanced cipher suites that support elliptical curve cryptography.","impact":"Not using latest version of TLS you missing security features that can lead to unauthorized access.","report_fields":["ARN"],"remediation":"Perform the following for each distribution that failed the rule:\n1. Navigate to the the AWS console CloudFront dashboard at https://console.aws.amazon.com/cloudfront.\n2. Select your distribution ID.\n3. Select the 'Origins' tab.\n4. Choose origin and click on Edit.\n5. Under the Minimum origin SSL protocol choose 'TLSv1.2'.\n6. Click on Save changes.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-286":{"article":"Any WorkSpace instances available within your AWS account that are not being utilized must be identified and removed. An AWS WorkSpaces instance is considered unused if has 0 (zero) known user connections registered within the past 30 days.","impact":"It is highly recommended to remove unused WorkSpaces instances available in your AWS account in order to optimize and reduce monthly costs.","report_fields":["WorkspaceId"],"remediation":"Perform the following to remove unused WorkSpaces based on the output collected from the audit procedure:\n1. Log in to the WorkSpaces dashboard at https://console.aws.amazon.com/workspaces/.\n2. In the left panel, click \"WorkSpaces\".\n3. Select the WorkSpace ID that you have identified as not being used.\n4. Click \"Actions\", \"Remove WorkSpaces\".\n5. Confirm using Audit, confirm that this is the WorkSpaces ID you should remove.\n6. Click \"Remove WorkSpaces\".","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-529":{"article":"This rule ensures that Amazon Elastic Block Store volumes that are attached to Amazon Elastic Compute Cloud (Amazon EC2) instances are marked for deletion when an instance is terminated.","impact":"If an Amazon EBS volume isn't deleted when the instance that it's attached to is terminated, it may violate the concept of least functionality.","report_fields":["InstanceId","OwnerId"],"remediation":"From the Console:\n1. At this time the delete on termination setting for existing instances can only be changed using AWS CLI.\n\nFrom the CLI:\n1. Run the modify-instance-attribute command using the list of instances collected in the audit.\n  aws ec2 modify-instance-attribute --instance-id i-123456abcdefghi0 --block-device-mappings \"[{\\\"DeviceName\\\": \\\"/dev/sda\\\",\\\"Ebs\\\":{\\\"DeleteOnTermination\\\":true}}]\"\n2. Repeat steps no. 1 with the other instances discovered in all AWS regions.\n\n**Note - If you get any errors running the modify-instance-attribute command confirm the instance id and the Device Name for that instance is correct. The above command is referencing the typical default device name.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-493":{"article":"Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console.","impact":"Without Container Insights it is harder to collect, aggregate and summarize metrics and logs from containerized applications and microservices.","report_fields":["clusterArn"],"remediation":"To enable Container Insights on all new clusters by default:\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. In the navigation pane, choose Account Settings.\n3. Select the check box at the bottom of the page to enable the Container Insights default.\n\nTo create a cluster with Container Insights enabled:\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. In the navigation pane, choose Clusters.\n3. Choose Create cluster.\n4. On the next page, do the following:\n4.1. Name your cluster.\n4.2. If you don't have a VPC already, select the check box to create one. You can use the default values for the VPC.\n4.3. Fill out all other needed information, including instance type.\n4.4. Select Enabled Container Insights.\n4.5. Choose Create.\n\nTo enable Container Insights on existing ECS cluster:\naws ecs update-cluster-settings --cluster myCICluster --settings name=containerInsights,value=enabled","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-483":{"article":"From a security perspective, logging is an important feature to enable for future forensics efforts in the case of any security incidents. Encryption of data at rest is a recommended best practice to add a layer of access management around your data. Encrypting the logs at rest reduces the risk that a user not authenticated by AWS will access the data stored on disk. It adds another set of access controls to limit the ability of unauthorized users to access the data.","impact":"Disabled encryption of logs allows a user to get unauthorized access to stored data.","report_fields":["arn"],"remediation":"1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Expand 'Build', choose 'Build project', and then choose the build project without encrypted S3 logs. \n3. Click 'Edit' button and select 'Logs'. \n4. For 'S3 Logs' uncheck the 'Disable S3 log encryption' box.\n5. Click 'Update logs'.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-383":{"article":"This policy identifies the Nat Gateway that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["NatGatewayId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Click on the 'NAT gateways'.\n3. Click on the required Nat Gateway.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-118":{"article":"Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance's role, you can associate an IAM role with an ECS task definition or RunTask API operation. The first will allow you to add all the privileges required by any task in the cluster to a single IAM role, potentially letting tasks use privileges that were not required.","impact":"Using an EC2 instance role can cause all the privileges required for any task in the cluster to be added to a single IAM role, potentially allowing the tasks to use privileges that were not required.","report_fields":["serviceArn"],"remediation":"For each finding, perform the following: \n1. Log in to your AWS Management Console at https://console.aws.amazon.com/vpc/home. \n2. Under Services, click on ECS.\n3. For each cluster, perform the following: \n4. Select the cluster from the list.\n5. Under the Services Tab, click on any Task Definition. \n6. On the Task Definition page, click on Create new revision button.\n7. On the Create new revision of Task Definition page, select a task role from the drop-down list. \n8. Click on the Create button at the bottom of the page. \nTo create an IAM role and assign it to ECS Cluster, perform the following: \n1. Open the IAM console at https://console.aws.amazon.com/iam/.\n2. In the Navigation pane, choose Roles, then choose Create New Role. \n3. In the Select Role Type section, for the Amazon Elastic Container Service Task Role service role, choose Select. \n3.1 To view the trust relationship for this role, check Amazon ECS Task Role. \n4. In the Attach Policy section, select the policy to use for your tasks (for example, AmazonECSTaskS3BucketPolicy) and then choose Next Step.\n5. For Role Name, type a name for your role, such as AmazonECSTaskS3BucketRole, and then choose Create Role to finish.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-435":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon MQ uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["BrokerArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Sign in to the AWS Admin Console and access the Amazon MQ at https://console.aws.amazon.com/amazon-mq.\n2. Click on the 'Create brokers'.\n3. Under the 'Configure settings' click on the 'Additional settings'.\n4. Click on the 'Customer managed CMKs' and choose existing key or create a new one.\n4. Save changes.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-144":{"article":"S3 object-level API operations such as GetObject, DeleteObject, and PutObject are called data events. By default, CloudTrail trails do not log data events and so it is recommended to enable Object-level logging for S3 buckets.","impact":"With disabled object-level logging for READ access, you are missing the opportunity to perform comprehensive security analysis, monitor specific patterns of user behavior in AWS account, or take immediate actions on any object-level API activity within S3 Buckets using Amazon CloudWatch Events. This can lead to undetected read requests made to S3 buckets with sensitive data.","report_fields":["account_id","account_name"],"remediation":"From Console:\n1. Login to the AWS Management Console and navigate to the S3 dashboard at https://console.aws.amazon.com/s3/.\n2. In the left navigation pane, click on the S3 Bucket Name that you want to examine.\n3. Click on the 'Properties' tab to see in detail the bucket configuration. \n4. Click on the 'Object-level logging' setting, enter the CloudTrail name for the recorded activity. You can choose an existing Cloudtrail or create a new one by navigating to the Cloudtrail console link https://console.aws.amazon.com/cloudtrail/. \n5. Once the Cloudtrail is selected, check the 'Read event' checkbox, so that object-level logging for 'Read' or 'All events' is enabled. \n6. Repeat steps 2 to 5 to enable object-level logging of read events for other S3 buckets.\n\nFrom Command Line: \n1. To enable object-level data events logging for S3 buckets within your AWS account, run 'put-event-selectors' command using the name of the trail that you want to reconfigure as identifier:\naws cloudtrail put-event-selectors --region <region-name> --trail-name <trail-name> --event-selectors '[{ \"ReadWriteType\": \"ReadOnly\", \"IncludeManagementEvents\":true, \"DataResources\": [{ \"Type\": \"AWS::S3::Object\", \"Values\": [\"arn:aws:s3:::<s3-bucket-name>/\"] }] }]'\n2. The command output will be object-level event trail configuration.\n3. If you want to enable it for all buckets at ones then change Values parameter to [\"arn:aws:s3\"] in command given above. \n4. Repeat step 1 for each s3 bucket to update object-level logging of read events. \n5. Change the AWS region by updating the --region command parameter and perform the process for other regions.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-414":{"article":"This policy identifies the Glue Job that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["Name"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Glue at https://console.aws.amazon.com/glue.\n2. Under the 'AWS Glue Studio' click on the 'Jobs'.\n3. Click on the required job.\n4. Open 'Job details' and click on the 'Advanced properties'.\n5. Click on the 'Add new tag'.\n6. Add new tag and save.","multiregional":false,"service":"AWS Glue"},"ecc-aws-156":{"article":"HTTPS (TLS) can be used to help prevent potential attackers from using person-in-the-middle or similar attacks to eavesdrop on or manipulate network traffic. Only encrypted connections over HTTPS (TLS) should be allowed. \nEncrypting data in transit can affect performance. You should test your application with this feature to understand the performance profile and the impact of TLS. TLS 1.2 provides several security enhancements over previous versions of TLS.","impact":"The HTTP protocol is not a secure method of transmitting data because data is not encrypted and can be seen by anyone who may be monitoring Internet traffic, and this leads to a breach of confidentiality. TLS 1.2 provides several security enhancements over previous versions of TLS.","report_fields":["ARN"],"remediation":"To enable TLS encryption, use the UpdateElasticsearchDomainConfig API operation to configure DomainEndpointOptions in order to set TLSSecurityPolicy.\nFor more information, see the Amazon Elasticsearch Service Developer Guide. \nUpdateElasticsearchDomainConfig:\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-configuration-api.html#es-configuration-api-actions-updateelasticsearchdomainconfig  \nDomainEndpointOptions:\nhttps://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/es-configuration-api.html#es-configuration-api-datatypes-domainendpointoptions","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-363":{"article":"Server-side encryption is a feature in Kinesis Video Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) that you specify. Data is encrypted before it is written to the Kinesis Video Streams stream storage layer, and it is decrypted after it is retrieved from storage. Server-side encryption is always enabled on Kinesis video streams. If a user-provided key is not specified when the stream is created, the default key (provided by Kinesis Video Streams) is used.","impact":"Without KMS CMK customer-managed keys, you do not have full and granular control over who can access encrypted Kinesis streams data.","report_fields":["StreamARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nA user-provided AWS KMS master key must be assigned to a Kinesis video stream when it is created. You can't later assign a different key to a stream.\nTo encrypt AWS Kinesis Video Streams with KMS customer master keys:\n1. Open the console at https://console.aws.amazon.com/kinesisvideo/home.\n2. On the \"Video streams\" page, choose \"Create video stream\".\n3. On the \"Create a new video stream\" page, type a name for the stream. Select the \"Custom configuration\" radio button.\n4. Under \"KMS customer master key (CMK)\", select \"Customer managed CMK\" and choose a Customer managed CMK.\n4. Choose \"Create video stream\".","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-229":{"article":"Amazon ECR repository offers encryption of data at rest, a security feature that helps prevent unauthorized access to AWS ECR repository data. Use encryption to better protect your data from loss or theft.\nFor more control over the encryption for your Amazon ECR repositories, you can use server-side encryption with KMS keys stored in AWS Key Management Service (AWS KMS). When you use AWS KMS to encrypt your data, you can either use the default AWS managed key, which is managed by Amazon ECR, or specify your own KMS key (referred to as a customer managed key).","impact":"With disabled KMS encryption, there is a possibility of data loss or theft and a possibility of unauthorized access to AWS ECR repository data. Additionally, without KMS CMK, you do not have full and granular control over access to ECR repository.","report_fields":["repositoryArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nThe KMS encryption settings cannot be changed or disabled after the repository is created. To create an Amazon ECR repository with KMS encryption:\n1. Open the Amazon ECR console at https://console.aws.amazon.com/ecr/repositories\n2. Click Create repository button.\n3. Under Encryption settings, enable KMS encryption and then choose either 'a' or 'b':\n  a) Don\u2019t check the box 'Customize encryption settings'. This way Amazon ECR uses an AWS-managed CMK with the alias aws/ecr by default. \n  b) Check the box 'Customize encryption settings' and choose an AWS KMS CMK key. Note that the key should be stored in the same region as the ECR, for this to work properly.\n4. Click the Create repository button.","multiregional":false,"service":"Amazon Elastic Container Registry"},"ecc-aws-296":{"article":"New engine versions have important performance and security improvements.","impact":"Without keeping the RDS MySQL up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["DBInstanceArn"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the console, choose Databases, and then choose the database.\n3. Choose Modify.\n4. For DB engine version, choose the MySQL 8.0 version to upgrade to, and then choose Continue.\n5. For Scheduling of modifications, choose Apply immediately.\n6. Choose Modify DB instance to start the upgrade.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-313":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon DMS uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["ReplicationInstanceArn","ReplicationInstanceIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create new DMS replication instance with kms cmk:\n1. Sign in to the AWS Management Console and open the DMS console at https://console.aws.amazon.com/dms/v2\n2. In the navigation pane, click on the 'Replication instances'.\n3. Click on the 'Create replication instance'.\n4. Under the 'Advanced security and network configuration'.\n5. Choose any existed KMS key (not 'Default') or create new one.\n6. Create instance.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-378":{"article":"This policy identifies the EBS volumes that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["VolumeId"],"remediation":"1. Sign in to the AWS Management Console and open the EC2 console at https://console.aws.amazon.com/ec2/v2/.\n2. In the navigation pane under 'Elastic Block Store' choose 'Volumes'.\n3. Choose required volume.\n4. Open 'Tags' and click on 'Manage tags'.\n5. Add a tags.\n6. Save.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-302":{"article":"The PostgreSQL planner/optimizer is responsible for parsing and verifying the syntax of each query received by the server. If the syntax is correct, a parse tree is built up. Otherwise, an error is generated. The 'log_parser_stats' flag controls the inclusion of parser performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_parser_stats' flag enables a crude profiling method for logging parser performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_parser_stats\".\n6. Choose 0.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-342":{"article":"Ensure that Route53 automatic domain renewal is enabled to avoid domain expiration. With this option enabled Amazon Route53 will automatically renew registration for a domain shortly before the expiration date.","impact":"If you do not renew your domain name, it will become expired. An expired Amazon Route53 domain can cause website or application downtime or failure. An expired domain could be taken over by a malicious individual or deleted by the domain registrar and it will become available for others to register.\nWhen automatic renewal is turned off, be aware of the following effects on your domain:\n  - Some TLD registries delete domains even before the expiration date if you don't renew early enough. It is strongly recommended that you leave automatic renewal enabled if you want to keep a domain name.\n  - It is also strongly recommended that you don't plan to re-register a domain after it has expired. Some registrars allow others to register domains immediately after the domains expire, so you might not be able to re-register before the domain is taken by someone else.\n  - Some registries charge a large premium to restore expired domains.\n  - On or near the expiration date, the domain becomes unavailable on the internet.","report_fields":["DomainName"],"remediation":"To enable automatic renewal for a domain:\n1. Sign in to the AWS Management Console and open the Route53 console at https://console.aws.amazon.com/route53/.\n2. In the navigation pane, choose 'Registered Domains'.\n3. Choose the name of the domain that you want to update.\n4. Choose 'Enable' for 'Auto renew'.\n5. If you encounter issues while enabling automatic renewal, you can contact AWS Support for free. \n   For more information, see 'Contacting AWS Support about domain registration issues': https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-contact-support.html.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-187":{"article":"To improve the security posture of your VPC, you can configure Amazon EC2 to use an interface VPC endpoint. Interface endpoints are powered by AWS PrivateLink, a technology that enables you to access Amazon EC2 API operations privately. It restricts all network traffic between your VPC and Amazon EC2 to the Amazon network. \nBecause endpoints are supported within the same Region only, you cannot create an endpoint between a VPC and a service in a different Region. This prevents unintended Amazon EC2 API calls to other Regions.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, and loss of data.","report_fields":["VpcId","OwnerId"],"remediation":"1. Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n2. In the navigation pane, choose Endpoints. \n3. Choose Create Endpoint. \n4. For Service category, choose AWS services. \n5. For Service Name, choose com.amazonaws.<region>.ec2.\n6. For Type, choose Interface. \n7. Complete the following information.    \n7.1. For VPC, select a VPC in which to create the endpoint.    \n7.2. For Subnets, select the subnets (Availability Zones) in which to create the endpoint network interfaces. Not all Availability Zones are supported for all AWS services.    \n7.3. To enable private DNS for the interface endpoint, select the check box for Enable DNS Name. This option is enabled by default. To use the private DNS option, the following attributes of your VPC must be set to true: enableDnsHostnames; enableDnsSupport.    \n7.4. For Security group, select the security groups to associate with the endpoint network interfaces.    \n7.5. (Optional) Add or remove a tag. To add a tag, choose Add tag and do the following: For Key, enter the tag name. For Value, enter the tag value.    \n7.6. To remove a tag, choose the delete button (x) to the right of the tag Key and Value. \n8. You can attach a policy to your VPC endpoint to control access to the Amazon EC2 API.  \n9. Choose Create endpoint.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-245":{"article":"Enabling the 'log_duration' setting causes the duration of each completed statement to be logged. This does not logs the text of the query and thus, behaves different from the 'log_min_duration_statement' flag. This parameter cannot be changed after the session start.","impact":"When this parameter is disabled it would be harder to find and investigate slow running queries.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_duration\".\n6. Choose 1.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-101":{"article":"A VPC subnet is a part of the VPC, with its own rules for traffic. Subnets with automatic Public IP assignment can inadvertently expose the instances within this subnet to the internet. It is recommended to disable this feature for subnets.","impact":"A public IP assignment can unintentionally expose the instances within this subnet to the internet, which can cause a loss of availability integrity and confidentiality.","report_fields":["SubnetId","VpcId","OwnerId"],"remediation":"1. Log in to the AWS console. \n2. On the console, select a specific region.\n3. Navigate to the 'VPC' service. \n4. In the navigation pane, click on 'Subnets'.\n5. Select the identified Subnet and Select the 'Modify auto-assign IP settings' option under Subnet Actions. \n6. Disable the 'Auto-Assign IP' option and save it.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-189":{"article":"An elastic network interface is a logical networking component in a VPC that represents a virtual network card. Multiple ENIs can cause dual-homed instances, meaning instances that have multiple subnets. This can add network security complexity and introduce unintended network paths and access.","impact":"Multiple ENIs can cause dual-homed instances, meaning instances that have multiple subnets. This can add network security complexity and introduce unintended network paths and access.","report_fields":["InstanceId","OwnerId"],"remediation":"To detach a network interface \n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. Under Network & Security, choose Network Interfaces. \n3. Filter the list by the noncompliant instance IDs to see the associated ENIs. \n4. Select the ENIs that you want to remove. \n5. From the Actions menu, choose Detach. \n6 If you see the prompt Are you sure that you want to detach the following network interface?, choose Detach.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-465":{"article":"Amazon FSx can take an automatic daily backup of your file system. These automatic daily backups occur during the daily backup window that was established when you created the file system. \nAutomatic daily backups are kept for a certain period of time, known as a retention period.\nIt is highly recommended to use automatic daily backups set at least to 7 days for file systems that have any level of critical functionality associated with them.","impact":"A retention period of fewer than 7 days set for FSx file systems can result in data loss and the inability to recover it in the event of failure.","report_fields":["ResourceARN"],"remediation":"To update your Amazon FSx systems configuration in order to set up a sufficient backup retention period, perform the following actions: \n1. Open the Amazon FSx for Lustre console at https://console.aws.amazon.com/fsx/.\n2. From the console dashboard, choose the name of the file system that reconfigure.\n3. In the navigation pane, choose Backups.\n4. In the Settings tab, click Update.\n5. Select 'Yes' to enable automatic backups.\n6. For 'Automatic Backup Retention' set number of days to retain a backup at least to 7 days.\n7. Click update.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-572":{"article":"Each AWS KMS key that you create in AWS KMS costs $1/month, regardless of whether it is enabled or disabled.\nUnless there is a business need to retain disabled KMS CMKs, you should remove them to avoid incurring unexpected charges and to maintain an accurate inventory of system components.\nWarning: Deleting an AWS KMS key is destructive and potentially dangerous. It deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable. You should delete a KMS key only when you are sure that you don't need to use it anymore.","impact":"Keeping unused KMS keys can result in escalating costs and cluttered AWS accounts.","report_fields":["KeyArn","AWSAccountId"],"remediation":"To enable customer managed key:\n  1. Sign in to the AWS Management Console and open the AWS Key Management Service (AWS KMS) console at https://console.aws.amazon.com/kms.\n  2. To change the AWS Region, use the Region selector in the upper-right corner of the page.\n  3. In the navigation pane, choose 'Customer managed keys'.\n  4. Choose the check box for the KMS keys that you want to enable.\n  5. To enable a KMS key, choose 'Key actions', 'Enable'.\n\nTo schedule key deletion:\n  Warning: Deleting an AWS KMS key is destructive and potentially dangerous. It deletes the key material and all metadata associated with the KMS key and is irreversible. After a KMS key is deleted, you can no longer decrypt the data that was encrypted under that KMS key, which means that data becomes unrecoverable. You should delete a KMS key only when you are sure that you don't need to use it anymore.\n  1. Sign in to the AWS Management Console and open the AWS Key Management Service (AWS KMS) console at https://console.aws.amazon.com/kms.\n  2. To change the AWS Region, use the Region selector in the upper-right corner of the page.\n  3. In the navigation pane, choose 'Customer managed keys'.\n  4. Choose the check box for the KMS keys that you want to schedule for deletion.\n  5. Choose 'Schedule key deletion'.\n  6. Set a waiting period of 7 - 30 days.\n  7. Confirm that you want to schedule these keys for deletion.\n  8. Click 'Schedule deletion'.","multiregional":false,"service":"AWS Key Management Service"},"ecc-aws-506":{"article":"When creating a Redshift cluster, you should change the default database name 'dev' to a unique value. Default names are public knowledge and should be changed upon configuration.","impact":"Well-known database name could lead to inadvertent access if it was used in IAM policy conditions.","report_fields":["ClusterIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. Click on the 'Create cluster'.\n3. Under 'Additional configurations' click on the 'Database configurations'.\n4. Change 'Database name' from default.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-214":{"article":"TLS can be used to help prevent potential attackers from using person-in-the-middle or similar attacks to eavesdrop on or manipulate network traffic. Only encrypted connections over TLS should be allowed. Encrypting data in transit can affect performance. You should test your application with this feature to understand the performance profile and the impact of TLS.","impact":"Without enabled encryption in transit, Redshift cluster databases accept a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["ClusterIdentifier"],"remediation":"1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. In the navigation menu, choose Config, then choose Workload management to display the Workload management page. \n3. Choose the parameter group that you want to modify.\n4. Choose Parameters.\n5. Choose Edit parameters then set require_ssl to 1. \n6. Enter your changes and then choose Save.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-153":{"article":"Audit logs are highly customizable. They allow you to track user activity on your Elasticsearch clusters, including authentication successes and failures, requests to Elasticsearch, index changes, and incoming search queries.","impact":"If security critical information is not recorded, there will be no trail for forensic analysis, and discovering the cause of problems or the source of attacks may become more difficult or impossible.","report_fields":["ARN"],"remediation":"From Console:\n1. Login to the AWS Management Console and open the ES domain using https://console.aws.amazon.com/esv3\n2. Choose the domain.\n3. Verify whether advanced security options are configured by going to the 'Security configuration' tab. \n4. If 'Fine-grained access control' disabled:\n  a) Select 'Edit'.\n  b) Check 'Enable fine-grained access control' and configure master user.\n  c) Make other necessary configurations and select 'Save changes'.\n5. Choose the Logs tab.\n6. Under 'CloudWatch Logs', choose 'Audit logs' and click 'Enable'.\n4. Create a CloudWatch log group, or choose the existing one. \n5. Choose an access policy that contains the appropriate permissions, or create a policy using a JSON file the console provides:\n  {  \n    \"Version\": \"2012-10-17\",  \n    \"Statement\":  [  \n    {  \n      \"Effect\": \"Allow\", \n      \"Principal\": {  \n        \"Service\": \"es.amazonaws.com\" \n      },  \n      \"Action\": [ \n        \"logs:PutLogEvents\", \n        \"logs:CreateLogStream\" \n      ], \"Resource\": \"cw_log_group_arn\"\n    } \n    ] \n  }\n6. Choose Enable\n\nFrom CLI:\naws es update-elasticsearch-domain-config --domain-name my-domain --log-publishing-options \"AUDIT_LOGS={{CloudWatchLogsLogGroupArn=arnawslogsus-east-1123456789012log-groupmy-log-group,Enabled=true}}\"","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-083":{"article":"Ensure that all your AWS CloudFront web distributions are integrated with the Web Application Firewall (AWS WAF) service to protect against application-layer attacks.","impact":"Not enabling AWS Cloudfront-WAF integration can lead to application-layer attacks that can compromise the security of your web applications or place unnecessary load on them.","report_fields":["ARN"],"remediation":"1. Navigate to the Cloudfront dashboard at https://console.aws.amazon.com/cloudfront/.\n2. On the Distributions page, select the relevant CDN.\n3. Click on the Distribution Settings button from the dashboard top menu. \n4. On the General tab, click on the Edit button.\n5. On the Distribution Settings page, verify the AWS WAF Web ACL configuration status. If AWS WAF Web ACL is set to None, AWS WAF is not associated with an Access Control List (ACL).","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-122":{"article":"AWS DynamoDb should be encrypted using AWS-managed Customer Master Key (CMK), instead of AWS-owned CMK. This is necessary in order to meet encryption regulatory requirements of Server-Side encryption for the sensitive data that may be stored in DynamoDB. \nIn addition, encrypting DynamoDb with AWS-managed CMK allows you to view the CMK and its key policy as well as audit the encryption/decryption events by examining the DynamoDB API calls using CloudTrail.","impact":"Without AWS-managed CMKs, you cannot view the CMK key and its policy, grant access to it, and audit the encryption and decryption of DynamoDB data.","report_fields":["TableArn"],"remediation":"1. Sign in to the AWS console. \n2. On the console, select the specific region. \n3. Navigate to the 'DynamoDB' dashboard. \n4. Select the reported table from the list of DynamoDB tables. \n5. In the 'Overview' tab, navigate to the 'Table Details' section. \n6. Click on the 'Manage Encryption' link available for 'Encryption Type'.\n7. In the 'Manage Encryption' pop-up window, select 'KMS' as the encryption type.","multiregional":false,"service":"Amazon DynamoDB"},"ecc-aws-265":{"article":"Using the latest generation of Elasticache nodes will improve performance at a lower cost. \nDue to the limited amount of previous generation node types, AWS cannot guarantee a successful replacement when a node becomes unhealthy in your cluster(s). In such a scenario, your cluster availability may be negatively impacted.","impact":"Using previous generation of Elasticache nodes you get less performance for more cost. \nDue to the limited amount of previous generation node types, AWS cannot guarantee a successful replacement when a node becomes unhealthy in your cluster(s). In such a scenario, your cluster availability may be negatively impacted.","report_fields":["ARN"],"remediation":"To change generation of nodes for Elasticache Redis cluster:\n1. Sign in to the Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the navigation pane, choose 'Redis clusters'.\n3. From the list of clusters, choose the cluster you want to update.\n4. Choose 'Actions' and then choose 'Modify'.\n5. Choose the new node type from the node type list.\n6. If you want to perform the update right away, choose 'Apply immediately'. If Apply immediately is not chosen, the update process is performed during the cluster's next maintenance window.\n7. Choose 'Modify'.\n\nTo change generation of nodes for Elasticache Memcached cluster, it must be redeployed to mitigate the issue.\nTo create an ElastiCache for Memcached cluster:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' from the navigation pane.\n4. Choose 'Create Memcached cluster'.\n5. Complete the 'Cluster settings' section. For 'Node type' select current generation node type.\n6. Click 'Next'.\n7. Complete the 'Advanced settings' section. \n8. Click 'Next'.\n9. Review all your entries and choices, then go back and make any needed corrections. When you're ready, choose 'Create' to launch your cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-147":{"article":"Elastic Compute Cloud (EC2) supports encryption at rest when using the Elastic Block Store (EBS) service.\nFor an added layer of security of your sensitive data in EBS volumes, you should enable EBS encryption at rest. Amazon EBS encryption offers a straightforward encryption solution for your EBS resources that doesn't require you to build, maintain, and secure your own key management infrastructure. It uses KMS keys when creating encrypted volumes and snapshots.\nEncryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.","impact":"Disabled encryption allows unauthorized or anonymous users to gain access to EBS containing sensitive data. With Elastic Block Store encryption enabled, the data stored on the volume, disk I/O, and snapshots created from the volume are all encrypted.","report_fields":["VolumeId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nThere is no direct way to encrypt an existing unencrypted volume or snapshot. You can only encrypt a new volume or snapshot when you create it.\n\nTo encrypt existing EBS Volume:\n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/.\n2. Under 'Elastic Block Store', click on 'Volumes'.\n3. Click on required volume.\n4. Click on the 'Actions'.\n5. Click on the 'Create snapshot'.\n6. Click 'Create snapshot'.\n7. Under 'Elastic Block Store', click on 'Snapshots'.\n8. Click on the required snapshot.\n9. Click on the 'Actions', and 'Create volume from snapshot'.\n10. Under the encryption click on the 'Encrypt this volume'.\n11. Choose key or create a new one.\n12. Click on the 'Create volume'.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-129":{"article":"Application and Network Load Balancers should have access logs enabled to capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.","impact":"Disabled access logs for Application and Network Load Balancers make it harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["LoadBalancerArn"],"remediation":"1. Sign in to the AWS console.\n2. From the console, select the specific region.\n3. Navigate to the EC2 dashboard.\n4. Click on 'Load Balancers' (Left Panel).\n5. Select the reported ELB.\n6. Click on the 'Actions' drop-down list.\n7. Click on 'Edit attributes'.\n8. In the 'Edit load balancer attributes' pop-up box, select 'Enable' for 'Access logs' and configure S3 location where you want to store ELB logs.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-401":{"article":"This policy identifies the DLM lifecycle policy that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["PolicyId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EC2 at https://console.aws.amazon.com/ec2/v2.\n2. Under the 'Elastic Block Store' click on the 'Lifecycle Manager'.\n3. Click on the required lifelycle policy.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Data Lifecycle Manager"},"ecc-aws-062":{"article":"Security groups provide stateful filtering of ingress and egress network traffic to AWS resources. \nIt is recommended that no security group allows unrestricted ingress access to remote server administration ports, such as SSH to port 22 and RDP to port 3389.","impact":"Exposing port 22 (SSH) to public access can increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromising.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home\n2. In the left pane, click on 'Security Groups'.\n3. For each security group, perform the following:\n4. Select the security group.\n5. Click on the 'Inbound Rules' tab.\n6. Click on the 'Edit inbound rules' button.\n7. Identify the rules to be edited or removed.\n8. Either:\n  A) Update the Source field to a range other than 0.0.0.0/0 or ::/0 \n  B) Click 'Delete' to remove the offending inbound rule.\n9. Click on 'Save rules'.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-250":{"article":"You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. \nIn general, a larger capacity gives a better performance, but also costs more.","impact":"If you do not enable cache, then the number of calls and the delay to endpoint will be greater than with the enabled cache.","report_fields":["stageName","restApiId"],"remediation":"1. Go to the API Gateway console.\n2. Choose the API.\n3. Choose Stages.\n4. In the Stages list for the API, choose the stage.\n5. Choose the Settings tab.\n6. Choose Enable API cache.\n  6.1. Choose required Cache capacity.\n7. Wait for the cache creation to complete.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-188":{"article":"A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. Network access control lists should always have to be attached.","impact":"Not cleaning out your network access control lists pose the risk that a forgotten network access control list will be used to accidentally open an attack surface.","report_fields":["NetworkAclId","OwnerId"],"remediation":"To delete a network ACL:\n1. Open the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n2. In the navigation pane, choose Network ACLs.\n3. Select the network ACL, and then choose Delete.\n4. In the confirmation dialog box, choose Yes, Delete.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-004":{"article":"At the Amazon S3 bucket level, you can configure permissions through a bucket policy making the objects accessible only through HTTPS.","impact":"The HTTP protocol is not a secure method of transmitting data. Any person monitoring the Internet traffic can see unencrypted data, which leads to a breach of confidentiality.","report_fields":["Name"],"remediation":"1. Login to the AWS Management Console and open the Amazon S3 console using  https://console.aws.amazon.com/s3/ .\n2. Select the Check box next to Bucket. \n3. Click on 'Permissions'. \n4. Click on 'Bucket Policy'.\n5. Filling in the required information, add the following to the existing policy: \n{\n  \"Sid\": \"<optional>\",  \n  \"Effect\": \"Deny\",  \n  \"Principal\": \"*\",  \n  \"Action\": \"s3:GetObject\",  \n  \"Resource\": \"arn:aws:s3:::<bucket_name>/*\",  \n  \"Condition\": {\n    \"Bool\": {\n      \"aws:SecureTransport\": \"false\"  \n    }  \n  }  \n} \n6. Save. \n7. Repeat for all the buckets in your AWS account that contain sensitive data.","multiregional":true,"service":"Amazon S3"},"ecc-aws-332":{"article":"WorkSpaces access should be restricted to trusted operating systems and clients. WorkSpaces access is supported from a variety of clients and operating systems, including HTML5 based browsers. Disabling Web Access prevents access to the Workspace from HTML5 based browsers, ensuring access can only occur from known operating systems.","impact":"Enabled Web Access allows access to the Workspace from HTML5 based browsers, it increases the attack surface and the opportunity for malicious activity.","report_fields":["DirectoryId"],"remediation":"Perform the following steps to disable Web Access:\n1. Log in to the WorkSpaces console at https://console.aws.amazon.com/workspaces/.\n2. In the left pane, click \"Directories\".\n3. Select the directory and then click \"Actions\", \"Update Details\".\n4. Expand \"Access Control Options\".\n5. Uncheck \"Web Access\".\n6. Click \"Update and Exit\".","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-040":{"article":"The Kubernetes project is rapidly evolving, introducing new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (1.XX), as generally available approximately every three months. Each minor version is supported for approximately nine months after it is first released. As new Kubernetes versions become available for Amazon EKS, we recommend that you proactively update your clusters to use the latest available version.","impact":"Without keeping the Kubernetes container-orchestration system up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["arn"],"remediation":"1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n2. Choose the name of the Amazon EKS cluster to update and choose 'Update cluster version'. \n3. For Kubernetes version, select the version to update your cluster to and choose 'Update'. \n4. For 'Cluster name', type the name of your cluster and choose 'Confirm'. The update takes several minutes to complete.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-347":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon MWAA uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).","impact":"Not encrypting data at rest can lead to unauthorized access to sensitive data. Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nPerform the following steps to encrypt MSK during creation:\n1. Login to the MSK console at https://console.aws.amazon.com/msk/.\n2. Click on the Clusters.\n3. Choose Create cluster.\n4. Choose 'Custom create' and 'Provisioned'.\n5. Inside 'Security' step under 'Encrypt data at rest' choose 'Use customer managed key'.\n6. Choose existing key or create new.\n7. Create cluster.","multiregional":false,"service":"Amazon Managed Streaming for Apache Kafka"},"ecc-aws-269":{"article":"A default Virtual Private Cloud is designed in such a way that you can quickly deploy AWS resources and not have to think about the underlying network. The default VPC comes with a default configuration that would not meet all security best practices, hence a non-default VPC should not be used for sophisticated AWS cloud applications.","impact":"Using default VPCs increases the chance that ElastiCache clusters can accidentally open an attack surface.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nChancing VPC on an existing Redis cluster:\n1. Create a manual backup of the cluster:\n  1.1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  1.2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n  1.3. Choose 'Redis clusters' from the navigation pane.\n  1.4. Choose the cluster and choose 'Action', and then 'Backup'.\n  1.5. Make sure the cluster name is right and enter backup name. \n  1.6. Select 'Encryption key'. \n  1.7. Choose 'Create Backup'.\n2. Create a new cluster by restoring from the backup:\n  2.1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2.2. Choose 'Redis clusters' from the navigation pane.\n  2.3. Select 'Create Redis cluster'.\n  2.4. For 'Choose a cluster creation method', choose 'Restore from backups'.\n  2.5. For 'Source' select 'Amazon ElastiCache backups'. And select backup created on the previous step.\n  2.6. Under 'Connectivity' choose subnet group with non-default VPC or choose 'Create new'.\n  2.7. Complete other settings and click 'Next'.\n  2.8. Complete 'Advanced settings' tab and click 'Next'.\n  2.9. Review settings and click 'Create'.\n3. Update the endpoints in your application to the new cluster's endpoints.\n4. Delete the old cluster.\n\nTo create an ElastiCache for Memcached cluster with non-default VPC:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' from the navigation pane.\n4. Choose 'Create Memcached cluster'.\n5. Under 'Connectivity' choose subnet group with non-default VPC or choose 'Create new'.\n6. Complete the 'Cluster settings' section. \n7. Click 'Next'.\n8. Complete 'Advanced settings' tab and click 'Next'.\n9. Complete the 'Advanced settings' section. \n10. Click 'Next'.\n11. Review all your entries and choices, then go back and make any needed corrections. When you're ready, choose 'Create' to launch your cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-280":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon OpenSearch uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nIf encryption of data at rest is not enabled at all, perform the following actions:\n1. Open the domain in the AWS console, then choose Actions and Edit security configuration.\n2. Under Encryption, select Enable encryption of data at rest.\n3. Choose an AWS KMS key to use, then choose Save changes.\n\nIf encryption of data at rest is already enabled with AWS-managed key, you have to create a new domain with KMS CMS:\n1. Sign in to the AWS Management Console and open the Amazon OpenSearch console at https://console.aws.amazon.com/esv3\n2. In the navigation pane, under Domains, click on the 'Create domain'.\n3. Under 'Choose an AWS KMS key', click on the 'Choose A different key'.\n4. Choose existing key or create a new one.\n6. Create domain.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-534":{"article":"A launch template is similar to a launch configuration, in that it specifies instance configuration information. However, defining a launch template instead of a launch configuration allows you to have multiple versions of a launch template. With versioning of launch templates, you can create a subset of the full set of parameters. Then, you can reuse it to create other versions of the same launch template. It is recommended to use launch templates to ensure that you're accessing the latest features and improvements. Not all Amazon EC2 Auto Scaling features are available when you use launch configurations. With launch templates, you can also use newer features of Amazon EC2.","impact":"Not using a launch template to create an Auto Scaling group, you missing out on access to the latest features and improvements.","report_fields":["AutoScalingGroupARN"],"remediation":"To copy a launch configuration to a launch template (console):\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, under 'Auto Scaling', choose 'Launch Configurations'.\n3. Select the launch configuration you want to copy and choose 'Copy to launch template', 'Copy selected'. This sets up a new launch template with the same name and options as the launch configuration that you selected.\n4. For 'New launch template name', you can use the name of the launch configuration (the default) or enter a new name. Launch template names must be unique.\n5. (Optional) To create an Auto Scaling group using the new launch template, select 'Create an Auto Scaling group using the new template'.\n6. Choose 'Copy'.\n\nTo replace the launch configuration for an Auto Scaling group (console):\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/, and choose 'Auto Scaling Groups' from the navigation pane.\n2. Select the Auto Scaling group that you want to modify.\n3. On the 'Details' tab, choose 'Launch configuration', 'Edit'.\n4. Choose 'Switch to launch template'.\n5. For 'Launch template', select your launch template.\n6. For 'Version', select the launch template version, as needed. After you create versions of a launch template, you can choose whether the Auto Scaling group uses the default or the latest version of the launch template when scaling out.\n7. When you have finished, choose 'Update'.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-531":{"article":"Use Amazon EBS encryption as a straight-forward encryption solution for your EBS resources associated with your EC2 instances. With Amazon EBS encryption, you aren't required to build, maintain, and secure your own key management infrastructure. Amazon EBS encryption uses AWS KMS keys when creating encrypted volumes and snapshots.","impact":"Not using encryption by default can lead to launch failures that would occur if unencrypted volumes were inadvertently referenced when an instance is launched in case of using IAM policies that restricts use of unencrypted volumes. Also disabled encryption allows unauthorized or anonymous users to gain access to EBS containing sensitive data.","report_fields":["account_id","account_name"],"remediation":"From Console \n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ \n2. Under Account attributes, click on EBS encryption. \n3. Click on Manage. \n4. Click on the Enable checkbox.\n5. Click on Update EBS encryption \n6. Repeat for every region that requires the change. \nFrom Command Line:\n1. Run 'aws --region <region> ec2 enable-ebs-encryption-by-default'\n2. Verify that \"EbsEncryptionByDefault\" true is displayed. \n3. Repeat for every region that requires the change.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-471":{"article":"To help you automating the handling of interruptions, EC2 Auto Scaling has a feature called Capacity Rebalaning that, if enabled, whenever a Spot instance in your Auto Scaling group is at an elevated risk of interruption, it will proactively attempt to launch a replacement Spot Instance.","impact":"When an Auto Scaling group (ASG) is not configured with rebalancing enabled and if instance is at an elevated risk of interruption, this instance will not be proactively augmented with a new Spot instance before instance is interrupted by Amazon EC2.","report_fields":["AutoScalingGroupARN"],"remediation":"To enable Capacity Rebalancing for an existing Auto Scaling group\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/, and choose Auto Scaling Groups from the navigation pane.\n2. Select the check box next to your Auto Scaling group. A split pane opens in the bottom of the Auto Scaling groups page.\n3. On the Details tab, choose 'Allocation strategies', 'Edit'.\n4. Under the 'Allocation strategies' section, to enable Capacity Rebalancing, select the 'Capacity rebalance' check box.\n5. Choose 'Update'.\n\nEnable Capacity Rebalancing (AWS CLI)\nUse the 'update-auto-scaling-group' command with the following parameter:\n--capacity-rebalance / --no-capacity-rebalance \u2014 Boolean value that indicates whether Capacity Rebalancing is enabled.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-498":{"article":"Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It is recommended to configure at least two availability zones to ensure availability of services, as the Elastic Load Balancer will be able to direct traffic to another availability zone if one becomes unavailable.","impact":"Load Balancer that does not span multiple Availability Zones is unable to redirect traffic to targets in another Availability Zone if the sole configured Availability Zone becomes unavailable.","report_fields":["LoadBalancerArn"],"remediation":"To add Availability Zones for Application Load Balancer:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, under LOAD BALANCING, choose Load Balancers.\n3. Select the load balancer.\n4. On the Description tab, under Basic Configuration, choose Edit subnets.\n5. To enable a zone, select the check box for that zone and select one subnet. If there is only one subnet for that zone, it is selected. If there is more than one subnet for that zone, select one of the subnets.\n6. To change the subnet for an enabled Availability Zone, choose Change subnet and select one of the other subnets.\n7. To remove an Availability Zone, clear the check box for that Availability Zone.\n8. Choose Save.\n\nTo add Availability Zones for Network Load Balancer:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, under LOAD BALANCING, choose Load Balancers.\n3. Select the load balancer.\n4. On the Description tab, under Basic Configuration, choose Edit subnets.\n5. To enable an Availability Zone, select the check box for that Availability Zone. If there is one subnet for that Availability Zone, it is selected. If there is more than one subnet for that Availability Zone, select one of the subnets. Note that you can select only one subnet per Availability Zone.\n6. For an internet-facing load balancer, you can select an Elastic IP address for each Availability Zone. For an internal load balancer, you can assign a private IP address from the IPv4 range of each subnet instead of letting Elastic Load Balancing assign one.\n7. Choose Save.\n\nNote: Affected resource must be redeployed to mitigate the issue.\nTo add Availability Zones for Gateway Load Balancer:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, under LOAD BALANCING, choose Load Balancers.\n3. Select \"Create Load Balancer\".\n4. Choose \"Gateway Load Balancer\".\n5. Under the \"Network mapping\" choose at least two subnets.\n6. Create load balancer.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-447":{"article":"Amazon MWAA can send Apache Airflow logs to Amazon CloudWatch. You can view logs for multiple environments from a single location to easily identify Apache Airflow task delays or workflow errors without the need for additional third-party tools.","impact":"If 'Webserver logs' is not enabled and the 'log_level' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Click \"Next\".\n5. Enable 'Airflow Webserver logs'.\n6. Choose required log level.\n7. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-395":{"article":"This policy identifies the VPC endpoint that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["AutoScalingGroupARN"],"remediation":"1. Sign in to the AWS Management Console and open the EC2 console at https://console.aws.amazon.com/ec2/v2/.\n2. In the navigation pane under 'Auto Scaling' choose 'Auto Scaling Groups'.\n3. Choose required Auto Scaling group.\n4. At the bottom on the 'Tags' tab click on 'Edit'.\n5. Click 'Add Tag'.\n6. Add a tags.\n7. Click 'Update'.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-548":{"article":"By selecting gp3 volumes over gp2 ones, you can elevate your application's performance, lessen storage expenditures, and fine-tune your infrastructure for heightened efficiency.","impact":"Not using gp3 over gp2 will lead to higher costs and worse performance.","report_fields":["VolumeId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create gp3 volume and migrate data:\n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/.\n2. Under Elastic Block Store, click on 'Volumes'.\n3. Click on required volume.\n4. Click on the 'Actions'.\n5. Click on the 'Create snapshot'.\n6. Create snapshot.\n7. Under Elastic Block Store, click on 'Snapshots'.\n8. Click on the required snapshot.\n9. Click on the 'Actions', and 'Create volume from snapshot'.\n10. Under the 'volume type' choose 'gp3'.\n11. Click on the Create volume.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-277":{"article":"Publishing slow logs to Amazon CloudWatch Logs enables you to publish Elasticsearch slow logs from your indexing and search operations performed on ES clusters and gain insights into the performance of those operations.\nYou can enable slow logs to identify whether a performance issue on your cluster is being caused by particular queries or is due to changes in usage. You can then use that information to work with your users to optimize their queries or index configuration to address the problem.","impact":"Disabled Elasticsearch slow logs, making it harder to troubleshoot performance and stability issues.","report_fields":["ARN"],"remediation":"1. Open the Amazon OpenSearch Service console at https://console.aws.amazon.com/esv3/.\n2. Under Domains, select the domain you want to update.\n3. Choose the Logs tab.\n4. Under 'CloudWatch Logs', select 'Search slow logs' if it is disabled and click Enable. \n5. In 'Setup index slow logs' page select an existing log group or create a new one. \n6. Specify CloudWatch an access policy.\n7. Choose Enable.\n8. Repeat 5 - 7 steps to enable 'Index slow logs'.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-204":{"article":"Amazon MySQL database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB instance that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Audit, Error, General, SlowQuery log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify.\n\nTo create a custom DB parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo enable and publish MySQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-539":{"article":"When you use CloudFront with an Amazon S3 bucket as the origin, you can configure CloudFront and Amazon S3 in a way that provides the following benefits:\n - Restricts access to the Amazon S3 bucket so that it's not publicly accessible\n - Makes sure that viewers (users) can access the content in the bucket only through the specified CloudFront distribution\u2014that is, prevents them from accessing the content directly from the bucket, or through an unintended CloudFront distribution\nTo do this, configure CloudFront to send authenticated requests to Amazon S3, and configure Amazon S3 to only allow access to authenticated requests from CloudFront. CloudFront provides two ways to send authenticated requests to an Amazon S3 origin: origin access control (OAC) and origin access identity (OAI). AWS recommends using OAC because it supports:\n - All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022\n - Amazon S3 server-side encryption with AWS KMS (SSE-KMS)\n - Dynamic requests (PUT and DELETE) to Amazon S3\nOAI doesn't work for the scenarios in the preceding list, or it requires extra workarounds in those scenarios.","impact":"When origin access control is disabled for AWS Cloudfront distributions, users bypass any permissions applied to the S3 content and have direct access to objects through Amazon S3 URLs.\nWhen AWS Cloudfront distributions use legacy OAI instead of OAC, you are missing out on supporting the following features:\n  - All Amazon S3 buckets in all AWS Regions, including opt-in Regions launched after December 2022\n  - Amazon S3 server-side encryption with AWS KMS (SSE-KMS)\n  - Dynamic requests (PUT and DELETE) to Amazon S3","report_fields":["ARN"],"remediation":"To migrate from origin access identity (OAI) to origin access control (OAC):\nUpdate the S3 bucket origin to allow both the OAI and OAC to access the bucket's content. This makes sure that CloudFront never loses access to the bucket during the transition. To allow both OAI and OAC to access an S3 bucket, update the bucket policy to include two statements, one for each kind of principal.\n\nThe following example S3 bucket policy allows both an OAI and an OAC to access an S3 origin. In the following example:\n  - Replace DOC-EXAMPLE-BUCKET with the name of the S3 bucket origin\n  - Replace 111122223333 with the AWS account ID that contains the CloudFront distribution and the S3 bucket origin\n  - Replace EDFDVBD6EXAMPLE with the ID of the CloudFront distribution\n  - Replace EH1HDMB1FH2TC with the ID of the origin access identity\nExample S3 bucket policy that allows read-only access to an OAI and an OAC:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AllowCloudFrontServicePrincipalReadOnly\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n              \"Service\": \"cloudfront.amazonaws.com\"\n          },\n          \"Action\": \"s3:GetObject\",\n          \"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"AWS:SourceArn\": \"arn:aws:cloudfront::111122223333:distribution/EDFDVBD6EXAMPLE\"\n              }\n          }\n      },\n      {\n          \"Sid\": \"AllowLegacyOAIReadOnly\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n              \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity EH1HDMB1FH2TC\"\n          },\n          \"Action\": \"s3:GetObject\",\n          \"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\n      }\n  ]\n}\nAfter you update the S3 origin's bucket policy to allow access to both OAI and OAC, you can update the distribution configuration to use OAC instead of OAI. \n\nTo create an origin access control:\n1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n2. In the navigation pane, choose 'Origin access'.\n3. Choose 'Create control setting'.\n4. On the 'Create control setting' form, do the following:\n  4.1 In the 'Details' pane, enter a 'Name' and (optionally) a 'Description' for the origin access control.\n  4.2 In the 'Settings' pane, we recommend that you leave the default setting ('Sign requests (recommended)').\n5. Choose 'Create'.\n6. After the OAC is created, make note of the Name. You need this in the following procedure.\n\nTo add an origin access control to an S3 origin in a distribution:\n\n1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n2. Choose a distribution with an S3 origin that you want to add the OAC to, then choose the 'Origins' tab.\n3. Select the S3 origin that you want to add the OAC to, then choose 'Edit'.\n4. In the 'Origin access' section, choose 'Origin access control settings (recommended)'.\n5. In the 'Origin access' control dropdown menu, choose the OAC that you want to use.\n6. Choose 'Save changes'.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-501":{"article":"Fine-grained access control offers additional ways of controlling access to your data on Amazon OpenSearch Service. For example, depending on who makes the request, you might want a search to return results from only one index. You might want to hide certain fields in your documents or exclude certain documents altogether.","impact":"Not using Fine-grained access control you missing following features:\n- Role-based access control;\n- Security at the index, document, and field level;\n- OpenSearch Dashboards multi-tenancy;\n- HTTP basic authentication for OpenSearch and OpenSearch Dashboards.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Sign in to the AWS Management Console and open the Amazon OpenSearch console at https://console.aws.amazon.com/esv3\n2. In the navigation pane, under Domains, click on the 'Create domain'.\n3. Under 'Fine-grained access control', make sure 'Enable fine-grained access control' is enabled.\n4. Choose either 'Set IAM ARN as master user' or 'Create master user'.\n4.1. In case of 'Set IAM ARN as master user' specify ARN of the master user.\n4.2. In case of 'Create master user' specify 'Master username' and 'Master password'.\n5. Create domain","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-573":{"article":"You are charged for each \u201cNAT Gateway-hour\" that your gateway is provisioned and available, each partial NAT Gateway-hour consumed is billed as a full hour. Additionally, unused NAT gateways can potentially allow unauthorized access to your VPC resources.\nIdentifying unused NAT gateways will lower the cost of your AWS bill. A NAT gateway is considered unused if the average value of BytesOutToDestination metric is 0 for the last 7 days. Unless there is a business need to retain unused NAT gateways, you should remove them to maintain an accurate inventory of system components.","impact":"Keeping unused NAT gateways can result in escalating costs and cluttered AWS accounts.","report_fields":["NatGatewayId"],"remediation":"To delete a NAT gateway in Amazon VPC, do the following:\n  1. Access the Amazon VPC console https://console.aws.amazon.com/vpc/.\n  2. Select the AWS Region for your Amazon VPC.\n  3. In the navigation pane, choose 'NAT Gateways'.\n  4. Select the option button for the NAT gateway, and then choose 'Actions', 'Delete NAT gateway'.\n  5. When prompted, enter 'delete' and then choose 'Delete'.\n  \n  Note: The NAT gateway entry might remain visible in the Amazon VPC console for an hour after removal.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-186":{"article":"A public IPv4 address is an IP address that is reachable from the internet. If you launch your instance with a public IP address, then your EC2 instance is reachable from the internet. \nA private IPv4 address is an IP address that is not reachable from the internet. You can use private IPv4 addresses for communication between EC2 instances in the same VPC or in your connected private network.","impact":"Instances with a public IP address could potentially be compromised, and an attacker could gain anonymous access to the instance and other resources connected to this instance. This can lead to malicious activity that affects sensitive data.","report_fields":["InstanceId","OwnerId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo remove a public IP address from your EC2 instance, do one of the following: \na) Re-launch the instance with the right network interface configuration.\nb) Disassociate an Elastic IP address from the instance.\n  \nUsing method a) \nTo re-launch instance, perform the following actions: \n  1. Navigate to EC2 dashboard at https://console.aws.amazon.com/ec2/.\n  2. In the navigation panel, under 'Instances', click 'Instances'.\n  3. Select the instance that requires a Public IP removal.\n  4. Click the 'Actions' button from the dashboard top menu, select 'Image and templates' and click 'Create Image'.\n  5. In the 'Create Image' dialog box, type a unique name and description, and then choose 'Create Image'. By default, Amazon EC2 shuts down the instance, takes snapshots of any attached volumes, creates and registers the AMI, and then reboots the instance. Choose 'No reboot', if you don't want your instance to be shut down.\n  Warning! If you choose 'No reboot', Amazon can't guarantee the file system integrity of the created image.\n  6. Click 'Create Image'.\n  7. After the process completes, you can use the new AMI to launch an instance.\n  8. To launch an instance from the AMI choose the 'Launch instances' button from the EC2 dashboard top menu.\n  9. Type a name for the instance. And add the required tags, copied from the source instance.\n  10. On the 'Application and OS Images (Amazon Machine Image)' section, click on the 'My AMIs' tab and select the AMI created at step 6.\n  11. On the 'Instance type', select the same instance type used by the source instance.\n  12. On the 'Key pair' section, select the same key pair as the source instance.\n  13. On the 'Network settings', at the right corner of this section click 'Edit'.\n  14. Select 'VPC' and 'Subnet' that attached to the source instance.\n  15. Select 'Disable' for the 'Auto-assign public IP'.\n  16. For the 'Firewall (security groups)' choose 'Select existing security group' and choose the security group attached to the source instance.\n  17. Make 'Advanced network configuration' if necessary.\n  18. Click 'Launch Instance'.\n  19. Click 'View All Instances' to return to the 'Instances' page. The new instance will have the same data and configuration (except the Public IP address) as the source instance.\n  20. Verify the instance, replace the source EC2 instance where needed.\n  21. Terminate the source instance in order to stop incurring charges for the resource.\n  22. You can delete created AMI at step 6.\n  \nUsing method b) \nIf your EC2 instance is associated with an Elastic IP address, then your EC2 instance is reachable from the internet. You can disassociate an Elastic IP address from an instance or network interface at any time. To disassociate an Elastic IP address, perform the following actions: \n  1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. \n  2. In the navigation pane, choose 'Elastic IPs'. \n  3. Select the Elastic IP address to disassociate. \n  4. From 'Actions', choose 'Disassociate Elastic IP address'. \n  5. Choose 'Disassociate'.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-485":{"article":"This rule checks if the deployment group for EC2/On-Premises Compute Platform is configured with a minimum healthy hosts fleet percentage or host count greater than or equal to the input threshold.\nCodeDeploy tracks the health status of the deployment group's instances during the deployment process and uses the deployment's specified minimum number of healthy instances to determine whether to continue the deployment. This practice prevents the possibility the next deployment will fail, dropping the number of healthy instances below the specified minimum number.","impact":"Not setting minimum number of healthy instances to a required number can cause next deployment to fail, because the number of healthy instances below required to ensure application stability.","report_fields":["applicationName","deploymentGroupId","deploymentGroupName"],"remediation":"1. Navigate to CodeDeploy Applications https://console.aws.amazon.com/codesuite/codedeploy/applications.\n2. Select an application to which you need to update deployment group.\n3. Select 'Deployment groups' tab.\n4. Select deployment group that you want to modify.\n5. Click 'Edit' at top right corner.\n6. In the 'Deployment settings' section, select an existing deployment configuration that meets the requirements or create a new one.\n7. Click 'Save changes'.","multiregional":false,"service":"AWS CodeDeploy"},"ecc-aws-263":{"article":"It is recommended to delete unused virtual private gateways or associate them (use them) in order not to reach the limit of 5 VPGs","impact":"Keeping unused virtual private gateways eventually can prevent creating new virtual private gateways because of the limit.","report_fields":["VpnGatewayId"],"remediation":"1. Open the AWS Direct Connect console at https://console.aws.amazon.com/directconnect/v2/home. \n2. In the navigation pane, choose Virtual private gateways and then select the Virtual private gateway. \n3. Choose Edit. \n4. Choose Actions and then either Delete Virtual Private Gateway or Attach to VPC. \n4.1. Choose the VPC to associate, and then choose \"Yes, Attach\".","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-552":{"article":"Identifying unused Amazon DynamoDB tables will lower the cost of your AWS bill. A table considered to be unused when it is older than 60 days and doesn't have on-demand mode enabled and has either 0 (zero) items in it or there weren't any Read/Write activity during 60 days. \nUnless there is a business need to retain unused tabled, you should remove them to maintain an accurate inventory of system components.","impact":"Keeping unused DynamoDB tables can result in escalating costs and cluttered AWS accounts.","report_fields":["TableArn"],"remediation":"To delete unused DynamoDB table:\n  1. Open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n  2. On the console, select the specific region. \n  3. Select the reported table from the list of DynamoDB tables. \n  3. Click 'Delete' button to delete a table.\n\nTo enable On-demand capacity mode:\n  1. Open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n  2. On the console, select the specific region. \n  3. Choose the table that you want to update.\n  4. Navigate to the 'Additional settings' tab.\n  4. In 'Read/write capacity' section, click 'Edit'.\n  5. For 'Capacity mode' select 'On-demand'.\n  6. Click 'Save changes'.","multiregional":false,"service":"Amazon DynamoDB"},"ecc-aws-032":{"article":"This policy identifies security group rules that allow inbound traffic to the MongoDB port (27017) from public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted MongoDB Database access can increase opportunities for malicious activity such as unauthorized access, denial-of-service (DoS) attacks, Brute Force attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group. \n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-337":{"article":"If you do not specify a Customer managed key when using environment variables, Amazon Lambda uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["FunctionArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Lambda https://console.aws.amazon.com/lambda/ .\n2. In the navigation pane click on the 'Functions'.\n3. Click on the required function.\n4. Click on the 'Configuration' and then 'Environment variables'.\n5. Click 'Edit'.\n6. Under 'Encryption configuration' choose 'Use a customer master key'.\n7. Select existing key or create a new one.\n8. CLick Save","multiregional":false,"service":"AWS Lambda"},"ecc-aws-439":{"article":"Enabling instance deletion protection is an additional layer of protection against accidental QLDB deletion or deletion by an unauthorized entity. \nWhile deletion protection is enabled, an QLDB cluster cannot be deleted. Before a deletion request can succeed, deletion protection must be disabled.","impact":"Accidentally deleted or deleted by an unauthorized entity, the QLDB cluster can result in data loss.","report_fields":["EnvironmentArn"],"remediation":"To enable termination protection:\n1. Open the AWS QLDB console at https://console.aws.amazon.com/qldb.\n2. Click on the required ledger.\n3. Enable termination protection.","multiregional":false,"service":"Amazon QLDB"},"ecc-aws-050":{"article":"Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure passwords are at least of a given length. \nIt is recommended that the password policy require a minimum password length of 14 characters.","impact":"Not using a password policy with minimum password length of 14 characters reduces security and account resiliency against brute-force attacks.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane.\n4. Set 'Minimum password length'to 14 or greater.\n5. Click on 'Apply password policy'.","multiregional":true,"service":"AWS Account"},"ecc-aws-406":{"article":"This policy identifies the Elasticache clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n3. Choose Redis from the navigation pane.\n4. Choose required cluster.\n5. Open 'Tags' and click on 'Manage tags'.\n6. Add a tags.\n7. Save.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-230":{"article":"Amazon ECR image scanning helps in identifying software vulnerabilities in your container images. When Scan on Push security feature is enabled, your container images are automatically scanned after being pushed to your Amazon ECR repository. \nIf Scan on Push is disabled on your repository, then each image scan must be manually started to get scan results.","impact":"When scan on push is disabled then the image with potential vulnerabilities can be missed.","report_fields":["repositoryArn"],"remediation":"1. Open the Amazon ECR console at https://console.aws.amazon.com/ecr/\n2. In the navigation pane, choose repositories.\n3. Choose Repository and then choose Edit\n4. Under Image scan settings, enable Scan on push.\n5. Choose Save.","multiregional":false,"service":"Amazon Elastic Container Registry"},"ecc-aws-171":{"article":"The default host and port settings configure Kibana to run on localhost:5601.\nUnrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:  \n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-267":{"article":"To help keep your data secure, Amazon ElastiCache and Amazon EC2 provide mechanisms to guard against unauthorized access of your data on the server. \nBy providing in-transit encryption capability, ElastiCache gives you a tool you can use to help protect your data when it is moving from one location to another.","impact":"Without enabled encryption in transit, ElastiCache cluster accepts a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["ARN"],"remediation":"Enabling in-transit encryption for an existing Redis cluster using the AWS Management Console:\nStep 1: Set your Transit encryption mode to Preferred\n  1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2. Choose 'Redis clusters' from the ElastiCache 'Resources' listed on the navigation pane, present on the left hand.\n  3. Choose the Redis cluster you want to update.\n  4. Choose the 'Actions' dropdown, then choose 'Modify'.\n  5. Choose 'Enable' under 'Encryption in transit' in the 'Security' section.\n  6. Choose 'Preferred' as the 'Transit encryption mode'.\n  7. Choose 'Preview changes' and save your changes.\n\nAfter you migrate all your Redis clients to use encrypted connections:\nStep 2: Set your Transit encryption mode to Required\n  1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2. Choose 'Redis clusters' from the ElastiCache 'Resources' listed on the navigation pane, present on the left hand.\n  3. Choose the Redis cluster you want to update.\n  4. Choose the 'Actions' dropdown, then choose 'Modify'.\n  5. Choose 'Required' as the 'Transit encryption mode', in the 'Security' section.\n  6. Choose 'Preview changes' and save your changes.\n\nNote: Affected ElastiCache for Memcached cluster must be redeployed to mitigate the issue.\nTo create an ElastiCache for Memcached cluster with encryption in-transit enabled:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' from the navigation pane.\n4. Choose 'Create Memcached cluster'.\n5. Complete the 'Cluster settings' section. \n6. Click 'Next'.\n7. On the 'Advanced settings' section, under 'Security', choose 'Enable' for 'Encryption in-transit'.\n8. Complete the 'Advanced settings' section. \n9. Click 'Next'.\n10. Review all your entries and choices, then go back and make any needed corrections. When you're ready, choose 'Create' to launch your cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-031":{"article":"This policy identifies security group  rules that allow inbound traffic to the Microsoft-DS port (445) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted access to 445 port can increase opportunities for malicious activity such as man-in-the-middle attacks (MITM), Denial of Service (DoS) attacks or the Windows Null Session Exploit.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-169":{"article":"Unrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:\n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-451":{"article":"AWS X-Ray makes it easy for developers to analyze the behavior of their distributed applications by providing request tracing, exception collection, and profiling capabilities.","impact":"Traditional debugging methods don't work so well for microservice based applications, in which there are multiple, independent components running on different services which makes it harder to analyze the behavior of applications.","report_fields":["EnvironmentArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Elastic Beanstalk console using https://console.aws.amazon.com/elasticbeanstalk/\n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list.\n3. In the navigation pane, choose Configuration.\n4. In the Software configuration category, choose Edit.\n5. In the AWS X-Ray section, select X-Ray daemon.\n6. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-208":{"article":"Amazon Aurora-MySQL database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB instance that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Audit, Error, General, SlowQuery log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify.\n\nTo create a custom DB parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo enable and publish Aurora-MySQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-038":{"article":"This policy identifies security group rules that allow inbound traffic to the SMTP port (25) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted SMTP access can increase opportunities for malicious activity such as spamming and Denial-of-Service (DoS) attacks.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-371":{"article":"Attached security groups to the primary elastic network interface (ENI) to manage ports and network communication should not be open to all communication. They should be restricted to what is required by WorkSpaces, the Organization and other services.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["WorkspaceId"],"remediation":"Perform the steps below to remove inbound rules that allows all traffic from all IP addresses:\n1. Login to the WorkSpaces https://console.aws.amazon.com/workspaces/home#listworkspaces.\n2. Note an IP address of the non-compliant WorkSpace.\n3. Navigate to Directories in the left panel and note a VPC Id of the corresponding Directory.\n4. Navigate to Network interfaces https://console.aws.amazon.com/ec2/v2/home#NIC.\n5. Filter network interfaces based on VPC Id and IP address of WorkSpace.\n6. Select the network interface.\n7. View attached security groups and edit or delete security group inbound rules that are overly permissive and have:\n  - Type: All traffic\n  - Protocol: All\n  - Source: '0.0.0.0/0' or '::/0'\n\nNote: Make sure that all the required ports are added to inbound rules so that connectivity to WorkSpaces is not impacted.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-049":{"article":"Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure passwords are comprised of different character sets. \nIt is recommended that the password policy require at least one number.","impact":"Not using a password policy with at least one number reduces security and account resiliency against brute-force attacks.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane. \n4. Check 'Require at least one number' \n5. Click on 'Apply password policy'.","multiregional":true,"service":"AWS Account"},"ecc-aws-530":{"article":"HTTPS (TLS) can be used to help prevent potential attackers from using person-in-the-middle or similar attacks to eavesdrop on or manipulate network traffic. Only encrypted connections over HTTPS (TLS) should be allowed. Encrypting data in transit can affect performance. You should test your application with this feature to understand the performance profile and the impact of TLS.","impact":"Not using HTTPS for your CloudFront CDN distribution cannot guarantee that the traffic between the edge (cache) servers and the application viewers cannot be decrypted by malicious users in case they are able to intercept packets sent across the CDN distribution network.","report_fields":["ARN"],"remediation":"Perform the following for each distribution that failed the rule: \nOn the AWS CloudFront console, check the details for the distribution and ensure that the Viewer Protocol Policy is HTTPS Only. \nConfigure CloudFront to require HTTPS between viewers and CloudFront. \n1. Navigate to the the AWS console CloudFront dashboard.\n2. Select your distribution ID.\n3. Select the 'Behaviors' tab.\n4. Check the behavior you want to modify, then select Edit.\n5. Select 'HTTPS Only' or 'Redirect HTTP to HTTPS' for Viewer Protocol Policy.\n6. Select 'Yes', then select Edit.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-407":{"article":"This policy identifies the Beanstalk that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["EnvironmentArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Beanstalk at https://console.aws.amazon.com//elasticbeanstalk.\n2. Click on the required environment.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-340":{"article":"Amazon MQ is integrated with Amazon CloudWatch Logs, a service that monitors, stores, and accesses your log files from a variety of sources. For example, you can configure CloudWatch alarms to receive notifications of broker reboots or troubleshoot ActiveMQ broker configuration errors.","impact":"With disabled logs for the MQ brokers, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["BrokerArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon MQ at https://console.aws.amazon.com/amazon-mq.\n2. Click on the required broker.\n3. Click 'Edit'.\n4. Under 'Logs' check 'General' and 'Audit' for ActiveMQ.\n4.1. Under 'Logs' check 'CloudWatch logs' for RabbitMQ.\n5. Schedule modification.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-310":{"article":"Amazon Database Migration Service is a managed web service that you can use to migrate data from a source database to a target database. An AWS DMS replication instance initiates the connection between the two data stores, transfers the data and caches any changes that occur on the source data store at the initial data load. The DMS service releases engine version upgrades regularly to introduce new software features, bug fixes, security patches and performance improvements.","impact":"Without keeping the DMS up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["ReplicationInstanceArn","ReplicationInstanceIdentifier"],"remediation":"1. Sign in to the AWS Management Console.\n2. Navigate to the Database Migration Service (DMS) dashboard at https://console.aws.amazon.com/dms/.\n3. In the left navigation panel, choose Replication instances.\n4. Select the AWS DMS replication instance that you want to reconfigure.\n5. Click the Modify button from the dashboard top menu to access the resource configuration panel.\n6. On the Modify Replication Instance page, select latest engine version.\n7. Select Apply changes immediately to apply your changes immediately.\n8. Click Modify to apply the configuration changes.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-257":{"article":"Kerberos uses secret-key cryptography to provide strong authentication so that passwords or other credentials aren't sent over the network in an unencrypted format. \nThe ability to authenticate users and services with Kerberos not only allows you to secure your big data applications, but it also enables you to easily integrate Amazon EMR clusters with an Active Directory environment. \nWhen you use Kerberos authentication, Amazon EMR configures Kerberos for the applications, components, and subsystems that it installs on the cluster so that they are authenticated with each other.","impact":"Without Kerberos authentication, you are missing out on the opportunity to secure your big data applications. Passwords or other credentials are sent over the network in an unencrypted format.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nIn order to enable Kerberos authentication, you need to clone it and add a security configuration.\n\nTo create a security configuration:\n1. Open the AWS EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, under 'EMR on EC2', select 'Security configurations'.\n3. Choose the 'Create' button to create the security configuration and specify the authentication parameters.\n4. Enter the name for the security configurations.\n5. In the 'Authentication' section, check the box 'Enable Kerberos authentication' and select the required configurations.\n6. Then select 'Create'.\n\nTo clone a cluster using the console:\n1. Open the AWS EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, under EMR on EC2, choose Clusters.\n3. Select the cluster you want to clone.\n4. At the top of the Cluster Details page, click Clone.\n5. In the dialog box, choose Yes to include the steps from the original cluster in the cloned cluster. Choose No to clone the original cluster's configuration without including any of the steps.\n6. The Create Cluster page appears with a copy of the original cluster's configuration. Review the configuration, make any necessary changes, and on the Step 4 'Security', select the security configuration that you created earlier.\n7. Then click Create Cluster.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-180":{"article":"CloudFront origin failover can increase availability. Origin failover automatically redirects traffic to a secondary origin if the primary origin is unavailable or if it returns specific HTTP response status codes.","impact":"Without origin failover configured, the availability of origin is at risk. On the other hand, with a configured origin failover, in the event of a primary origin failure, the content is automatically delivered from the secondary origin maintaining high distribution reliability.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n2. Choose the distribution that you want to create the origin group for.\n3. Choose the Origins and Origin Groups tab.\n4. On the Origin and Origin Groups tab, choose Create Origin Group.\n5. Choose the origins for the origin group. After you add origins, use the arrows to set the priority that is, which origin is primary and which is secondary.\n6. Choose the HTTP status codes to use as failover criteria. You can choose any combination of the following status codes: 500, 502, 503, 504, 404, or 403. When CloudFront receives a response with one of the status codes that you specify, it fails over to the secondary origin.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-196":{"article":"Private subnets allow you to limit access to deployed components, and to control security and routing of the system.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nDuring launch, you can control whether your instance in a default or non-default subnet is assigned a public IPv4 address.\nBy default, default subnets have this attribute set to true. Non-default subnets have the IPv4 public addressing attribute set to false, unless it was created by the Amazon EC2 launch instance wizard. In that case, the wizard sets the attribute to true.\nYou need to launch your cluster in a VPC with a private subnet that has the IPv4 public addressing attribute set to false.\nAfter launch, you cannot manually disassociate a public IPv4 address from your instance.\nTo remediate this finding, you need to create a new cluster in VPC private subnet. For information on how to launch a cluster in into a VPC private subnet, see Launch clusters into a VPC in the Amazon EMR Management Guide: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-vpc-launching-job-flows.html.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-251":{"article":"Amazon AppFlow always encrypts your data and sensitive information with AWS managed key during transit and at rest. Encryption for data at rest is currently available for Amazon S3 only. It is recommended to use a customer managed CMK, as it puts you in full control over your encrypted data.","impact":"Without KMS CMK customer-managed keys, you do not have full and granular control over who can access encrypted AppFlow data.","report_fields":["flowArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt Amazon AppFlow flows using customer-managed Customer Master Keys (CMKs), you have to re-create your AppFlow flows with the appropriate encryption type by performing the following operations: \n\nPart 1. Creating symmetric KMS keys\n1. Sign in to the AWS Management Console and open the AWS Key Management Service (AWS KMS) console at https://console.aws.amazon.com/kms.\n2. In the navigation pane, choose Customer managed keys.\n3. Choose Create key.\n4. To create a symmetric KMS key, for Key type choose Symmetric.\n5. Choose Next.\n6. Type an alias for the KMS key.\n7. (Optional) Type a description for the KMS key.\n8. Choose Next.\n9. (Optional) Type a tag key and an optional tag value.\n10. Choose Next.\n11. Select the IAM users and roles that can administer the KMS key.\n12. (Optional) To prevent the selected IAM users and roles from deleting this KMS key, in the Key deletion section at the bottom of the page, clear the Allow key administrators to delete this key check box.\n13. Choose Next.\n14. Select the IAM users and roles that can use the key in cryptographic operations\n15. (Optional) You can allow other AWS accounts to use this KMS key for cryptographic operations. To do so, in the Other AWS accounts section at the bottom of the page, choose Add another AWS account and enter the AWS account identification number of an external account. To add multiple external accounts, repeat this step.\n16. Choose Next.\n17. Review the key settings that you chose. You can still go back and change all settings.\n18. Choose Finish to create the KMS key.\nPart 2. Create a flow\n1. Open the Amazon AppFlow console at https://console.aws.amazon.com/appflow/.\n2. In the navigation panel, choose Flows.\n3. Click on the name of the flow that you want to re-create and note all the required configuration information.\n4. Navigate back to the Flows home page and click on the 'Create flow' button.\n5. On the Create flow page, for Flow details, enter a name and description for the flow.\n6. Choose Data encryption, Customize encryption settings and then select a previously created KMS CMK.\n7. Next, perform the actions, based on the configuration information collected at step 3 from the source flow at Part 1.\n8. When you are on the Step 5 Review and create, review the information for your flow. To change the information for a specific step, choose Edit. When you are finished, choose Create flow.","multiregional":false,"service":"Amazon AppFlow"},"ecc-aws-312":{"article":"The DMS service releases engine version upgrades regularly to introduce new software features, bug fixes, security patches and performance improvements.\nAuto minor version upgrade option allows to have minor engine upgrades applied automatically to the replication instance during the maintenance window or immediately if you choose the Apply changes immediately option.","impact":"With an automatic update disabled, you are missing updates that may contain new software features, bug fixes, security patches and performance improvements.","report_fields":["ReplicationInstanceArn","ReplicationInstanceIdentifier"],"remediation":"To update your Amazon DMS replication instances configuration in order to enable Auto Minor Version Upgrade, perform the following actions: \n1. Sign in to the AWS Management Console.\n2. Navigate to the Database Migration Service dashboard at https://console.aws.amazon.com/dms/.\n3. In the left navigation panel, choose Replication instances.\n4. Select the AWS DMS replication instance that you want to reconfigure.\n5. Click the Modify button from the dashboard top menu to access the resource configuration panel.\n6. On the Modify Replication Instance page, click the Maintenance tab and select the Auto minor version upgrade checkbox to enable the feature.\n7. Select Apply changes immediately to apply the changes right away. With this option any pending modifications will be asynchronously applied as fast as possible, regardless of the maintenance window setting for the selected instance. If Apply changes immediately checkbox is not selected, the changes will be applied automatically during the next scheduled maintenance window.\n8. Click Modify to apply the configuration changes.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-386":{"article":"This policy identifies the Security Group that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["GroupId","OwnerId","VpcId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Click on the 'Security groups'.\n3. Click on the required Security Group.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add tag and save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-492":{"article":"Amazon ECR lifecycle policies provide more control over the lifecycle management of images in a private repository. A lifecycle policy contains one or more rules, where each rule defines an action for Amazon ECR. This provides a way to automate the cleaning up of your container images by expiring images based on age or count.","impact":"Without configuring lifecycle policy, you can unintentionally use outdated images in your repository. \nAdditionally, you pay for the amount of data you store in your public or private repositories, thus keeping unnecessary images will increase your monthly bill.","report_fields":["repositoryArn"],"remediation":"To create a lifecycle policy:\nImportant: It is considered best practice to create a lifecycle policy preview to ensure that the images affected by your lifecycle policy rules are what you intend. For more information, see https://docs.aws.amazon.com/AmazonECR/latest/userguide/lpp_creation.html.\n1. Open the Amazon ECR console at https://console.aws.amazon.com/ecr/.\n2. In the navigation pane, choose 'Repositories'.\n3. Choose repository and then choose 'Actions' and select 'Lifecycle policies'.\n4. On the repository lifecycle policy page, choose Create rule.\n  a) For 'Rule priority', type a number for the rule priority.\n  b) For 'Rule description', type a description for the lifecycle policy rule.\n  c) For 'Image status', choose 'Tagged', 'Untagged', or 'Any'.\n  d) If you specified 'Tagged' for 'Image status', then for 'Tag prefixes', you can optionally specify a list of image tags on which to take action with your lifecycle policy. If you specified 'Untagged', this field must be empty.\n  e) For 'Match criteria', choose values for 'Since image pushed' or 'Image count more than' (if applicable).\n  f) Choose 'Save'.\n5. Choose Save.","multiregional":false,"service":"Amazon Elastic Container Registry"},"ecc-aws-124":{"article":"When you define and use your own KMS CMK customer-managed keys to protect data and metadata in the EFS file systems, you gain full control over who can use these keys to access the data (including metadata). The AWS KMS service allows you to create, rotate, disable, and audit CMK encryption keys for your file systems.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in EFS.","report_fields":["FileSystemArn","OwnerId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon Elastic File System console at https://console.aws.amazon.com/efs/.\n2. Choose 'Create file system' to open the file system creation wizard.\n2a. Configure file system access: choose your VPC, create your mount targets, and then choose 'Next Step'.\n2b. Configure optional settings: add any tags, choose your performance mode, check the box to encrypt your file system, and then choose 'Next Step'.\n2c. Review and create: review your settings and choose 'Create File System'.\n3. Check the 'Enable encryption' checkbox and choose the name of the AWS Key from the 'Select KMS master key' drop-down list to enable encryption using your own KMS CMK key.\nIn order to encrypt the existing Amazon EFS with your own AWS KMS CMK, you need to create a new Amazon EFS and copy all the data from the existing Amazon EFS to the new one with encryption enabled.","multiregional":false,"service":"Amazon Elastic File System"},"ecc-aws-537":{"article":"It is recommended to remove elevated privileges from ECS task definitions. When the privilege parameter is true, the container is given elevated privileges on the host container instance (similar to the root user).","impact":"With privileged parameter set to 'true', the principle of least-privileges will be violated.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. From the navigation bar, choose the Region that contains your task definition.\n3. In the navigation bar, choose 'Task definitions'. \n4. On the task definitions page, select the box to the left of the task definition to revise and choose 'Create new revision with JSON'. \n5. On the 'Create new revision' page. \n6. Change value for field 'privileged' to false or delete this field.\n7. Verify the information and choose 'Create'. \n7. If your task definition is used in a service, update your service with the updated task definition.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-333":{"article":"By default, Amazon FSx encrypts at rest all types of file systems with keys managed using AWS Key Management Service (AWS KMS). Data is automatically encrypted before being written to the file system, and automatically decrypted as it is read. This KMS key can be one of the two following types: AWS-managed KMS key or Customer-managed KMS key. Customer-managed KMS key is the most flexible KMS key to use, because you can configure its key policies and grants for multiple users or services.","impact":"Without AWS KMS Customer Master Keys (CMKs), you do not have full and granular control over who can use the encryption keys to access AWS FSx file system data.","report_fields":["ResourceARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt your Amazon FSx file system data using your own AWS KMS Customer Master Key, you have to re-create the non-compliant FSx file system with the required encryption configuration. Perform the following actions: \n1. Open the Amazon FSx console at https://console.aws.amazon.com/fsx/.\n2. On the dashboard, choose Create file system to start the file system creation wizard.\n3. On the Select file system type page, choose the same type of file system as non-compliant FSx file system that you want to re-create.\n4. Within Encryption section, select the alias of the KMS Customer Master Key (CMK) instead the default one from the Encryption key dropdown list. \n5. Perform all the necessary configurations.\n6. Review the file system configuration. For your reference, note which file system settings you can modify after the file system is created.\n7. Choose Create file system.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-096":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Security Groups are a stateful packet filter that controls ingress and egress traffic within a VPC. It is recommended that a metric filter and alarm be established for detecting changes made to Security Groups.","impact":"Lack of monitoring and logging of security group changes can result in insufficient response time to detect accidental or intentional modifications that may lead to unrestricted or unauthorized access.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = AuthorizeSecurityGroupIngress) || ($.eventName = AuthorizeSecurityGroupEgress) || ($.eventName = RevokeSecurityGroupIngress) || ($.eventName = RevokeSecurityGroupEgress) || ($.eventName = CreateSecurityGroup) || ($.eventName = DeleteSecurityGroup)}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-170":{"article":"Port 5500 is designated as fcp-addr-srvr1 by the IANA. It is used by SecurID. It is also used by Oracle for the Oracle Enterprise Manager Web Console. \nUnrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console: \n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-481":{"article":"By default, Docker containers do not allow access to any devices. Privileged mode grants a build project's Docker container access to all devices. Setting privilegedMode with value true enables running the Docker daemon inside a Docker container. The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.","impact":"If privileged mode is set to true it can lead to unintended access to Docker APIs as well as the container\u2019s underlying hardware as unintended access to privilegedMode may risk malicious tampering or deletion of critical resources.","report_fields":["arn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Choose 'Create build project'.\n3. Under the 'Environment' make sure 'Privileged' is unchecked.\n4. Create build project.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-428":{"article":"This policy identifies the RDS snapshots that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["DBSnapshotArn","DBInstanceIdentifier"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon RDS at https://console.aws.amazon.com/rds.\n2. Click on the snapshots.\n3. Click on the required snapshot.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-036":{"article":"This policy identifies security group rules that allow inbound traffic to the POP3 port (110) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted NetBIOS access can increase opportunities for malicious activity such as man-in-the-middle attacks (MITM), Denial of Service (DoS) attacks, and possibly execute arbitrary code. Also, remote attackers may gain root privileges after POP3 authentication.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-314":{"article":"The 'audit_sys_operations' setting provides auditing of all user activities conducted under the SYSOPER and SYSDBA accounts. The setting should be set to TRUE to enable this auditing.","impact":"If 'audit_sys_operations' is FALSE, there will be no trail for forensic analysis of SYSOPER and SYSDBA accounts, and discovering the cause of problems or the source of attacks may become more difficult or impossible.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"audit_sys_operations\".\n6. Choose TRUE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-017":{"article":"AWS IAM users can access AWS resources using different types of credentials, such as passwords or access keys. It is recommended that all the credentials that have not been used or rotated for 45 or more days be removed or deactivated.\nRoot account is excluded in the audit since the root account should not be used for day to day business and would likely be unused for more than 45 days.","impact":"If credentials are unused or unrotated for more than 45 days, it increases the likelihood of an old, lost, or stolen access key being used on a compromised or terminated account.","report_fields":["Arn"],"remediation":"Perform the following to manage Unused Password (IAM user console access):\n1. Login to the AWS Management Console.\n2. Click on 'Services'.\n3. Click on 'IAM'.\n4. Click on 'Users'.\n5. Click on 'Security Credentials'.\n6. Select user whose 'Console last sign-in' is greater than 45 days.\n7. Click on 'Security Credentials'.\n8. In section 'Sign-in credentials', chose 'Console password' and click on 'Manage'.\n9. Select 'Disable' under Console Access.\n10.Click on 'Apply'.\n\nPerform the following to deactivate Access Keys:\n1. Login to the AWS Management Console.\n2. Click 'Services'.\n3. Click 'IAM'.\n4. Click on 'Users'.\n5. Click on 'Security Credentials'.\n6. Select any access keys that are over 45 days old and that have been used and click on 'Make Inactive'.\n7. Select any access keys that are over 45 days old and that have not been used and click the 'X' to Delete.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-008":{"article":"Ensure that SSL/TLS certificates stored in AWS IAM are renewed one month before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid, and the communication between a client and an AWS resource that implements the certificates is no longer secure.","report_fields":["Arn"],"remediation":"From AWS CLI: \naws iam upload-server-certificate --server-certificate-name ExampleCertificate --certificate-body file://Certificate.pem --certificate-chain file://CertificateChain.pem --private-key file://PrivateKey.pem --tags '{\"Key\": \"ExampleKey\", \"Value\": \"ExampleValue\"}'","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-521":{"article":"Enabling this option reduces security attack vectors since the container instance's filesystem cannot be tampered with or written to unless it has explicit read-write permissions on its filesystem folder and directories.","impact":"If 'readonlyRootFilesystem' is not enabled, attacker can tamper or write to the filesystem which can lead to lose of integrity and data.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. In the left navigation pane, choose Task Definitions.\n3. Select the container definition that needs to be updated.\n4. Choose Edit Container. For Storage and Logging, select Read only root file system.\n5. Choose Update at the bottom of the Edit Container tab.\n6. Choose Create.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-335":{"article":"AWS X-Ray helps to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error.","impact":"If active tracing is not enabled it will be harder to debug and operate your functions as the X-Ray service support allows you to rapidly diagnose errors, identify bottlenecks, slowdowns and timeouts, by breaking down the latency for your Lambda functions.","report_fields":["FunctionArn"],"remediation":"To turn on active tracing:\n  1. Open the AWS Lambda console at https://console.aws.amazon.com/lambda/\n  2. Navigate to Functions and then select your Lambda function.\n  3. Choose 'Configuration' and then choose 'Monitoring and operations' tools.\n  4. Choose Edit.\n  5. Under 'AWS X-Ray', toggle on 'Active tracing'.\n  6. Choose Save.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-014":{"article":"It is recommended to use HTTPS instead of HTTP to encrypt the communication between the application clients and the application load balancer.","impact":"An HTTP protocol is not a secure method of transmitting data. Any person monitoring the Internet traffic can see unencrypted data, which leads to a breach of confidentiality.","report_fields":["LoadBalancerArn"],"remediation":"1. Sign in to the AWS console. \n2. Select the region in which the alert is generated from the region drop-down list.  \n3. Navigate to the EC2 dashboard.\n4. Select 'Load Balancers' (Left Panel). \n5. Select the reported ELB. \n6. Select the 'Listeners' tab. \n7. 'Edit' the 'Listener ID' rule that uses HTTP. \n8. Select 'HTTPS' and other options in 'Protocol:port'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-139":{"article":"Enable IAM Access analyzer for IAM policies about all resources in each region. IAM Access Analyzer is a technology introduced at AWS reinvent 2019. After the Analyzer is enabled in IAM, scan results are displayed on the console showing the accessible resources. \nScans show resources that other accounts and federated users can access, such as KMS keys and IAM roles. So the results allow you to determine if an unintended user is allowed, making it easier for administrators to monitor least privileges access. Access Analyzer analyzes only policies that are applied to resources in the same AWS Region.","impact":"With a disabled AWS IAM Access Analyzer, it will be harder to identify unintended access to AWS resources and data that poses a security risk.","report_fields":["account_id","account_name"],"remediation":"1. Open the IAM console at https://console.aws.amazon.com/iam/. \n2. Choose Access analyzer. \n3. Choose Create analyzer. \n4. On the Create analyzer page, confirm that the Region displayed is the Region where you want to enable Access Analyzer.\n5. Enter a name for the analyzer (Optional as it will generate a name automatically).\n6. Add any tags that you want to apply to the analyzer (Optional). \n7. Choose Create Analyzer. \n8. Repeat these step for each active region.\nFrom Command Line:\n'aws accessanalyzer create-analyzer --analyzer-name <NAME> --type <ACCOUNT|ORGANIZATION>'","multiregional":false,"service":"AWS Account"},"ecc-aws-090":{"article":"RDS snapshots should prohibit public access. Also ensure that access to the snapshot and permission to change Amazon RDS configuration is restricted to authorized principals only.","impact":"Publicly accessible RDS snapshot may result in unintended data exposure of RDS instance.","report_fields":["DBSnapshotArn","DBInstanceIdentifier"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Navigate to Snapshots and then select the public Snapshot you want to modify.\n3. From the Actions list, choose Share Snapshots. \n4. From DB snapshot visibility, choose Private. \n5. Under DB snapshot visibility, select For all. \n6. Choose Save.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-355":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon MWAA uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["ClusterIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nAWS redshift instance Encryption of data at rest can only be enabled at the time of the file system creation. To resolve this issue, create a new instance with encryption enabled, migrate the file data from the reported Redshift instances to this new instance, and then delete the original instances. \n1. Sign in to the AWS Admin Console and Access the Redshift service. \n2. Click on the identified Redshift cluster and take a snapshot of it. \n3. Create a new Redshift cluster (now with 'Encryption' set to 'Use key from current account' during creation time) and use the snapshot to populate (restore) the new cluster. \n4. Once the new cluster is populated, delete the older cluster (without encryption).","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-323":{"article":"The _trace_files_public parameter is used to make trace files used for debugging database applications and events available to all database users.","impact":"Making the file world readable means anyone can read the instance's trace file, which could contain sensitive information about instance operations.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type '_trace_files_public'.\n6. Choose FALSE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-432":{"article":"This policy identifies the SQS that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["QueueArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon SQS at https://console.aws.amazon.com/sqs/v2.\n2. Click on the required queue.\n3. Open 'Tagging' and click on the 'Edit'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Simple Queue Service"},"ecc-aws-490":{"article":"Choose the desired HTTP PUT response hop limit for instance metadata requests. The larger the number, the further instance metadata requests can travel.","impact":"Not setting desired HTTP PUT response hop limit you can't control how far or close metadata can travel.","report_fields":["InstanceId","OwnerId"],"remediation":"To change token hop limit use AWS CLI:\naws ec2 modify-instance-metadata-options --instance-id <value> --http-put-response-hop-limit <value> --http-endpoint enabled","multiregional":false,"service":"Amazon EC2"},"ecc-aws-143":{"article":"S3 object-level API operations such as GetObject, DeleteObject, and PutObject are called data events. By default, CloudTrail trails do not log data events and so it is recommended to enable Object-level logging for write S3 buckets.","impact":"With disabled object-level logging for WRITE access, you are missing the opportunity to perform comprehensive security analysis, monitor specific patterns of user behavior in AWS account, or take immediate actions on any object-level API activity within S3 Buckets using Amazon CloudWatch Events. This can lead to undetected write or delete requests within the S3 bucket with sensitive data.","report_fields":["account_id","account_name"],"remediation":"From Console:\n1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/.\n2. On the CloudTrail service home page, the Trails page, or the Trails section of the Dashboard page, choose 'Create trail'.\n3. On the 'Create Trail' page, for Trail name, enter a name for your trail. Leave all other settings at their default.\n4. For Storage location, you can choose to create an S3 bucket or use an existing one. If you choose to use an existing bucket, browse to select it.\n5. Under 'Log file SSE-KMS encryption', clear the 'Enabled' checkbox.\n6. On Choose log events, clear the Management events checkbox and select Data events. By default, basic event selectors log all the read/write events for all the selected S3 buckets.\n7. Click on 'Next'.\n8. In 'Events' section select 'Data events'.\n9. In 'Data events' section, for 'Data event type' select 'S3'.\n10. For 'Log selector template' select 'Log all events'.\n11. Choose Next.\n12. On the 'Review and create' page, review your selections, and then choose 'Create trail'.\n\nFrom Command Line:\n1. To enable object-level data events logging for S3 buckets within your AWS account, run 'put-event-selectors' command using the name of the trail that you want to reconfigure as identifier:\naws cloudtrail put-event-selectors --region <region-name> --trail-name <trail-name> --event-selectors '[{ \"ReadWriteType\": \"WriteOnly\", \"IncludeManagementEvents\":true, \"DataResources\": [{ \"Type\": \"AWS::S3::Object\", \"Values\": [\"arn:aws:s3\"] }] }]'\n2. The command output will be the object-level event trail configuration.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-437":{"article":"Object Lock is an Amazon S3 feature that blocks object version deletion during a user-defined retention period, to enforce retention policies as an additional layer of data protection and/or for strict regulatory compliance. The feature provides two ways to manage object retention: retention periods and legal holds. A retention period specifies a fixed time frame during which an S3 object remains locked, meaning that it can't be overwritten or deleted. A legal hold implements the same protection as a retention period, but without an expiration date. Instead, a legal hold remains active until you explicitly remove it.","impact":"Not using S3 buckets with Object Lock cannot guarantee data integrity as the files stored within these buckets can be accidentally or intentionally deleted.","report_fields":["Name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Login to the AWS Console.\n2. Find the S3 service.\n3. Create bucket.\n4. Inside \"Advances settings\" enable \"Object Lock\".\n5. Open recently created bucket.\n6. Click on the \"Properties\".\n7. Under the \"Object Lock\" click \"Edit\".\n8. Enable \"Default retention\".\n9. Choose required retention mode.\n10. Set required retention period.","multiregional":true,"service":"Amazon S3"},"ecc-aws-455":{"article":"Enabling instance deletion protection is an additional layer of protection against accidental emr deletion or deletion by an unauthorized entity. \nWhile deletion protection is enabled, an EMR cluster cannot be deleted. Before a deletion request can succeed, deletion protection must be disabled.","impact":"Accidentally deleted or deleted by an unauthorized entity, the EMR cluster can result in data loss.","report_fields":["ClusterArn"],"remediation":"To enable termination protection:\n1. Open the AWS EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. Click on the required cluster.\n3. Under 'Summary' enable 'Termination protection'.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-227":{"article":"Use AWS Key Management Service (KMS) keys to provide envelope encryption of Kubernetes secrets stored in Amazon EKS. Implementing envelope encryption is considered a security best practice for applications that store sensitive data and is part of a defense in depth security strategy.","impact":"Disabled encryption can lead to unauthorized access to sensitive and confidential data and violate compliance requirements for data-at-rest encryption within your organization.","report_fields":["arn"],"remediation":"Enabling secret encryption on an existing cluster:\n1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n2. Choose the cluster to which you want to add KMS encryption.\n3. Click on the Configuration tab.\n4. Scroll down to the Secrets encryption section and click on the Enable button.\n5. Select a key from the dropdown menu and click the Enable button. If no keys are listed, you must create one first.\n6. Click the Confirm button to use the chosen key.\n\nCreating an Amazon EKS cluster with secret encryption:\n1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/.\n2. Choose Clusters.\n3. Choose Add Cluster.\n4. Under Secrets encryption, choose Enable envelope encryption.\n5. Choose existing KMS key or create new.\n6. Finish creating cluster.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-443":{"article":"You can use AWS WAF to protect your Appsync API from common web exploits, such as SQL injection and cross-site scripting (XSS) attacks. These could affect API availability and performance, compromise security, or consume excessive resources. \nFor example, you can create rules to allow or block requests from specified IP address ranges, requests from CIDR blocks, requests that originate from a specific country or region, requests that contain malicious SQL code, or requests that contain malicious script.","impact":"Not enabling Appsync API WAF integration could affect API availability and performance, compromise security, or consume excessive resources.","report_fields":["arn","name"],"remediation":"To associate the web ACL with an AWS AppSync API in the AWS AppSync Console:\n  1. Sign in to the AWS Management Console and open the AppSync Console.\n  2. Choose the API that you want to associate with a web ACL.\n  3. In the navigation pane, choose Settings.\n  4. In the Web application firewall section, turn on Enable AWS WAF.\n  5. In the Web ACL dropdown list, choose the name of the web ACL to associate with your API.\n  Note. If the Web ACL you need doesn't exist yet, choose Create WebACL. You will be redirected to the AWS WAF Console, where you need to create a Web ACL. Then return to the AWS Appsync console to associate the Appsync API with the created Web ACL.\n  6. Choose Save to associate the web ACL with your API.","multiregional":false,"service":"AWS AppSync"},"ecc-aws-356":{"article":"In order to keep data secure in transit 'require_ssl' parameter should be enabled between the clients (applications) and your warehouse clusters.","impact":"Without enforcing HTTPS connection, Redshift accepts a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["ClusterIdentifier"],"remediation":"To change default parameter group:\n1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshiftv2/.\n2. In the navigation pane, choose Provisioned clusters dashboard.\n3. Choose the required Cluster.\n4. Choose Properties.\n5. Under Database configurations, choose Edit, Edit parameter group.\n6. Choose existing parameter or create new parameter group.\n\nTo update parameter in existing non default parameter group:\n1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshiftv2/.\n2. In the navigation pane, choose Provisioned clusters dashboard.\n3. Choose the required Cluster.            \n4. Choose Properties.\n5. Click on Parameter group name.\n6. Click on Parameters.\n7. Click Edit Parameters\n8. Set 'require_ssl' to true.\n9. Choose Save.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-219":{"article":"Secrets Manager helps you improve the security posture of your organization. Secrets include database credentials, passwords, and third-party API keys. You can use Secrets Manager to store secrets centrally, encrypt secrets automatically, control access to secrets, and rotate secrets safely and automatically. \nSecrets Manager can rotate secrets. You can use rotation to replace long-term secrets with short-term ones. Rotating your secrets limits how long an unauthorized user can use a compromised secret. For this reason, you should rotate your secrets frequently. \nIn addition to configuring secrets to rotate automatically, you should ensure that those secrets rotate successfully based on the rotation schedule.","impact":"If secrets have not been successfully rotated, it can lead to unauthorized access to resources.","report_fields":["Name","ARN"],"remediation":"If the automatic rotation fails, then Secrets Manager might have encountered errors with the configuration.\nTo rotate secrets in Secrets Manager, you use a Lambda function that defines how to interact with the database or service that owns the secret.\nFor help on how to diagnose and fix common errors related to secrets rotation, see 'Troubleshooting AWS Secrets Manager rotation of secrets' in the AWS Secrets Manager User Guide:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/troubleshoot_rotation.html","multiregional":false,"service":"AWS Secrets Manager"},"ecc-aws-311":{"article":"Using Amazon KMS Customer Master Keys (CMKs) to protect data in SageMaker notebook instances gives full control over who can use the encryption keys to access SageMaker data. Amazon KMS service allows to easily create, rotate, disable and audit Customer Master Keys created for SageMaker notebook instances.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["NotebookInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\n1. Open the Sagemaker console at https://console.aws.amazon.com/sagemaker/\n2. In the navigation pane, choose Notebook instances.\n3. Choose Create Notebook instance.\n4. Under Permissions and encryption choose Encryption key.\n5. Choose Create Notebook instance.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-053":{"article":"CloudTrail log file validation creates a digitally signed digest file containing a hash of each log that CloudTrail writes to S3. These digest files can be used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. It is recommended that file validation be enabled on all CloudTrails.","impact":"With disabled log file validation, you miss out on additional integrity checks of CloudTrail logs. CloudTrail log file validation is used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log.","report_fields":["TrailARN"],"remediation":"Perform the following to enable log file validation on a given trail:\n1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail.\n2. Click on 'Trails' on the left navigation panel.\n3. Click on target trail.\n4. Within the 'General details' section click 'Edit'.\n5. Under the 'Advanced settings' section.\n6. Check the enable box under 'Log file validation'.\n7. Click 'Save changes'.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-290":{"article":"When you launch a WorkSpace, you can encrypt the root volume (C drive for Windows and / for Amazon Linux) and the user volume (D drive for Windows and /home for Amazon Linux). This ensures that the data stored at rest for WorkSpaces is encrypted.\nYou must encrypt a WorkSpace when it is launched. You cannot create a custom image from an encrypted WorkSpace and you cannot disable encryption once encryption is enabled for a WorkSpace.","impact":"Not encrypting data at rest can lead to unauthorized access to sensitive data.","report_fields":["WorkspaceId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nPerform the following steps to encrypt WorkSpace volumes when creating WorkSpace:\n1. Login to the WorkSpaces console at https://console.aws.amazon.com/workspaces/.\n2. Click Launch WorkSpaces and complete the first three steps.\n3. For the WorkSpaces Configuration step, do the following: \n  - Select the volumes to encrypt: Root Volume, User Volume, or both volumes. \n  - For Encryption Key, select an AWS KMS CMK. The CMK that you select must be symmetric.\n4. Click Next Step.\n5. Click Launch WorkSpaces.\nNOTE: To encrypt existing AWS WorkSpaces data you must re-create the necessary WorkSpaces instances with the volumes encryption feature enabled as outlined above.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-106":{"article":"Checks for ACM Certificates with wildcard domain names instead of single domain names. An ACM allows you to use wildcards (*) in the domain name to protect several sites in the same domain. With this type of certificate, there is a risk that if the private key of a certificate is compromised, then all domains and subdomains using the compromised certificate are potentially compromised. \nIt is recommended to use single domain name certificates instead of wildcard certificates to reduce these associated risks.","impact":"If a certificate private key is compromised, then all sites that use the compromised certificate are potentially affected.","report_fields":["CertificateArn","DomainName"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nTo create a new certificate with a single domain:\n1. Sign in to the AWS console.\n2. On the console, select the specific region. \n3. Navigate to Certificate Manager.\n4. Perform the following steps on the 'Request a certificate' page, \na. Step 1 Choose the 'Add domain names' page, enter the fully qualified domain name in the 'Domain name' box, and then click on 'Next' \nb. Step 2 Choose the 'Select validation method' page, select the validation method and click on 'Review' \nc. Step 3 Choose the 'Review' page, review the domain name and validation method details, and then click on 'Confirm' \nd. Step 4 Choose the 'Validation' page, validate the certificate request based on the selected validation method, then click on 'Continue' The certificate status should change from 'Pending validation' to 'Issued'. Now, access your application's web server configuration and replace the wildcard certificate with the newly issued single domain name certificate.  \nTo delete a wildcard certificate: \n1. Sign into the AWS console. \n2. On the console, select the specific region. \n3. Navigate to the Certificate Manager(ACM) service. \n4. Select the certificate. \n5. Under the 'Actions' drop-down list, click 'Delete'.\n6. In the 'Delete certificate' pop-up window, click on 'Delete'.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-125":{"article":"In order to protect sensitive data, AWS ElastiCache Redis clusters should be encrypted at rest. \nEncryption of data at rest prevents unauthorized access to your sensitive data stored on AWS ElastiCache Redis clusters and associated cache storage.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in ElastiCache.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nAWS ElastiCache Redis cluster at-rest encryption can be set only at the time of the cluster creation. To fix this issue, create a new cluster with at-rest encryption, migrate all the required ElastiCache Redis cluster data from the unencrypted cluster to the new one, and then delete the old cluster.\nTo create a new ElastiCache Redis cluster with at-rest encryption set, perform the following:\n1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n3. Choose 'Redis clusters' from the navigation pane.\n5. Click on the 'Create Redis cluster' button.\n6. Complete 'Cluster settings' page and click 'Next'.\n7. At the 'Security' section, enable 'Encryption at rest'. And select a key that will be used to protect the key used to encrypt data at rest for this cluster.\n8. Click on 'Create' button to launch your new ElastiCache Redis cluster.\n\nEnabling at-rest encryption on an existing ElastiCache Redis cluster:\n1. Create a manual backup of the cluster:\n  1.1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  1.2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n  1.3. Choose 'Redis clusters' from the navigation pane.\n  1.4. Choose the cluster and choose 'Action', and then 'Backup'.\n  1.5. Make sure the cluster name is right and enter backup name. \n  1.6. Select 'Encryption key'. \n  1.7. Choose 'Create Backup'.\n2. Create a new cluster by restoring from the backup:\n  2.1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2.2. Choose 'Redis clusters' from the navigation pane.\n  2.3. Select 'Create Redis cluster'.\n  2.4. For 'Choose a cluster creation method', choose 'Restore from backups'.\n  2.5. For 'Source' select 'Amazon ElastiCache backups'. And select backup created on the previous step.\n  2.6. Complete other settings and click 'Next'.\n  2.7. At the 'Security' section, enable 'Encryption at rest'. And select a key that will be used to protect the key used to encrypt data at rest for this cluster.\n  2.8. Complete other settings and click 'Next'.\n  2.9. Review settings and click 'Create'.\n3. Update the endpoints in your application to the new cluster's endpoints.\n4. Delete the old cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-162":{"article":"RDS event notifications use Amazon SNS to make you aware of changes in the availability or configuration of your RDS resources. These notifications allow for a rapid response.\nA notification for 'security group' event type should be enabled for all security groups and all event categories ('configuration change','failure').","impact":"Not enabling RDS security group event notifications can lead to slow or no response to a security group configuration change.","report_fields":["account_id","account_name"],"remediation":"To subscribe to RDS database security group event notifications:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Event subscriptions.\n3. Under Event subscriptions, choose Create event subscription.\n4. In the Create event subscription dialog, do the following.\n  4.1 For Name, enter a name for the event notification subscription.\n  4.2 For Send notifications to, choose an existing Amazon SNS ARN for an SNS topic.\n      To use a new topic, choose create topic to enter the name of a topic and a list of recipients.\n  4.3 For Source type, choose Security groups.\n  4.4 Under Instances to include, select All security groups.\n  4.5 Under Event categories to include, select Specific event categories. The control also passes if you select All event categories.\n  4.6 Select configuration change and failure.\n  4.7 Choose Create.\n5. Confirm subscription to topic, by clicking or visiting the link in an email that you specified in the topic.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-058":{"article":"AWS provides a support center that can be used for incident notification and response, as well as for technical support and customer services. Create an IAM Role to allow authorized users to manage incidents with AWS Support.","impact":"By default, without AWSSupportAccess role, IAM users cannot access the Support Center. Without this role, they won't be able to access resources and perform actions, such as to open Support Center cases and use the AWS Support API.","report_fields":["account_id","account_name"],"remediation":"1. Create an IAM role for managing incidents with AWS: \n  - Create a trust relationship policy document that allows you to manage AWS incidents and save it locally as /tmp/TrustPolicy.json: \n  { \n      \"Version\": \"2012-10-17\", \n      \"Statement\": [ \n          { \n              \"Effect\": \"Allow\", \n              \"Principal\": { \n                  \"AWS\": \"<iam_user>\" \n              }, \n              \"Action\": \"sts:AssumeRole\" \n          } \n      ] \n  } \n2. Create an IAM role using the above trust policy: \n    aws iam create-role --role-name <aws_support_iam_role> --assume-role-policy-document file:///tmp/TrustPolicy.json \n3. Attach the 'AWSSupportAccess' managed policy to the created IAM role: \n    aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AWSSupportAccess --role-name <aws_support_iam_role>","multiregional":true,"service":"AWS Account"},"ecc-aws-317":{"article":"A remote listener is a listener residing on one computer that redirects connections to a database instance on another computer.","impact":"Permitting a remote listener for connections to the database instance can allow for the potential spoofing of connections and that could compromise data confidentiality and integrity.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type 'remote_listener'.\n6. Delete all values and leave this parameter empty.\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-459":{"article":"Code signing for AWS Lambda helps to ensure that only trusted code runs in your Lambda functions. When you enable code signing for a function, Lambda checks every code deployment and verifies that the code package is signed by a trusted source.","impact":"Without a code signing it is possible to run altered code or code from untrusted publisher.","report_fields":["FunctionArn"],"remediation":"1. Sign in to the AWS Console and access the Lambda at https://console.aws.amazon.com/lambda.\n2. Click on the required lambda function.\n3. Go to 'Configuration' and click on the 'Code signing'.\n4. Click 'Edit'.\n5. Choose existing configuration or create a new one.\n6. Click Save.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-427":{"article":"This policy identifies the RDS DB clusters do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["DBClusterArn"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation panel, choose Databases, then choose the DB cluster that you want to modify.\n3. Choose the \"Tags\" tab.\n4. Add tags and save.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-119":{"article":"Use KMS customer-managed keys (CMK ) to protect Kinesis Streams and metadata. Using KMS CMK, you gain full control over who can use the keys to access AWS Kinesis data (including the system metadata). The AWS KMS service allows you to create, rotate, disable and audit CMK encryption keys.","impact":"Without KMS CMK customer-managed keys, you do not have full and granular control over who can access encrypted Kinesis streams data.","report_fields":["StreamARN"],"remediation":"1. Navigate to the AWS KMS console and select the region in which your Kinesis stream is located.  \n2. In the Navigation pane, choose Customer Managed Keys and then choose Create Key.\n3. Follow the steps to create a key.\n4. Note the ARN for the key.\n5. Navigate to the Kinesis console, select your stream and enter the ARN for the key in the KMS Key ID field.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-325":{"article":"Amazon Database Migration Service (DMS) replication instances are using Multi-AZ deployment configurations to provide High Availability (HA) through automatic failover to standby replicas in the event of a failure such as an Availability Zone (AZ) outage, an internal hardware or network outage, a software failure or in case of a planned maintenance session.","impact":"Disabled multiple Availability Zones for DMS instances threaten the availability of stored data.","report_fields":["ReplicationInstanceArn","ReplicationInstanceIdentifier"],"remediation":"To update your Amazon DMS replication instances configuration in order to enable Multi-AZ deployment, perform the following actions: \n1. Sign in to the AWS Management Console.\n2. Navigate to the Database Migration Service (DMS) dashboard at https://console.aws.amazon.com/dms/.\n3. In the left navigation panel, choose Replication instances.\n4. Select the AWS DMS replication instance that you want to reconfigure.\n5. Click the Modify button from the dashboard top menu to access the resource configuration panel.\n6. On the Modify Replication Instance page, select Yes from the Multi-AZ dropdown menu to enable the feature.\n7. Select Apply changes immediately to apply your changes immediately. With this option any pending modifications will be asynchronously applied, regardless of the maintenance window setting for the selected replication instance. If Apply changes immediately checkbox is not selected, your changes will be applied during the next scheduled maintenance window.\n8. Click Modify to apply the configuration changes.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-514":{"article":"Identify and deactivate any unnecessary IAM access keys as a security best practice. AWS allows you to assign maximum two active access keys.","impact":"Unnecessary AWS IAM access keys generate unnecessary management work in auditing and rotating IAM credentials.","report_fields":["Arn"],"remediation":"Perform the following to delete inactive access keys:\n1. Login to the AWS Management Console.\n2. Click on 'Services'.\n3. Click on 'IAM'.\n4. Click on 'Users'.\n5. Click on required user.\n6. Click on 'Security Credentials'.\n6. Delete inactive access keys.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-266":{"article":"For any Redis cluster, you can enable automatic backups. When automatic backups are enabled, ElastiCache creates a backup of the cluster on a daily basis. There is no impact on the cluster and the change is immediate. Automatic backups can help guard against data loss. \nIn the event of a failure, you can create a new cluster, restoring your data from the most recent backup. The result is a warm-started cluster, preloaded with your data and ready for use.","impact":"Elasticache redis cluster without automatic backup and retention period not set at least to 7 days can result in data loss and the inability to recover it in the event of failure.","report_fields":["ARN"],"remediation":"Enabling automatic backups on an existing cluster:\n1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region with cluster.\n3. Choose 'Redis clusters' from the navigation pane.\n4. Choose existing cluster. \n5. Choose 'Actions', 'Modify'.\n6. Choose 'Enable automatic backups' and set 'Backup retention period' to at least 7 days.\n7. Complete the configurations and click 'Preview changes'. \n8. Review all your entries and choices, then go back and make any needed corrections. \n9. If you want to perform the update right away, choose 'Apply immediately'. If Apply immediately is not chosen, the update process is performed during the cluster's next maintenance window.\n10. When you're ready, choose 'Modify'.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-474":{"article":"A Classic Load Balancer can be set up to distribute incoming requests across Amazon EC2 instances in a single Availability Zone or multiple Availability Zones","impact":"A Classic Load Balancer that does not span multiple Availability Zones is unable to redirect traffic to targets in another Availability Zone if the sole configured Availability Zone becomes unavailable.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load balancers. \n3. Choose an Classic Load Balancer. \n4. On the Instances tab, choose Edit Availability Zones.\n5. On the Add and Remove Availability Zones page, select the Availability Zone.\n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-182":{"article":"You can create an on-demand mode table instead of provisioned so that you don't have to manage any capacity settings for servers, storage, or throughput. DynamoDB instantly accommodates your workloads as they ramp up or down to any previously reached traffic level. If a workload\u2019s traffic level hits a new peak, DynamoDB adapts rapidly to accommodate the workload. \nOr, you can allow DynamoDB auto scaling to manage your table's throughput capacity. However, you still must provide initial settings for read and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point, and then adjusts them dynamically in response to your application's requirements.","impact":"With provisioned capacity mode, it is harder to administer the DynamoDB table, this can affect DynamoDB data availability and it can increase DynamoDB costs.","report_fields":["TableArn"],"remediation":"To enable autoscaling:\n1. Open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n2. Choose the table that you want to update.\n3. Navigate to the 'Additional settings' tab.\n4. In 'Read/write capacity' section, click 'Edit'.\n5. In the 'Table capacity' settings section, set 'Auto scaling' to 'On' for 'Read capacity' or 'Write capacity', or both. For each of these, set your desired scaling policy for the table and, optionally, all global secondary indexes of the table.\n  - Minimum capacity units \u2014 Enter your lower boundary for the auto scaling range.\n  - Maximum capacity units \u2014 Enter your upper boundary for the auto scaling range.\n  - Target utilization \u2014 Enter your target utilization percentage for the table.\n6. When the settings are as you want them, choose 'Save changes'.\n\nTo enable On-demand capacity mode:\nTo enable autoscaling:\n1. Open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n2. Choose the table that you want to update.\n3. Navigate to the 'Additional settings' tab.\n4. In 'Read/write capacity' section, click 'Edit'.\n5. For 'Capacity mode' select 'On-demand'.\n6. Click 'Save changes'.","multiregional":false,"service":"Amazon DynamoDB"},"ecc-aws-175":{"article":"Encrypt Amazon RDS instances at rest by enabling the encryption option for your Amazon RDS DB instance.\nWhen dealing with production databases that hold sensitive and critical data, it is highly recommended to implement encryption in order to protect your data from unauthorized access. With RDS encryption enabled, the data stored on the instance underlying storage, the automated backups, Read Replicas, and snapshots, become all encrypted.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in RDS instances, backups, read replicas, and snapshots.","report_fields":["DBInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n    \nEncrypting Existing AWS RDS Database:\n1. Login to the AWS Management Console.\n2. Navigate to RDS dashboard at https://console.aws.amazon.com/rds/.\n3. In the navigation panel, under RDS Dashboard, click Instances.\n4. Select the RDS database instance that you want to encrypt.\n5. Click Instance Actions button from the dashboard top menu and select Take Snapshot.\n6. On the Take DB Snapshot page, enter a name for the instance snapshot in the Snapshot Name field and click Take Snapshot (the backup process may take few minutes and depends on your instance storage size).\n7. Select the new created snapshot and click the Copy Snapshot button from the dashboard top menu.\n8. On the Make Copy of DB Snapshot page, perform the following:\n  8.1 In the New DB Snapshot Identifier field, enter a name for the new snapshot (copy).\n  8.2 Check Copy Tags so the new snapshot can have the same tags as the source snapshot.\n  8.3 Select Yes from the Enable Encryption dropdown list to enable encryption. You can choose to use the AWS default encryption key or your custom key (key ARN required) by selecting it from the Master Key dropdown list.\n9. Click Copy Snapshot to create an encrypted copy of the selected instance snapshot.\n10. Select the new snapshot copy (encrypted) and click Restore Snapshot button from the dashboard top menu. This will restore the encrypted snapshot to a new database instance.\n11. On the Restore DB Instance page, enter a unique name for the new database instance in the DB Instance Identifier field.\n12. Review the instance configuration details and click Restore DB Instance.\n13. As soon as the new instance provisioning process is completed (its status becomes available), you can update your application configuration to refer to the endpoint of the new (encrypted) database instance. Once the database endpoint is changed at your application level, you can remove the unencrypted instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-061":{"article":"AWS Key Management Service (KMS) allows customers to rotate the backing key which is the key material stored within the KMS. The backing key is tied to the key ID of the customer created Customer Master Key (CMK). It is the backing key that is used to perform cryptographic operations such as encryption and decryption.\nAutomated key rotation currently retains all prior backing keys so that decryption of encrypted data can be transparent. It is recommended that CMK key rotation be enabled for symmetric keys. Key rotation can not be enabled for any asymmetric CMK.","impact":"Disabled rotation of KMS keys increases the risk of key exposure. Data encrypted with a compromised key can be accessed by an unauthorized user.","report_fields":["KeyArn","AWSAccountId"],"remediation":"1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam. \n2. In the left navigation pane, choose 'Encryption Keys'. \n3. Select a customer created master key (CMK) where 'Key spec = SYMMETRIC_DEFAULT'.\n4. Underneath the 'General configuration' panel open the tab 'Key rotation'. \n5. Check the 'Automatically rotate this KMS key every year' checkbox.","multiregional":false,"service":"AWS Key Management Service"},"ecc-aws-456":{"article":"Instance metadata is used to configure or manage the running instance. The IMDS provides access to temporary, frequently rotated credentials. These credentials remove the need to hard code or distribute sensitive credentials to instances manually or programmatically.\nVersion 2 of the IMDS adds new protections for the following types of vulnerabilities. These vulnerabilities could be used to try to access the IMDS:\n  - Open website application firewalls;\n  - Open reverse proxies;\n  - Server-side request forgery (SSRF) vulnerabilities;\n  - Open Layer 3 firewalls and network address translation (NAT).","impact":"Instances that use IMDSv1 are exposed to the following vulnerabilities:\n- Open website application firewalls;\n- Open reverse proxies;Server-side request forgery (SSRF) vulnerabilities;\n- Open Layer 3 firewalls and network address translation (NAT).","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nIn order to disable imdsv1 for existing AWS EMR clusters, you must create an EMR security configuration then clone clusters with the new security configuration. \nTo create a security configuration using the console:\n1. Open the Amazon EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, choose Security Configurations, Create security configuration.\n3. Type a Name for the security configuration.\n4. Enable 'Configure EC2 instance metadata service'.\n5. Click on the 'Turn off IMDSv1 and only allow IMDSv2 on cluster'.\n6. Create security configuration.\n\nTo clone a cluster using the console:\n1. Open the AWS EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, under EMR on EC2, choose Clusters.\n3. Select the cluster that you want to clone.\n4. At the top of the Cluster Details page, click Clone.\n5. In the dialog box, choose Yes to include the steps from the original cluster in the cloned cluster. Choose No to clone the original cluster's configuration without including any of the steps.\n6. The Create Cluster page appears with a copy of the original cluster's configuration. Review the configuration, make any necessary changes, and on the 'Step 4: Security' select the security configuration that you created earlier.\n7. Then click Create Cluster.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-489":{"article":"In Amazon EC2, Detailed Monitoring enables a more rapid response to performance changes in underlying infrastructure. These performance changes could result in a lack of availability of the data.","impact":"Without Detailed Monitoring, it is not possible to quickly respond to performance changes in the underlying infrastructure. This can lead to data unavailability.","report_fields":["InstanceId","OwnerId"],"remediation":"To enable detailed monitoring: \n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. \n2. In the navigation pane, under Instances, choose Instances. \n3. Open the instance, and then choose Monitoring. \n4. Click on the 'Manage detailed monitoring'.\n5. Enable detailed monitoring.\n6. Save changes","multiregional":false,"service":"Amazon EC2"},"ecc-aws-042":{"article":"As a best practice, use KMS key to encrypt the data in your storage bucket and ensure full control over your data.","impact":"Disabled S3 bucket encryption makes it impossible to protect S3 data at the bucket from attackers or unauthorized personnel.\nMoreover, S3-Managed Keys (SSE-S3) have several disadvantages compared to KMS keys (SSE-KMS).\nThere are separate permissions for the use of a KMS key that provides additional protection against unauthorized access to objects in S3. SSE-KMS also provides an audit trail that shows when the KMS key was used and by whom.","report_fields":["Name"],"remediation":"1. Login to the AWS Console.\n2. Go to S3.\n3. Select S3.\n4. Click on the properties.\n5. Click on the  Default encryption.\n6. Choose AWS-KMS.","multiregional":true,"service":"Amazon S3"},"ecc-aws-418":{"article":"This policy identifies the Kinesis data stream that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["StreamARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Kinesis at https://console.aws.amazon.com/kinesis.\n2. Click on 'Data streams'.\n3. Click on the required data stream.\n4. Open 'Configuration' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-502":{"article":"The RDS service releases engine version upgrades regularly to introduce new software features, bug fixes, security patches and performance improvements. Auto minor version upgrade option allows to have minor engine upgrades applied automatically to the replication instance during the maintenance window or immediately if you choose the Apply changes immediately option.","impact":"With an automatic update disabled, you are missing updates that may contain new software features, bug fixes, security patches and performance improvements.","report_fields":["DBInstanceArn"],"remediation":"To enable automatic minor version upgrade:  \n1. Open the RDS console at https://console.aws.amazon.com/rds.\n2. In the navigation pane, choose Databases. \n3. Choose instance and press modify. \n4. Scroll to the Maintenance section. \n5. Enable 'auto minor version upgrade'.\n6. Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-560":{"article":"Identifying unused Amazon SNS topics will reduce the costs of your AWS bill. A SNS topic is considered to be unused when it has 0 (zero) subscriptions confirmed or messages have not been published to a topic for 30 days.\nUnless there is a business need to retain unused SNS topics, you should remove them to avoid incurring unexpected charges and to maintain an accurate inventory of system components.","impact":"Keeping unused Amazon SNS topics can result in escalating costs and cluttered AWS accounts.","report_fields":["TopicArn"],"remediation":"Perform the following to create a new subscription:\n  1. Sign in to the Amazon SNS console https://console.aws.amazon.com/sns/home.\n  2. On the console, select the specific region. \n  3. Select the desired topic you want to subscribe to.\n  4. Go to 'Subscriptions' section.\n  5. Click 'Create subscription' and configure a new subscription.\n  5. When you have finished, click 'Create subscription'.\n\nPerform the following to delete a topic:\n  1. Sign in to the Amazon SNS console https://console.aws.amazon.com/sns/home.\n  2. On the console, select the specific region. \n  3. On the 'Topics' page, choose a topic you want to delete.\n  4. Choose 'Delete' and confirm deletion.","multiregional":false,"service":"Amazon Simple Notification Service"},"ecc-aws-543":{"article":"By having logs immediately available, an entity can quickly identify and minimize impact of a data breach.","impact":"Without formal processes to detect exceptions and anomalies, the entity may be unaware of unauthorized and potentially malicious activities occurring within their network.","report_fields":["ARN"],"remediation":"To create a real-time log configuration (console):\n1. Sign in to the AWS Management Console and open the Logs page in the CloudFront console.\n2. Choose Real-time log configurations.\n3. Choose Create configuration.\n4. Choose the desired setting for the real-time log configuration.\n5. When finished, choose Create configuration.\nIf successful, the console shows the details of the real-time log configuration that you just created.\n\nTo create a real-time log configuration (CLI with input file):\n1. Use the following command to create a file named rtl-config.yaml that contains all of the input parameters for the create-realtime-log-config command.\n  aws cloudfront create-realtime-log-config --generate-cli-skeleton yaml-input > rtl-config.yaml\n2. Open the file named rtl-config.yaml that you just created. Edit the file to specify the real-time log configuration settings that you want, then save the file.\n  For more information about the real-time long configuration settings, see Understanding real-time log configurations (https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/real-time-logs.html#understand-real-time-log-config).\n3. Use the following command to create the real-time log configuration using input parameters from the rtl-config.yaml file.\n  aws cloudfront create-realtime-log-config --cli-input-yaml file://rtl-config.yaml\nIf successful, the command's output shows the details of the real-time log configuration that you just created.\n\nTo attach a real-time log configuration to an existing distribution (CLI with input file):\n1. Use the following command to save the distribution configuration for the CloudFront distribution that you want to update. Replace distribution_ID with the distribution's ID.\n  aws cloudfront get-distribution-config --id distribution_ID --output yaml > dist-config.yaml\n2. Open the file named dist-config.yaml that you just created. Edit the file, making the following changes to each cache behavior that you are updating to use a real-time log configuration.\n  - In the cache behavior, add a field named RealtimeLogConfigArn. For the field's value, use the ARN of the real-time log configuration that you want to attach to this cache behavior.\n  - Rename the ETag field to IfMatch, but don't change the field's value.\n3. Use the following command to update the distribution to use the real-time log configuration. Replace distribution_ID with the distribution's ID.\n  aws cloudfront update-distribution --id distribution_ID --cli-input-yaml file://dist-config.yaml\nIf successful, the command's output shows the details of the distribution that you just updated.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-519":{"article":"A VPN tunnel is an encrypted link where data can pass from the customer network to or from AWS within an AWS Site-to-Site VPN connection. Each VPN connection includes two VPN tunnels which you can simultaneously use for high availability. Ensuring that both VPN tunnels are up for a VPN connection is important for confirming a secure and highly available connection between an AWS VPC and your remote network.\nIf there's a device failure within AWS, your VPN connection automatically fails over to the second tunnel so that your access isn't interrupted. From time to time, AWS also performs routine maintenance on your VPN connection, which may briefly disable one of the two tunnels of your VPN connection.","impact":"When both tunnels have state 'DOWN' a VPN connection is not available at all. When one of the tunnels is in the 'DOWN' state, the site-to-site VPN connection is not redundant and if the second tunnel also becomes unavailable, the site-to-site VPN connection is interrupted.","report_fields":["VpnConnectionId"],"remediation":"To remediate this issue, you have to configure a tunnel that has 'DOWN' state, or if there is a problem with a connection that requires troubleshooting, use the following AWS documentation for solving the problem: https://docs.aws.amazon.com/vpn/latest/s2svpn/Troubleshooting.html.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-288":{"article":"The Amazon WorkSpaces service periodically checks the health of a WorkSpace by sending it a status request. The WorkSpace is marked as unhealthy if a response isn\u2019t received from the WorkSpace in a timely manner.","impact":"If a WorkSpace can not respond and is marked as unhealthy, it can be inoperable and affect the ability of staff to perform their duties.","report_fields":["WorkspaceId"],"remediation":"The following troubleshooting steps can return the WorkSpace to a healthy state:\nFirst, reboot the WorkSpace from the Amazon WorkSpaces console. If rebooting the WorkSpace doesn't resolve the issue, either use RDP, or connect to an Amazon Linux WorkSpace using SSH.\nIf the WorkSpace is unreachable by a different protocol, rebuild the WorkSpace from the Amazon WorkSpaces console.\nIf a WorkSpaces connection cannot be established, verify the following:\n- Verify CPU utilization\n- Verify the computer name of the WorkSpace\n- Confirm that WorkSpaces services are running and responsive\n- Verify Firewall rules\n\nMore information could be found here: https://aws.amazon.com/premiumsupport/knowledge-center/workspaces-unhealthy/.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-350":{"article":"Broker logs enable you to troubleshoot your Apache Kafka applications and to analyze their communications with your MSK cluster. \nYou can configure new or existing MSK cluster to deliver INFO-level broker logs to one or more of the following types of destination resources: a CloudWatch log group, an S3 bucket, a Kinesis Data Firehose delivery stream. Through Kinesis Data Firehose you can then deliver the log data from your delivery stream to OpenSearch Service.","impact":"Without logging it is harder to troubleshoot Apache Kafka applications and to analyze their communications with MSK cluster.","report_fields":["ClusterArn"],"remediation":"Perform the following steps to enable log delivery for MSK:\n1. Login to the MSK console at https://console.aws.amazon.com/msk/.\n2. Click on the Clusters.\n3. Choose required cluster.\n4. Click on the Actions and 'Edit Log Delivery'.\n5. Under the 'Broker log delivery' choose one or multiple delivery methods.\n6. Save changes.","multiregional":false,"service":"Amazon Managed Streaming for Apache Kafka"},"ecc-aws-066":{"article":"The EKS cluster must not be publicly accessible from the Internet. As the cluster endpoint is the entry point for managing the cluster, an attacker may cause damage to the infrastructure or data loss.","impact":"Everyone and everything on the Internet can establish a connection to EKS clusters and this can increase the opportunity for malicious activities and attacks.","report_fields":["arn"],"remediation":"1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.  \n2. Choose the name of the cluster to display your cluster information.  \n3. Under Networking, choose Update.  \n4. For Private access, choose whether to enable or disable private access to your cluster's Kubernetes API server endpoint. If you enable private access, Kubernetes API requests that originate from within your cluster's VPC use the private VPC endpoint. You must enable private access to disable public access.  \n5. For Public access, choose whether to enable or disable public access to your cluster's Kubernetes API server endpoint. If you disable public access, your cluster's Kubernetes API server can only receive requests from within the cluster VPC. \n6. (Optional) If you've enabled Public access, you can specify which addresses from the Internet can communicate with the public endpoint. Select Advanced Settings. Enter a CIDR block, such as 203.0.113.5/32. The block cannot include reserved addresses. You can enter additional blocks by selecting Add Source. There is a maximum number of CIDR blocks that you can specify.            \nFor more information, see Amazon EKS service quotas (p. 297). If you specify no blocks, then the public API server endpoint receives requests from all (0.0.0.0/0) IP addresses. \nIf you restrict access to your public endpoint using CIDR blocks, it is recommended that you also enable private endpoint access so that worker nodes and Fargate pods (if you use them) can communicate with the cluster. Without the private endpoint enabled, your public access endpoint CIDR sources must include the egress sources from your VPC. \nFor example, if you have a worker node in a private subnet that communicates to the internet through a NAT Gateway, you will need to add the outbound IP address of the NAT gateway as part of a whitelisted CIDR block on your public endpoint.  \n7. Choose Update to finish.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-370":{"article":"It is recommend to maintain WorkSpaces on a regular basis. WorkSpaces schedules default maintenance windows for your WorkSpaces. During the maintenance window, the WorkSpace installs important updates from Amazon WorkSpaces and reboots as necessary. If available, operating system updates are also installed from the OS update server that the WorkSpace is configured to use. During maintenance, your WorkSpaces might be unavailable.","impact":"Without keeping the WorkSpaces up-to-date, it is possible to miss out on bug fixes, security patches, and performance improvements.","report_fields":["DirectoryId"],"remediation":"Perform the following steps to enable maintenance mode From the Console:\n1. Login to the WorkSpaces console at https://console.aws.amazon.com/workspaces/.\n2. In the left pane, click \"Directories\".\n3. Select your directory.\n4. Choose \"Actions\", \"Update Details\".\n5. Expand \"Maintenance Mode\".\n6. To enable automatic updates, choose \"Enabled\".\n7. Click \"Update and Exit\".\nNote: If you prefer to manage updates manually or with another tool document usage of that, and choose Disabled.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-136":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements, or compliance standards. It is highly recommended that access to data be restricted to encrypted protocols. \nThis rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/.\n2. Go to Load Balancers.\n3. Click on the reported Load Balancer.\n4. In the 'Description' tab, under 'Security' select security group to edit.\n5. In the 'Inbound rules' tab, click 'Edit inbound rules'.\n6. Modify rules.\n7. Click on 'Save rules'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-298":{"article":"When you create and use your own KMS CMK customer-managed keys to protect the contents of your SQS queue messages, you obtain full control over who can use the CMK keys and access the data encrypted within queue messages. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs) for Amazon SQS.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["QueueArn"],"remediation":"1. Navigate to https://console.aws.amazon.com/sqs/v2\n2. Choose Queues.\n3. Click on the reported Queue.\n4. Press Edit.\n5. Under Encryption, enable Server-side encryption.\n6. Choose AWS Key Management Service key.\n7. Choose Customer key.","multiregional":false,"service":"Amazon Simple Queue Service"},"ecc-aws-361":{"article":"To help debug issues related to request execution to API Gateway REST API, you can enable Amazon CloudWatch Logs to log API calls. \nExecution logs contain information that you can use to identify and troubleshoot most API errors.","impact":"With disabled monitoring and logging of API Gateway, it may be difficult to troubleshoot any issues that might happen with APIs.","report_fields":["restApiId","stageName"],"remediation":"Create an IAM role for logging to CloudWatch:\n1. In the AWS Identity and Access Management (IAM) console, in the left navigation pane, choose Roles.\n2. On the Roles pane, choose Create role.\n3. On the Create role page, do the following:\n  3.1 For Trusted entity type, choose AWS Service.\n  3.2 For use case, choose API Gateway.\n  3.3 Choose the API Gateway radio button.\n  3.4 Choose Next.\n4. Under Permissions Policies, note that the AWS managed policy AmazonAPIGatewayPushToCloudWatchLogs is selected by default. The policy has all the required permissions.\n5. Choose Next.\n6. Under Name, review and create, do the following:\n  6.1 For Role name, enter a meaningful name for the role.\n  6.2 (Optional) For Role description, edit the description to your preferences.\n  6.3 (Optional) Add tags.\n  6.4 Choose Create role.\n7.    On the Roles pane, in the search bar, enter the name of the role that you created. Then, choose the role from the search results.\n8.    On the Summary pane, copy the Role ARN. You'll need this Amazon Resource Name (ARN) in the next section.\n\nAdd the IAM role in the API Gateway console:\nNote: If you're developing multiple APIs across different AWS Regions, complete these steps in each Region.\n1. In the API Gateway console, on the APIs pane, choose the name of an API that you created.\n2. In the left navigation pane, at the bottom, below the Client Certificates section, choose Settings.\n3. Under Settings, for CloudWatch log role ARN, paste the IAM role ARN that you copied.\n4. Choose Save.\nNote: The console doesn't confirm that the ARN is saved.\n\nTurn on logging for your API and stage:\n1. In the API Gateway console, find the Stage Editor for your API.\n2. On the Stage Editor pane, choose the Logs/Tracing tab.\n3. On the Logs/Tracing tab, under CloudWatch Settings, do the following to turn on execution logging:\n  3.1 Choose the Enable CloudWatch Logs check box.\n  3.2 For Log level, choose INFO to generate execution logs for all requests. Or, choose ERROR to generate execution logs only for requests to your API that result in an error.\n4. Under Custom Access Logging, do the following to turn on access logging:\n  4.1 Choose the Enable Access Logging check box.\n  4.2 For Access Log Destination ARN, enter the ARN of a CloudWatch log group or an Amazon Kinesis Data Firehose stream.\n  4.3 Enter a Log Format. For guidance, choose CLF, JSON, XML, or CSV to see an example in that format.\n5. Choose Save Changes.\nNote: The console doesn't confirm that settings are saved.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-412":{"article":"This policy identifies the FSX backups that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["FileSystemId"],"remediation":"1. Open the Amazon FSx for Lustre console at https://console.aws.amazon.com/fsx/.\n2. Click on the 'Backups'.\n3. CLick on the required backup.\n4. Under the 'Tags' click on the 'Add'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-025":{"article":"Ensure protection against accidental termination of instances is enabled.","impact":"With numerous instances running in the infrastructure, it is quite possible to delete an EC2 instance by mistake. It can result in the loss of critical resources such as sensitive data or computing resources for production workloads. By default, the volumes associated with EC2 instances are deleted when these are terminated.\nBy enabling the Termination Protection feature, you can ensure safety of EBS data.","report_fields":["InstanceId","OwnerId"],"remediation":"To enable termination protection for an instance at launch, do as follows:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. \n2. On the dashboard, choose Launch Instance and follow the directions in the wizard. \n3. On the Configure Instance Details page, select the Enable termination protection check box.  \nTo enable termination protection for a running or stopped instance, do as follows: \n1. Select the instance and choose in sequence: Actions, Instance Settings, Change Termination Protection. \n2. Choose Yes, Enable.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-300":{"article":"The access policy defines the accounts, users, and roles that can access the queue. The access policy also defines the actions (such as SendMessage, ReceiveMessage, or DeleteMessage) that the users can access.\nTo avoid data leakage and unexpected costs on your AWS bill, limit access to your queues by implementing the necessary policies and avoid granting access to all users (anonymous users).","impact":"Publicly-available Amazon SQS queues give unauthorized users access to potentially intercept, delete, or send queue messages, which can lead to data leaks.","report_fields":["QueueArn"],"remediation":"To configure the access policy for an existing queue:\n1. Open the Amazon SQS console at https://console.aws.amazon.com/sqs/.\n2. In the navigation pane, choose Queues.\n3. Choose a queue and choose Edit.\n4. Scroll to the Access policy section.\n5. Edit the access policy statements in the input box. Clear the Everybody (*) checkbox and enter the AWS account ID of the person allowed or denied.\n6. When you finish configuring the access policy, choose Save.","multiregional":false,"service":"Amazon Simple Queue Service"},"ecc-aws-322":{"article":"SQL92_SECURITY specifies whether users must have been granted the SELECT object privilege in order to execute such UPDATE or DELETE statements.","impact":"A user without a SELECT privilege can still infer the value stored in a column by referring to that column in a DELETE or UPDATE statement. This setting prevents inadvertent information disclosure by ensuring that only users who already have the SELECT privilege can execute the statements that would allow them to infer the stored values.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type 'sql92_security'.\n6. Choose TRUE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-499":{"article":"Ensure that all the IAM groups within your AWS account are currently used and have at least one user attached. Otherwise, remove any orphaned (unused) IAM groups in order to prevent attaching unauthorized users.","impact":"Removing orphaned and unused IAM groups eliminates the risk that a forgotten group will be used accidentally to allow unauthorized users to access AWS resources.","report_fields":["Arn"],"remediation":"1. Login to the IAM AWS Management Console at https://console.aws.amazon.com/iamv2.\n2. In the left pane, click on 'User Groups'.\n3. Delete all empty groups.","multiregional":false,"service":"AWS Identity and Access Management"},"ecc-aws-431":{"article":"This policy identifies the SNS that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["TopicArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EFS at https://console.aws.amazon.com/sns/v3.\n2. Click on the required sns topic.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Simple Notification Service"},"ecc-aws-120":{"article":"Enable Server Side Encryption (SSE) of your AWS Kinesis Server data at rest to protect your data and metadata from breach or unauthorized access, and fulfill compliance requirements for data-at-rest encryption within your organization.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in Amazon Kinesis Server","report_fields":["StreamARN"],"remediation":"1. Navigate to the Kinesis dashboard at https://console.aws.amazon.com/kinesis/.\n2. In the Navigation pane, under Amazon Kinesis, choose Streams. \n3. Select the relevant Kinesis stream, click on the Actions drop-down menu and select Details to access the stream configuration details. \n4. Choose the Details tab from the top panel and verify the SSE feature status available next to Server-side encryption is set to Enable.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-225":{"article":"Control plane logs provide visibility into operation of the EKS Control plane component systems. The API server audit logs record all accepted and rejected requests in the cluster. When enabled via EKS configuration the control plane logs for a cluster are exported to a CloudWatch Log Group for persistence.","impact":"With disabled control plane logging, it will not be possible to detect anomalous configuration activity, track configuration changes conducted manually and programmatically, and trace back unapproved changes.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Management Console and open the EKS console at https://console.aws.amazon.com/eks.\n2. Click on Cluster Name of the cluster you are auditing.\n3. Click 'Logging'.\n4. Select 'Manage Logging' from the button on the right side.\n5. Toggle each selection to the 'Enabled' position.\n6. Click 'Save Changes'.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-151":{"article":"TCP ports 20 is used for data transfer and communication by the File Transfer Protocol (FTP) client-server applications.\nUnrestricted access (0.0.0.0/0) to port 20 increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data (spoofing attacks and packet capture).","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console \n1. Login to the AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/ \n2. In the navigation pane, choose Security Groups. \n3. Select the security group to update, choose Actions, and then choose 'Edit inbound rules' to remove an inbound rule or 'Edit outbound rules' to remove an outbound rule. \n4. Choose the Delete button to the right of the rule to delete. \n5. Choose Preview changes, than choose Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-109":{"article":"Checks an ACM for Invalid or Failed certificates. An Invalid certificate is the one that has not been validated within 72 hours. A certificate fails 'for these reasons:' \n- the certificate is requested for invalid public domains;\n- the certificate is requested for domains which are not allowed or missing contact information;\n- typographical errors.\nThese certificates cannot be used, and you will have to request new ones. It is recommended to delete Failed or Invalid certificates.","impact":"Invalid or failed certificates can result in deploying an invalid certificate in resources. This may trigger an error in the front end and cause a loss of credibility for the web application/website.","report_fields":["CertificateArn","DomainName"],"remediation":"To delete certificates: \n1. Sign in to the AWS console.\n2. On the console, select the region. \n3. Navigate to the Certificate Manager(ACM) service. \n4. Select the certificate that was reported. \n5. Under the 'Actions' drop-down list, click on 'Delete'.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-415":{"article":"This policy identifies the IAM Users that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["Arn"],"remediation":"1. Login to the AWS Console.\n2. Find the IAM service.\n3. Choose Users without tags.\n4. Click on the \"Tags\".\n5. Click on the \"Add tags\".\n6. Add tag.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-166":{"article":"Remote Procedure Call (RPC) TCP port 135 is used for client-server communications by Microsoft Message Queuing (MSMQ) as well as other Microsoft Windows/Windows Server software.\nUnrestricted access (0.0.0.0/0) increases opportunities for  malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:  \n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-012":{"article":"Enforce the use of secure ciphers TLS v1.2 in the CloudFront Distribution certificate configuration. This is the best security practice. This signature scans for any deviations from this practice and returns the results.","impact":"Insecure and deprecated ciphers could make an SSL connection between the CloudFront and viewers vulnerable to exploits and result in a breach of confidentiality or integrity.","report_fields":["ARN"],"remediation":"On the AWS CloudFront console, check the details for each distribution that failed the rule and ensure that the Viewer Protocol Policy is HTTPS Only or Redirect HTTP to HTTPS.\nPerform the following for each distribution that failed the rule:\n1. Navigate to the the AWS console CloudFront dashboard at https://console.aws.amazon.com/cloudfront.\n2. Select your distribution ID.\n3. On the 'General' tab.\n4. Click on 'Edit' in the 'Settings' section.\n5. For 'Security policy' choose the latest 'TLSv1.2' protocol version.\n6. Click on Save changes.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-134":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements or compliance standards. It is highly recommended that access to data be restricted to encrypted protocols. This rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/.\n2. Go to Load Balancers.\n3. Click on the reported Load Balancer.\n4. In the 'Description' tab, under 'Security' select security group to edit.\n5. In the 'Inbound rules' tab, click 'Edit inbound rules'.\n6. Modify rules.\n7. Click on 'Save rules'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-052":{"article":"AWS CloudTrail is a web service that records AWS API calls for your account and delivers log files to you. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. \nCloudTrail provides a history of AWS API calls for an account, including API calls made via the Management Console, SDKs, command line tools, and higher-level AWS services (such as CloudFormation).","impact":"Without enabling CloudTrail across different regions, it is hard to monitor the infrastructure and maintain security across all regions.","report_fields":["account_id","account_name"],"remediation":"1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/cloudtrail. \n2. Click on Trails in the left navigation pane. \n3. Click on Get Started Now, if presented:   \n- Click on Add new trail;\n- Enter a trail name in the Trail name box;\n- Set the Apply trail to all regions option to Yes;\n- Specify the S3 bucket name in the S3 bucket box;\n- Click on Create. \n4. If one or more trails already exist, select the target trail to enable for global logging. \n5. Click on the edit icon (pencil) next to Apply trail to all regions, click Yes, and click Save. \n6. Click on the edit icon (pencil) next to Management Events, click All for setting Read/Write Events, and click Save.","multiregional":true,"service":"AWS CloudTrail"},"ecc-aws-221":{"article":"Encrypting data at rest reduces the risk of data stored on disk being accessed by a user not authenticated to AWS. It also adds another set of access controls to limit the ability of unauthorized users to access the data. \nFor example, API permissions are required to decrypt the data before it can be read. SNS topics should be encrypted at-rest for an added layer of security.","impact":"Unauthorized users can read confidential information available on SNS topics.","report_fields":["TopicArn"],"remediation":"1. Open the Amazon SNS console at https://console.aws.amazon.com/sns/v3/home.\n2. In the navigation pane, choose Topics. \n3. Choose the name of the topic to encrypt.\n4. Choose Edit. \n5. Under Encryption, choose Enable Encryption. \n6. Choose the KMS key to use to encrypt the topic. \n7. Choose Save changes.","multiregional":false,"service":"Amazon Simple Notification Service"},"ecc-aws-400":{"article":"This policy identifies the DAX that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ClusterArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon DynamoDB at https://console.aws.amazon.com/dynamodbv2.\n2. Click on the Clusters.\n3. Click on the required dax cluster.\n4. Open 'Settings' click on 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon DynamoDB Accelerator"},"ecc-aws-087":{"article":"When Amazon Redshift clusters are publicly accessible and have a public IP address, every machine on the Internet can establish a connection to these clusters which increases the opportunity for malicious activity such as SQL injections or Distributed Denial of Service (DDoS) attacks. Unless you intend to make your cluster publicly accessible, it should not be configured with public access.","impact":"When Amazon Redshift clusters are publicly accessible and have a public IP address, every machine on the Internet can establish a connection to these clusters which increases the opportunity for malicious activity such as SQL injections or Distributed Denial of Service (DDoS) attacks. Unless you intend to make your cluster publicly accessible, it should not be configured with public access.","report_fields":["ClusterIdentifier"],"remediation":"1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. On the navigation pane, choose Clusters and then select your public Amazon Redshift cluster. \n3. From the Cluster drop-down menu, choose Modify cluster. \n4. In Publicly accessible, choose No. \n5. Choose Modify.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-233":{"article":"The debug_print_parse setting enables printing the resulting parse tree for each executed query. These messages are emitted at the LOG message level. Unless directed otherwise by your organization's logging policy, it is recommended this setting be disabled by setting it to off. Enabling any of the DEBUG printing variables may cause the logging of sensitive information that would otherwise be omitted based on the configuration of the other logging settings.","impact":"Enabling any of the DEBUG printing variables may cause the logging of sensitive information that would otherwise be omitted based on the configuration of the other logging settings.\nUnless directed otherwise by your organization's logging policy, it is recommended this setting be disabled by setting it to 0.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type 'debug_print_parse'.\n6. Choose 0 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-441":{"article":"Cache encryption comes in the following two flavors. These are similar to the settings that ElastiCache for Redis allows. You can enable the encryption settings only when first enabling caching for your AWS AppSync API.\n- Encryption in transit \u2013 Requests between AWS AppSync, the cache, and data sources (except insecure HTTP data sources) are encrypted at the network level. Because there is some processing needed to encrypt and decrypt the data at the endpoints, in-transit encryption can impact performance.\n- Encryption at rest \u2013 Data saved to disk from memory during swap operations are encrypted at the cache instance. This setting also impacts performance.","impact":"Disabled encryption allows unauthorized users to gain access to AppSync data saved to disk from memory during swap operations. With encryption enabled, this data is protected.","report_fields":["name","arn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt, cache you should delete the existing cache and recreate it.\n1. Navigate to the https://console.aws.amazon.com/appsync/home\n2. Select the API that for which you want enable encryption.\n3. At the left panel, choose 'Caching'.\n4. Click 'Delete cache'.\n5. Submit deletion.\n6. Select caching behavior:\n  - Full request caching: All requests are fully cached.\n  - Per-resolver caching: Individual resolvers that you specify are cached.\n7. In 'Cache settings' section, make sure you have selected 'Encryption at rest' option.\n8. Click 'Create cache'.","multiregional":false,"service":"AWS AppSync"},"ecc-aws-419":{"article":"This policy identifies the Kinesis video stream that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["StreamARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Kinesis Video at https://console.aws.amazon.com/kinesisvideo\n2. Click on 'Video streams'.\n3. Click on the required video stream.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-104":{"article":"Geo Restriction has the ability to block IP addresses based on Geo IP by whitelist or blacklist a country in order to allow or restrict users in specific locations from accessing web application content. \nIt is recommended to have geo restriction feature enabled, to restrict or allow users in specific locations accessing web application content. This configuration helps with prevention of DDos Attacks.","impact":"Not enabling CloudFront geo restriction can lead to user access to content from unwanted geographic locations and possible DoS attacks.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS console.\n2. Select the region from the drop-down list in which the issue has occured.\n3. Navigate to CloudFront Distributions Dashboard.\n4. Select relevant Distribution.\n5. On 'Restrictions' tab, click the 'Edit' button.\n6. On 'Edit Geo-Restrictions' page, Set 'Enable Geo-Restriction' to 'Yes' and  whitelist/blacklist countries as per your requirement. \n7. Click 'Yes, Edit'.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-013":{"article":"Ensure that a SSL policy for a Classic Load balancer is based on TLS 1.3 or TLS 1.2, and not on TLS 1.0,TLS 1.1, SSLv2, SSLv3.","impact":"Insecure and deprecated ciphers for ELB Predefined Security Policy or Custom Security Policy could make an SSL connection between the client and the load balancer vulnerable to exploits and result in a breach of confidentiality or integrity.","report_fields":["LoadBalancerArn"],"remediation":"Login to the AWS Management Console. \n1. Navigate to the EC2 dashboard.\n2. On the navigation panel, under 'Load balancing', click on 'Load Balancers'.\n3. Select your Elastic Load Balancer.\n4. Select the 'Listeners' tab from the bottom panel. In the 'Cipher' column of the HTTPS listener, click 'Change'.\n5. Review the 'SSL Ciphers' section for any insecure / deprecated cipher definitions. \nThe following list defines all the insecure ciphers that should be removed: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-security-policy-table.html","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-547":{"article":"Using the latest generation classes of RDS database instances over their predecessors offers concrete advantages. These encompass enhanced hardware performance, including expanded computing capacity and swifter CPUs, optimized memory utilization, and increased network throughput. Additionally, staying up-to-date with the latest database engine versions, such as MySQL 5.7, is better facilitated. This transition can also lead to cost savings in terms of memory and storage expenditure.","impact":"Not using the latest generation of RDS database you will get less performance for more costs. Additionally, oldest generations not supporting latest database engine versions, such as MySQL 5.7.","report_fields":["DBInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nChanging AWS RDS Database type:\n1. Login to the AWS Management Console.\n2. Navigate to RDS dashboard at https://console.aws.amazon.com/rds/.\n3. In the navigation panel, under RDS Dashboard, click Instances.\n4. Select the RDS database instance that will be changed.\n5. Click Instance 'Modify' button.\n6. Change 'DB instance class'.\n7. Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-308":{"article":"An Amazon S3 Glacier vault is the primary resource in S3 Glacier. You can add permissions to the policy associated with a S3 Glacier vault. An Amazon S3 Glacier vault access policy is a resource-based policy that you can use to manage permissions to your vault. \nYou can now define and use the policy to grant access to individual users, business groups, and to external business partners. Using a single access policy to control access to a vault can be simpler than using individual user and group IAM policies in many cases.","impact":"An Amazon S3 Glacier with a policy that allows anyone to access your resource (\"Principal\": \"*\") can lead to data leaks.","report_fields":["VaultARN"],"remediation":"To configure the access policy for an existing Amazon S3 Glacier:\n1. Open the Amazon S3 Glacier console at https://console.aws.amazon.com/glacier/home.\n2. Choose Vault that you want to reconfigure.\n3. Choose Permissions tab.\n4. Click 'Edit policy document' button.\n5. Edit 'Principal' value by deleting \"*\"  and then enter an allowed AWS account ID or a user.\n6. When you finish configuring the access policy, choose Save.","multiregional":false,"service":"Amazon S3 Glacier"},"ecc-aws-234":{"article":"The debug_print_rewritten setting enables printing the query rewriter output for each executed query. These messages are emitted at the LOG message level. Unless directed  otherwise by your organization's logging policy, it is recommended this setting be disabled by setting it to off.","impact":"Enabling any of the DEBUG printing variables may cause the logging of sensitive information that would otherwise be omitted based on the configuration of the other logging settings.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type 'debug_print_rewritten'.\n6. Choose 0 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-100":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes made to VPCs.","impact":"Lack of monitoring and logging of VPC changes can result in insufficient response time to detect accidental or intentional modifications that may lead to unauthorized network access or other security breaches.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = CreateVpc) || ($.eventName = DeleteVpc) || ($.eventName = ModifyVpcAttribute) || ($.eventName = AcceptVpcPeeringConnection) || ($.eventName = CreateVpcPeeringConnection) || ($.eventName = DeleteVpcPeeringConnection) || ($.eventName = RejectVpcPeeringConnection) || ($.eventName = AttachClassicLinkVpc) || ($.eventName = DetachClassicLinkVpc) || ($.eventName = DisableVpcClassicLink) || ($.eventName = EnableVpcClassicLink)}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-238":{"article":"PostgreSQL does not maintain the beginning or ending of a connection internally for later review. It is only by enabling the logging of these that one can examine connections for failed attempts, 'over long' duration, or other anomalies. \nNote that enabling this without also enabling log_connections provides little value. Generally, you would enable/disable the pair together.","impact":"PostgreSQL database engine does not log information such as session duration and session end by default. Without the \"log_disconnections\" flag we cannot  properly record PostgreSQL activity data that can be useful to identify, troubleshoot, and repair configuration errors and sub-optimal performance for your PostgreSQL database instances.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_disconnections\".\n6. Choose \"1\" in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-044":{"article":"This policy identifies the AWS S3 buckets that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["Name"],"remediation":"1. Login to the AWS Console.\n2. Find the S3 service.\n3. Choose the respective S3 Bucket witout tags.\n4. Click on the Properties tab.\n5. Find Tags, click on it and add a new one.\n6. Save it.","multiregional":true,"service":"Amazon S3"},"ecc-aws-377":{"article":"This policy identifies the AMI images that do not have any tags. Tags can be used for an easy identification and search process.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ImageId","OwnerId"],"remediation":"1. Sign in to the AWS Management Console and open the EC2 console at https://console.aws.amazon.com/ec2/v2/.\n2. In the navigation pane under 'Images' choose 'AMIs'.\n3. Choose required AMI.\n4. Open 'Tags' tab and click on 'Manage tags'.\n5. Add new tags.\n6. Click 'Save'.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-382":{"article":"This policy identifies the Internet Gateway that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["InternetGatewayId","OwnerId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/\n2. Click on the 'Internet gateways'.\n3. Click on the required internet gateway.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-448":{"article":"Amazon MWAA can send Apache Airflow logs to Amazon CloudWatch. You can view logs for multiple environments from a single location to easily identify Apache Airflow task delays or workflow errors without the need for additional third-party tools.","impact":"If 'Worker logs' is not enabled and the 'log_level' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Click \"Next\".\n5. Enable 'Airflow Worker logs'.\n6. Choose required log level.\n7. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-003":{"article":"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. After you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs. It is recommended that VPC Flow Logs be enabled for packet 'Rejects' for VPCs.","impact":"Without enabled VPC Flow Logs, you miss out on the opportunity to detect security and access issues like overly permissive security groups or network ACLs. Also, you cannot receive alerts about abnormal activities triggered within your VPC network, such as rejected connection requests or unusual levels of data transfer.","report_fields":["VpcId","OwnerId"],"remediation":"1. Sign into the management console. \n2. Select Services, then select a VPC. \n3. In the left navigation pane, select Your VPCs. \n4. Select the VPC. \n5. In the right pane, select the Flow Logs tab. \n6. If no Flow Log exists, click Create Flow Log. \n7. For Filter, select Reject. \n8. Enter in Role and Destination Log Group. \n9. Click on Create Log Flow. \n10. Click on CloudWatch Logs Group.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-423":{"article":"This policy identifies the Log group that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon CloudWatch at https://console.aws.amazon.com/cloudwatch.\n2. Click on the 'Log groups'.\n3. Click on the required log group.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon CloudWatch"},"ecc-aws-272":{"article":"Engine version management is designed so that you can have as much control as possible over how patching occurs. \nHowever, ElastiCache reserves the right to patch your cluster on your behalf in the unlikely event of a critical security vulnerability in the system or cache software.","impact":"Without keeping the ElastiCache up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nAWS ElastiCache Redis cluster  engine version can be set only at the time of the cluster creation. To fix this issue, create a new cluster with the latest engine version, migrate all the required ElastiCache Redis cluster data from the old cluster to the new one, and then delete the old cluster:\n1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n3. Choose 'Redis clusters' from the navigation pane.\n5. Click on the 'Create Redis cluster' button.\n6. For 'Engine version' select the latest one.\n7. Complete 'Cluster settings' page and click 'Next'.\n8. Complete 'Advanced settings' page and click 'Next'.\n9. Click on 'Create' button to launch your new ElastiCache Redis cluster.\n\nTo create an ElastiCache for Memcached cluster:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' from the navigation pane.\n4. Choose 'Create Memcached cluster'.\n5. Under 'Cluster settings', for 'Engine version' select the latest one.\n6. Complete the 'Cluster settings' section. \n7. Click 'Next'.\n8. Complete the 'Advanced settings' section. \n9. Click 'Next'.\n10. Review all your entries and choices, then go back and make any needed corrections. When you're ready, choose 'Create' to launch your cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-244":{"article":"Enabling the 'log_lock_waits' flag for a PostgreSQL instance creates a log for any session waits that take longer than the allotted deadlock_timeout time to acquire a lock.","impact":"Not logging sessions longer than the deadlock_timeout by enabling \"log_lock_waits\" database flag can lead to insufficient response time to identify poor performance due to locking delays.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_lock_waits\".\n6. Choose 1.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-168":{"article":"TCP port 1433 is used by the Microsoft SQL Server which is a relational database management system developed by Microsoft.\nUnrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:\n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-174":{"article":"If you use a known port to deploy an RDS cluster, an attacker can guess information about the cluster. The attacker can use this information together with other information to connect to an RDS cluster or instance or gain additional information about your application.\nUsing a custom port can protect against potential brute-force and dictionary attacks. When you change the port, you must also update the existing connection strings that were used to connect to the old port.\nChanging the default port number for RDS database clusters represents a basic security measure and does not completely secure the clusters from port scanning and network attacks.","impact":"If you use a known port to deploy an RDS cluster, an attacker can guess information about the cluster. The attacker can use this information together with other information to connect to an RDS cluster or instance or gain additional information about your application.\nUsing default ports also increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["DBClusterArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Databases.\n3. Select the Cluster to modify.\n4. Choose Modify.\n5. Under Database options, change Database port to a non-default value.\n6. Choose Continue.\n7. Under Scheduling of modifications, choose when to apply modifications. You can choose either Apply during the next scheduled maintenance window or Apply immediately.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-149":{"article":"Check whether Amazon Relational Database Service instances are not publicly accessible. The rule is NON_COMPLIANT if the publicly accessible field is true in the instance configuration item.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["DBInstanceArn"],"remediation":"Modifying an existing DB instance:\n1. Log in to the AWS management console and navigate to the RDS dashboard at https://console.aws.amazon.com/rds/.\n2. Under the navigation panel, on RDS Dashboard, click 'Databases'.\n3. Select the RDS instance that you want to update.\n4. Click 'Modify' from the dashboard top menu.\n5. On the 'Modify DB Instance panel', under the 'Connectivity' section, click on 'Additional connectivity configuration' and update the value for 'Publicly Accessible' to Not publicly accessible to restrict public access. \nFollow the below steps to update subnet configurations:\n  5.1 Select the 'Connectivity and security' tab, and click on the VPC attribute value inside the 'Networking' section.\n  5.2 Select the 'Details' tab from the VPC dashboard bottom panel and click on Route table configuration attribute value.\n  5.3 On the Route table details page, select the 'Routes' tab from the dashboard bottom panel and click on 'Edit routes'.\n  5.4 On the Edit routes page, update the Destination of Target which is set to 'igw-xxxxx' and click on 'Save routes'.\n6. On the 'Modify DB Instance' panel click on 'Continue' and in the 'Scheduling of modifications' section, perform one of the following actions based on your requirements:\n  - Select 'Apply during the next scheduled maintenance window' to apply the changes automatically during the next scheduled maintenance window.\n  - Select 'Apply immediately to apply the changes right away'. With this option, any pending modifications will be asynchronously applied as soon as possible, regardless of the maintenance window setting for this RDS database instance. \n  Note that any changes available in the pending modifications queue are also applied. If any of the pending modifications require downtime, choosing this option can cause unexpected downtime for the application.\n7. Repeat steps 3 to 6 for each RDS instance available in the current region.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-152":{"article":"Enabling connection draining on Classic Load Balancers ensures that the load balancer stops sending requests to instances that are de-registering or unhealthy. It keeps the existing connections open. This is particularly useful for instances in Auto Scaling groups to ensure that connections aren't severed abruptly.","impact":"Without connection draining, Classic Load Balancers cannot send requests to instances that are de-registering or unhealthy.","report_fields":["LoadBalancerArn"],"remediation":"From Console:\n1. Login to the AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, under LOAD BALANCING, choose Load Balancers. \n3. Select your load balancer. \n4. On the 'Instances' tab, for 'Connection Draining', choose 'Edit'. \n5. On the 'Configure Connection Draining' page, select 'Enable Connection Draining'. \n6. (Optional) For 'Timeout', type a value between 1 and 3,600 seconds. \n7. Choose 'Save'. \n\nFrom AWS CLI:\naws elb modify-load-balancer-attributes --load-balancer-name my-loadbalancer --load-balancer-attributes \"{{\\\"ConnectionDraining\\\"{{\\\"Enabled\\\"true,\\\"Timeout\\\"300}}}}\"","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-115":{"article":"Checks for expired certificates in the AWS Certificate Manager. It is recommended to delete expired certificates.","impact":"Removing expired AWS ACM certificates eliminates the risk of deploying an invalid SSL/TLS certificate in resources which may trigger an error in the front end and cause a loss of credibility for the web application/website.","report_fields":["CertificateArn","DomainName"],"remediation":"1. Open the AWS console. \n2. On the console, select the specific region.\n3. Navigate to the Certificate Manager(ACM) service. \n4. Select the certificate that was reported. \n5. Verify that the 'Status' column shows 'Expired' for the reported certificate. \n6. Under the 'Actions' drop-down list, click on 'Delete'.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-364":{"article":"A public IPv4 address is an IP address that is reachable from the internet. If you create launch configuration with a public IP address, then your EC2 instance is reachable from the internet.\nA private IPv4 address is an IP address that is not reachable from the internet. You can use private IPv4 addresses for communication between EC2 instances in the same VPC or in your connected private network.","impact":"Instances with a public IP address could potentially be compromised, and an attacker could gain anonymous access to the instance and other resources connected to this instance. This can lead to malicious activity that affects sensitive data.","report_fields":["LaunchConfigurationName"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo disable automatic ip association, create new launch configuration:\n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2/.\n2. Under Auto Scaling, click on the Launch Configurations.\n3. Click on the Create launch configuration.\n4. Additional configuration, click on the Advanced details.\n5. Under IP address type, choose 'Do not assign a public IP address to any instances'.\n6. Create configuration.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-137":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements or compliance standards. It is highly recommended that access to data be restricted to encrypted protocols. \nThis rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/.\n2. Go to Load Balancers.\n3. Click on the reported Load Balancer.\n4. In the 'Description' tab, under 'Security' select security group to edit.\n5. In the 'Inbound rules' tab, click 'Edit inbound rules'.\n6. Modify rules.\n7. Click on 'Save rules'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-253":{"article":"Ensure that your Amazon Glue Data Catalogs are using KMS Customer Master Keys (CMKs) instead of AWS managed-keys in order to have a more granular control over data-at-rest encryption/decryption process and meet compliance requirements.","impact":"Without AWS KMS Customer Master Keys (CMKs), you do not have full and granular control over who can use the encryption keys to access AWS Glue data.","report_fields":["CatalogId"],"remediation":"1. Sign in to the AWS Management Console and open the AWS Glue console at https://console.aws.amazon.com/glue/.\n2. Choose Settings in the navigation pane.\n3. On the Data catalog settings page, select Metadata encryption, and choose an AWS KMS Customer Master Key.","multiregional":false,"service":"AWS Glue"},"ecc-aws-246":{"article":"If default route table association option is enabled, all Transit Gateway attachments are automatically added to default route table.\nTo manage the VPC environment and the transit gateway, it is preferable to manually establish the table association for the transit gateway.","impact":"With the Default Route Table Association enabled, you have no control over which Transit Gateway Route Table you can associate Transit Gateway with, and without it you can not implement custom routing rules and routing requirements.","report_fields":["TransitGatewayArn"],"remediation":"Perform the following steps in order to set 'Default route table association' to disable:\n1. Sign in to the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n2. Choose Transit Gateways.\n3. Choose relevant gateway and click Actions and select Modify.\n4. Uncheck 'Default route table association'.\n5. Update route table with the necessary routes.","multiregional":false,"service":"AWS Transit Gateway"},"ecc-aws-094":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for changes to S3 bucket policies.","impact":"Lack of monitoring and logging of S3 Bucket changes can result in insufficient response time to detect accidental or intentional modifications that may lead to unauthorized data access or other security breaches.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventSource = s3.amazonaws.com) && (($.eventName = PutBucketAcl) || ($.eventName = PutBucketPolicy) || ($.eventName = PutBucketCors) || ($.eventName = PutBucketLifecycle) || ($.eventName = PutBucketReplication) || ($.eventName = DeleteBucketPolicy) || ($.eventName = DeleteBucketCors) || ($.eventName = DeleteBucketLifecycle) || ($.eventName = DeleteBucketReplication))}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-201":{"article":"Enabling instance deletion protection is an additional layer of protection against accidental database deletion or deletion by an unauthorized entity. \nWhile deletion protection is enabled, an RDS DB instance cannot be deleted. Before a deletion request can succeed, deletion protection must be disabled.","impact":"Accidentally deleted or deleted by an unauthorized entity, the RDS instance can result in data loss.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases, then choose the DB instance that you want to modify. \n3. Choose Modify. \n4. Under Deletion protection, choose Enable deletion protection.\n5. Choose Continue. \n6. Under Scheduling of modifications, choose when to apply modifications. The options are Apply during the next scheduled maintenance window or Apply immediately. \n7. Choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-260":{"article":"Ensure that all Amazon EMR cluster log files are periodically archived and uploaded to S3 in order to keep the logging data for historical purposes or to track and analyze the EMR clusters behavior for a long period of time.\nThis ensures that the log files are available after the cluster terminates, whether this is through normal shut down or due to an error. Amazon EMR archives the log files to Amazon S3 at 5 minute intervals.","impact":"Without periodically archiving and uploading cluster log files to S3, there is a possibility to lose logging data. Because these logs exist on the master node, when the node terminates either because the cluster was shut down or because an error occurred, these log files are no longer available.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\n1. Login to the AWS Management Console.\n2. Navigate to EMR dashboard at https://console.aws.amazon.com/elasticmapreduce/.\n3. In the navigation panel, under Amazon EMR, click Clusters to access your AWS EMR clusters page.\n4. Select the EMR cluster that you want to relaunch then click on the Clone button from the dashboard top menu.\n5. Inside the Cloning <your-cluster-ID> dialog box, choose 'Yes' to include the steps from the original cluster in the cloned cluster or 'No' to clone the original cluster's configuration without including any of the existing steps. Click Clone to start the cloning process.\n6. On the Create Cluster page, select Step 3: General Cluster Settings from the left navigation panel to access the cloned cluster general settings.\n7. On the General Options panel, under Cluster Name, click on the Logging checkbox to enable the feature. Once enabled, the EMR dashboard will display the S3 default folder path where the cluster log files will be uploaded automatically.\n8. Click the Next button without changing any other configuration options.\n9. On the Security Options panel, click Create Cluster to create your new (cloned) AWS EMR cluster.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-462":{"article":"Lambda functions in a single AWS account in one Region share the concurrency limit. If one function exceeds the concurrent limit, this prevents other functions from being invoked by the Lambda service. You can set reserved concurrency for Lambda functions to ensure that they can be invoked even if the overall capacity has been exhausted.","impact":"If reserved concurrency is disabled and if the overall capacity has been exhausted Lambda functions cannot be invoked.","report_fields":["FunctionArn"],"remediation":"To turn on active tracing:\n1. Open the AWS Lambda console at https://console.aws.amazon.com/lambda/\n2. Navigate to Functions and then select your Lambda function.\n3. Choose 'Configuration' and then choose 'Concurrency'.\n4. Under Concurrency, choose Edit.\n5. Choose Reserve concurrency. Enter the amount of concurrency to reserve for the function.\n6. Choose Save.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-045":{"article":"Password policies, in part, enforce password complexity requirements. Use IAM password policies to ensure that passwords use different character sets. We recommend that the password policy require at least one uppercase letter. \nSetting a password complexity policy increases account resiliency against brute force login attempts.","impact":"Not using a password policy with at least one uppercase letter reduces security and account resiliency against brute-force attacks.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings).\n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings on the Left Pane.\n4. Check 'Requires at least one uppercase letter'.\n5. Click on 'Apply password policy'.","multiregional":true,"service":"AWS Account"},"ecc-aws-316":{"article":"The 'global_names' parameter specifies whether a database link is required to have the same name as the database to which it connects.","impact":"Not requiring database connections to match the domain that is being called remotely could allow unauthorized domain sources to potentially connect via brute-force tactics.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"global_names\".\n6. Choose TRUE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-133":{"article":"Amazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious or unauthorized behavior in order to protect them. It monitors accounts for such activites as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers.","impact":"With a disabled GuardDuty service, you lose the opportunity to receive intelligent and centralized threat detection without additional security software or infrastructure to deploy and maintain.","report_fields":["account_id","account_name"],"remediation":"For a standalone account environment, perform the following steps: \n1. Open the GuardDuty console at https//console.aws.amazon.com/guardduty/\n2. Choose Get Started. \n3. Choose Enable GuardDuty. \nFor a multi-account environment, perform the following steps:\n1. Log in to the AWS Organizations management account \n2. Open the GuardDuty console at https://console.aws.amazon.com/guardduty/. \nIf GuardDuty is not already enabled, select Get Started and then designate a GuardDuty delegated administrator on the Welcome to GuardDuty page. \nIf GuardDuty is enabled, designate a GuardDuty delegated administrator on the Settings page. \n3. Enter the twelve-digit AWS account ID of the account that you want to designate as the GuardDuty delegated administrator for the organization and choose Delegate. \n4. To add member accounts, choose Settings in the Navigation pane and then choose Accounts. The accounts table displays all the accounts in the organization. \n5. Choose the accounts that you want to add as members by checking the box next to the account ID. Then select Add member in the Action menu.","multiregional":false,"service":"AWS Account"},"ecc-aws-145":{"article":"This control applies only to AWS Organizations management account.\nReal-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for AWS Organizations changes made in the master AWS Account.\nMonitoring AWS Organizations changes can help you prevent any unwanted, accidental or intentional modifications that may lead to unauthorized access or other security breaches. This monitoring technique helps you to ensure that any unexpected changes performed within your AWS Organizations can be investigated and any unwanted changes can be rolled back.","impact":"Lack of monitoring and logging of Organizations changes calls can lead to insufficient response time to accidental or intentional changes. Overlooking this can result in unauthorized access or other security breaches.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<organizations_changes>` --metric-transformations metricName=`<organizations_changes>`,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{ ($.eventSource = organizations.amazonaws.com) && (($.eventName = \"AcceptHandshake\") || ($.eventName = \"AttachPolicy\") || ($.eventName = \"CreateAccount\") || ($.eventName = \"CreateOrganizationalUnit\") || ($.eventName = \"CreatePolicy\") || ($.eventName = \"DeclineHandshake\") || ($.eventName = \"DeleteOrganization\") || ($.eventName = \"DeleteOrganizationalUnit\") || ($.eventName = \"DeletePolicy\") || ($.eventName = \"DetachPolicy\") || ($.eventName = \"DisablePolicyType\") || ($.eventName = \"EnablePolicyType\") || ($.eventName = \"InviteAccountToOrganization\") || ($.eventName = \"LeaveOrganization\") || ($.eventName = \"MoveAccount\") || ($.eventName = \"RemoveAccountFromOrganization\") || ($.eventName = \"UpdatePolicy\") || ($.eventName = \"UpdateOrganizationalUnit\")) }'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<organizations_changes>` --metric-name `<organizations_changes>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-262":{"article":"If you specified that acceptance is required for connection requests, you must manually accept or reject endpoint connection requests to your endpoint service. Otherwise, endpoint connections are automatically accepted.","impact":"When manual acceptance is disabled it is possible that someone accidentally or on purpose connect an unnecessary service to endpoint and expose sensitive information.","report_fields":["ServiceId"],"remediation":"1. Sign in to the AWS Management Console and open the VPC console at https://console.aws.amazon.com/vpc.\n2. In the navigation pane, choose Endpoint Services. \n3. Choose the name of the intended enpoint service. \n4. Choose the Actions and then choose Modify endpoint acceptance setting.\n5. Enable Acceptance required.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-158":{"article":"Identification and inventory of your IT assets is a crucial aspect of governance and security. You need to have visibility of all your RDS DB instances so that you can assess their security posture and take action on potential areas of weakness. \nSnapshots should be tagged in the same way as their parent RDS database instances. Enabling this setting ensures that snapshots inherit the tags of their parent database instances.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Databases. \n3. Select the DB instance to modify. \n4. Choose Modify. \n5. Under Backup, select Copy tags to snapshots. \n6. Choose Continue. \n7. Under Scheduling of modifications, choose when to apply modifications. You can choose either Apply during the next scheduled maintenance window or Apply immediately.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-092":{"article":"One or more AMIs are exposed to public internet. It is recommended that you do not publicly share your AMI images with other AWS accounts to avoid sensitive data exposure. If required, AMI images should only be shared with relevant AWS accounts without making them public.","impact":"Publicly shared AMIs with the other AWS accounts may lead to sensitive data exposure. If required, AMI images should only be shared with relevant AWS accounts without making them public.","report_fields":["ImageId","OwnerId"],"remediation":"1. Navigate to the EC2 dashboard at https://console.aws.amazon.com/ec2/.\n2. In the left navigation pane, select IMAGES --> AMIs. \n3. Select the relevant image. \n4. Select the Permissions tab (dashboard bottom panel) and check the current AMI permissions. If the selected image is public, the following status will be displayed on the EC2 dashboard 'This image is currently Public.'. \n5. Repeat steps 3-4 to verify the permissions for the rest of the AMIs available in the current region. \n6. Change the AWS region from the navigation bar and repeat steps 1-5 for the all the regions.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-403":{"article":"This policy identifies the ECS clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["clusterArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon ECS at https://console.aws.amazon.com/ecs.\n2. Click on the required ECS cluster.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-528":{"article":"Certificate Transparency logging guards against SSL/TLS certificates issued by mistake or by a compromised certificate authority. Most modern browsers require that public certificates issued for your domain be recorded in a certificate transparency log.","impact":"Browsers no longer trust SSL/TLS certificates that are not recorded in a certificate transparency log.","report_fields":["CertificateArn","DomainName"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nYou can't use the console to enable transparency logging.\n\nUse AWS CLI to enable transparency logging:\n'aws acm request-certificate \\\n    --domain-name www.example.com \\\n    --validation-method DNS \\\n    --options CertificateTransparencyLoggingPreference=ENABLED \\'","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-222":{"article":"Systems Manager is an AWS service that you can use to view and control your AWS infrastructure. To help you to maintain security and compliance, Systems Manager scans your stopped and running managed instances. A managed instance is a machine that is configured for use with Systems Manager. Systems Manager then reports or takes corrective action on any policy violations that it detects. \nSystems Manager also helps you to configure and maintain your managed instances.","impact":"If System Manager is disabled, you will not be able to track policy violations it detects through EC2 instances or take corrective action.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Open the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager/.\n2. In the navigation menu, choose Quick setup. \n3. Choose Create. \n4. Under Configuration type, choose Host Management, then choose Next. \n5. On the configuration screen, you can keep the default options. You can optionally make the following changes: \n5.1. If you use CloudWatch to monitor EC2 instances, select Install and configure the CloudWatch agent and Update the CloudWatch agent once every 30 days. \n5.2. Under Targets, choose the management scope to determine the accounts and Regions where this configuration is applied. \n5.3. Under Instance profile options, select Add required IAM policies to existing instance profiles attached to your instances. \n6. Choose Create.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-446":{"article":"Amazon MWAA can send Apache Airflow logs to Amazon CloudWatch. You can view logs for multiple environments from a single location to easily identify Apache Airflow task delays or workflow errors without the need for additional third-party tools.","impact":"If 'Task logs' is not enabled and the 'log_level' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Click \"Next\".\n5. Enable 'Airflow Task logs'.\n6. Choose required log level.\n7. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-209":{"article":"Amazon Aurora-PostgreSQL database engine logs (Postgresql, Upgrade) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"To create a custom DB parameter group\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo enable and publish Aurora-PostgreSQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- log_statement=all\n- log_min_duration_statement=minimum query duration (ms) to log\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.\n\nTo publish PostgreSQL logs to CloudWatch Logs from the AWS Management Console\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose all of the log files to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-223":{"article":"A State Manager association is a configuration that is assigned to your managed instances. The configuration defines the state that you want to maintain on your instances. For example, an association can specify that antivirus software must be installed and running on your instances or that certain ports must be closed. \nAfter you create one or more State Manager associations, compliance status information is immediately available to you. You can view the compliance status in the console or in response to AWS CLI commands or corresponding Systems Manager API actions. For associations, Configuration Compliance shows the compliance status (Compliant or Non-compliant). It also shows the severity level assigned to the association, such as Critical or Medium.","impact":"If AWS Systems Manager association compliance status is \"NON_COMPLIANT\", it means the task has not been completed, and this needs to be fixed.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Open the AWS Systems Manager console at https://console.aws.amazon.com/systems-manager/.\n2. In the navigation pane, under Node Management, choose Fleet Manager.\n3. Choose the instance ID that has an Association status of Failed.\n4. Choose View details.\n5. Choose Associations.\n6. Note the name of the association that has an Association status of Failed. This is the association that you need to investigate. You need to use the association name in the next step.\n7. In the navigation pane, under Node Management, choose State Manager. Search for the association name, then select the association.\n8. After you determine the issue, edit the failed association to correct the problem. For information on how to edit an association, see 'Edit an association': https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-state-assoc-edit.html.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-055":{"article":"AWS CloudTrail is a web service that records AWS API calls for a given AWS account. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, the request parameters, and the response elements returned by the AWS service. CloudTrail uses Amazon S3 for log file storage and delivery, so log files are stored durably. \nIn addition to capturing CloudTrail logs within a specified S3 bucket for long term analysis, realtime analysis can be performed by configuring CloudTrail to send logs to CloudWatch Logs. For a trail that is enabled in all regions in an account, CloudTrail sends log files from all those regions to a CloudWatch Logs log group. \nIt is recommended that CloudTrail logs be sent to CloudWatch Logs. Note: The recommendation intends to ensure the AWS account activity is being captured, monitored, and appropriately alarmed on. CloudWatch Logs is a native way to accomplish this using AWS services, though it does not preclude the use of an alternate solution.","impact":"Failure to integrate CloudTrail trails with CloudWatch logs can lead to insufficient response time to detect anomalous, accidental, or intentional account activity. It can result in unrestricted or unauthorized access.","report_fields":["TrailARN"],"remediation":"1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/. \n2. Under All Buckets, click on the target bucket you wish to evaluate. \n3. Click on Properties on the top right of the console. \n4. Click on Trails in the left menu. \n5. Click on each trail where no CloudWatch Logs are defined. \n6. Go to the CloudWatch Logs section and click on Configure. \n7. Define a new or select an existing log group. \n8. Click Continue.\n9. Configure IAM Role which will deliver CloudTrail events to CloudWatch Logs:\n- Create/Select an IAM Role and Policy Name;\n- Click Allow to continue.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-015":{"article":"The root account is the most privileged user in an AWS account. MFA adds an extra layer of protection on top of a username and password. With MFA enabled, when a user signs in to an AWS website, they will be prompted for their username and password as well as for an authentication code from their AWS MFA device. \nNote: When virtual MFA is used for root accounts, it is recommended that the device used is NOT a personal device, but rather a dedicated mobile device (tablet or phone) that is managed to be kept charged and secured independent of any individual personal devices. ('non-personal virtual MFA') This lessens the risks of losing access to MFA due to the device loss, device trade-in or if the individual owning the device is no longer employed by the company.","impact":"One layer of protection (a password) for a root account is not enough to prevent credentials from being compromised. The likelihood of a password being cracked and an account being compromised is much higher without MFA enabled.","report_fields":["account_id","account_name"],"remediation":"Perform the following to establish MFA for the root account: \n1. Sign in to the AWS Management Console and open the IAM console. \n2. Choose Dashboard, and under Security Status, expand Activate MFA on your root account. \n3. Choose Activate MFA. \n4. In the wizard, choose a virtual MFA device and then choose Next Step. \n5. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the 'secret configuration key' that is available for manual entry on devices that do not support QR codes.\n6. Open your virtual MFA application. If the virtual MFA application supports multiple accounts, choose the option to create a new account. \n7. Determine whether the MFA app supports QR codes and then do one of the following: \n- Use the app to scan the QR code. For example, you might choose the camera icon or choose an option similar to Scan code and then use the device's camera to scan the code. \n- In the Manage MFA Device wizard, choose Show secret key for manual configuration and then type the secret configuration key into your MFA application. When finished, the virtual MFA device starts generating one-time passwords. \n7.1. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the one-time password that currently appears in the virtual MFA device. \nWait up to 30 seconds for the device to generate a new one-time password. \nThen type the second one-time password in the Authentication Code 2 box. \nChoose Active Virtual MFA.","multiregional":true,"service":"AWS Account"},"ecc-aws-191":{"article":"Including EFS file systems in the backup plans helps you to protect your data from deletion and data loss.","impact":"Without a backup plan, it is impossible to quickly recover data in the event of failure.","report_fields":["FileSystemArn","OwnerId"],"remediation":"1. Open the Amazon Elastic File System console at https//console.aws.amazon.com/efs/.\n2. On the File systems page, choose the file system for which to enable automatic backups. The File system details page is displayed. \n3. Under General, choose Edit.\n4. To enable automatic backups, select Enable automatic backups. \n5. Choose Save changes.","multiregional":false,"service":"Amazon Elastic File System"},"ecc-aws-059":{"article":"This control is non-compliant if AWS Config recorder does not record all resources, or does not include global resources, or if the recording is disabled or has the status 'FAILURE'.\nAWS Config is a web service that performs configuration management of supported AWS resources within your account and delivers log files to you. The recorded information includes the configuration item (AWS resource), relationships between configuration items (AWS resources), any configuration changes between resources. It is recommended that AWS Config be enabled in all regions.\nTo exercise better governance over your resource configurations and to detect resource misconfigurations, you need fine-grained visibility into what resources exist and how these resources are configured at any time. You can use AWS Config to notify you whenever resources are created, modified, or deleted without having to monitor these changes by polling the calls made to each resource. \nWhen you use multiple AWS resources that depend on one another, a change in the configuration of one resource might have unintended consequences on related resources. With AWS Config, you can view how the resource you intend to modify is related to other resources and assess the impact of your change. You can also use the historical configurations of your resources provided by AWS Config to troubleshoot issues and to access the last known good configuration of a problem resource.","impact":"Failure to audit and evaluate AWS resource configurations using AWS Config can result in a loss of ability to automate the evaluation of recorded configurations against desired configurations. \nAdditionally, this can lead to increased complexity in security analysis, change management, compliance auditing, and operational troubleshooting.","report_fields":["account_id","account_name"],"remediation":"1. Sign in to the AWS Management Console and open the AWS Config console at https://console.aws.amazon.com/config/.\n2. On the top right of the console select target Region.\n3. If presented with 'Setup AWS Config' - follow remediation procedure:\n4. For 'Resource types to record' select 'Record all resources supported in this region'.\n  4.1 Click on 'Get started'.\n  4.2 For 'Resource types to record' select 'Record all resources supported in this region'.\n  4.3 Choose 'Include global resources (IAM resources)'.\n  4.4 Specify an S3 bucket either in the same account or in another managed AWS account.\n  4.5 (Optional) Create a SNS Topic either from  the same AWS account or from another managed AWS account.\n  4.6 Click 'Next'. \n  4.7 (Optional) On the 'Rules' page, you can specify the rules that you want AWS Config to use to evaluate compliance information for the recorded resource types.\n  4.8 Click 'Next'. Review your AWS Config setup details. You can go back to edit changes for each section. \n  4.9 Choose 'Confirm' to finish setting up AWS Config.\n5. If AWS Config already enabled, check the following:\n  5.1 Navigate to 'Settings' tab on the left panel and click 'Edit'\n  5.2 On the 'Edit settings' select 'Enable recording' if it is not enabled yet.\n  5.3 Make sure that 'Record all resources supported in this region' selected for 'Resource types to record'.\n  5.3 Make sure that 'Include global resources (IAM resources)' selected.\n  5.4 Click on 'Save'.\n6. Check that AWS Config recorder does not have status 'FAILURE'. You can check it using AWS CLI command: \"aws configservice describe-configuration-recorder-status\". If status is 'lastStatus' is 'FAILURE', look at values of 'lastErrorCode' and 'lastErrorMessage' parameters for information about the type of problem.\nFollow this AWS Guide on how to troubleshoot AWS Config console error messages: https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/.","multiregional":true,"service":"AWS Config"},"ecc-aws-289":{"article":"Ensure AutoScaling Groups do not have configuration issues which could prevent the ASG from functioning properly or launching instances.\nThe following AutoScaling Group items are checked:\n  - invalid subnets\n  - invalid security groups\n  - invalid key pair name\n  - invalid launch config volume snapshots\n  - invalid AMIs\n  - invalid ELB health check","impact":"If an AutoScaling Group has an invalid configuration, the AutoScaling Group will not function properly or be able to launch instances.\nWhen AutoScaling Groups fail to launch new EC2 instances due to invalid configuration, the scaling mechanism is unable to add compute resources to handle the increased traffic load and this will cause a significant negative impact on performance and can lead to downtime.","report_fields":["AutoScalingGroupARN"],"remediation":"To retrieve an error message from the description of scaling activities, use the 'aws autoscaling describe-scaling-activities' command. \nUse the following link to find a solution for an issue: https://docs.aws.amazon.com/autoscaling/ec2/userguide/CHAP_Troubleshooting.html.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-409":{"article":"This policy identifies the EMR clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ClusterArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EMR at https://console.aws.amazon.com/elasticmapreduce.\n2. Click on the required cluster.\n3. Find Tags and click on the 'Edit'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-268":{"article":"ElastiCache for Redis supports symmetric customer managed AWS KMS keys (KMS key) for encryption at rest. \nCustomer-managed KMS keys are encryption keys that you create, own and manage in your AWS account.","impact":"Without KMS CMK customer-managed keys, you do not have full and granular control over who can access encrypted Elasticache data.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nAWS ElastiCache Redis cluster at-rest encryption can be set only at the time of the cluster creation. To fix this issue, create a new cluster with at-rest encryption, migrate all the required ElastiCache Redis cluster data from the unencrypted cluster to the new one, and then delete the old cluster.\nEnabling at-rest encryption using KMS CMK on an existing ElastiCache Redis cluster:\n1. Create a manual backup of the cluster:\n  1.1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  1.2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n  1.3. Choose 'Redis clusters' from the navigation pane.\n  1.4. Choose the cluster and choose 'Action', and then 'Backup'.\n  1.5. Make sure the cluster name is right and enter backup name. \n  1.6. Select 'Encryption key'. \n  1.7. Choose 'Create Backup'.\n2. Create a new cluster by restoring from the backup:\n  2.1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2.2. Choose 'Redis clusters' from the navigation pane.\n  2.3. Select 'Create Redis cluster'.\n  2.4. For 'Choose a cluster creation method', choose 'Restore from backups'.\n  2.5. For 'Source' select 'Amazon ElastiCache backups'. And select backup created on the previous step.\n  2.6. Complete other settings and click 'Next'.\n  2.7. At the 'Security' section, enable 'Encryption at rest'. And select a KMS CMK key that will be used to protect the key used to encrypt data at rest for this cluster.\n  2.8. Complete other settings and click 'Next'.\n  2.9. Review settings and click 'Create'.\n3. Update the endpoints in your application to the new cluster's endpoints.\n4. Delete the old cluster.\n\nTo create a new ElastiCache Redis cluster with at-rest encryption set using KMS CMK, perform the following:\n1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n3. Choose 'Redis clusters' from the navigation pane.\n5. Click on the 'Create Redis cluster' button.\n6. Complete 'Cluster settings' page and click 'Next'.\n7. At the 'Security' section, enable 'Encryption at rest'. And select a KMS CMK key that will be used to protect the key used to encrypt data at rest for this cluster.\n8. Click on 'Create' button to launch your new ElastiCache Redis cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-334":{"article":"Server-Side Encryption (SSE) for Amazon Kinesis Firehose delivery streams helps you meet these security requirements by providing an extra layer of protection for your Kinesis data-at-rest. \nThe data can be encrypted with either AWS KMS default keys or KMS Customer Master Keys (CMKs). Use customer-managed CMKs for your AWS services whenever possible to have full control over all operations related to CMK permissions, management, and lifecycle.\nThis control checks only Amazon Kinesis Firehose delivery streams that have Source type \"Direct PUT\". Because when you configure a Kinesis data stream as the data source of a Kinesis Data Firehose delivery stream, Kinesis Data Firehose no longer stores the data at rest. Instead, the data is stored in the data stream.","impact":"Disabled SSE encryption allows a user to get unauthorized access to sensitive data in Amazon Kinesis Firehose delivery streams.","report_fields":["DeliveryStreamARN"],"remediation":"To enable server-side encryption for Amazon Kinesis Data Firehose service:\n1. Navigate to the Amazon Kinesis Data Firehose dashboard at https://console.aws.amazon.com/firehose/.\n2. Choose the delivery stream that you want to reconfigure, then click on its name to access the resource configuration.\n3. Select the 'Configuration' tab and scroll down to 'Server-side encryption (SSE)' section and click the 'Edit' button.\n4. Tick the box 'Enable server-side encryption for source records in delivery stream'.\n5. Select the 'Encryption type'. Select the default AWS owned CMK key or an AWS KMS Customer Master Key (CMK). It is recommended to use AWS KMS CMK.\n6. Click 'Save changes' to apply the configuration changes.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-105":{"article":"Rotate the keys of your Kinesis Streams in order to protect your data and metadata from breach or unauthorized access and fulfill compliance requirements for key management within your organization.","impact":"Not rotated Kinesis Streams Keys can lead to breaches or unauthorized access and not fulfill compliance requirements for key management within your organization.","report_fields":["StreamARN"],"remediation":"1. Navigate to the KMS dashboard at https://console.aws.amazon.com/kms/.\n2. In the left navigation pane, click on the Customer managed keys. \n3. Select the alias of the CMK that you need to check under the Alias column. \n4. In the Key Rotation section, enable the 'Rotate this key every year' checkbox. Note: AWS managed keys are automatically rotated every 3 years.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-461":{"article":"When lambda functions using the latest version of the implemented runtime environment, functions benefit from new features and enhancements, better security, performance and reliability","impact":"Without keeping the Lambda functions runtime up-to-date, it is possible to miss out security patches or other updates. And eventually it will be impossible to update the function.","report_fields":["FunctionArn"],"remediation":"Use AWS CLI to update lambda runtime version:  \naws lambda update-function-configuration --function-name \"function_name\" --runtime \"runtime_version\"","multiregional":false,"service":"AWS Lambda"},"ecc-aws-157":{"article":"Identification and inventory of your IT assets is a crucial aspect of governance and security. You need to have visibility of all your RDS DB clusters so that you can assess their security posture and take action on potential areas of weakness.\nSnapshots should be tagged in the same way as their parent RDS database clusters. Enabling this setting ensures that snapshots inherit the tags of their parent database clusters.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["DBClusterArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Databases. \n3. Select the DB cluster to modify. \n4. Choose Modify. \n5. Under Backup, select Copy tags to snapshots.\n6. Choose Continue.\n7. Under Scheduling of modifications, choose when to apply modifications. You can choose either Apply during the next scheduled maintenance window or Apply immediately.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-540":{"article":"The AWS Glue version determines the versions of Apache Spark and Python that AWS Glue supports. The Python version indicates the version that's supported for jobs of type Spark","impact":"Without keeping the Glue up-to-date, it is possible to miss out on new features, bug fixes, security patches, and performance improvements.","report_fields":["Name"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Glue at https://console.aws.amazon.com/glue.\n2. Under the 'AWS Glue Studio' click on the 'Jobs'.\n3. Click on the required job.\n4. Open 'Job details' and click on the 'Glue version'.\n5. Choose the latest version.\n6. Save.","multiregional":false,"service":"AWS Glue"},"ecc-aws-020":{"article":"A good cloud provisioning strategy should include tagging of resources. Tags may be used to associate function, owner, environment, or other attributes with application instances, and a consistent tagging strategy is a recommended best practice.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Sign in to the AWS Management Console and open the AWS Config console at https://console.aws.amazon.com/config/. \n2. Choose Rules on the left and then, on the Rules page, choose Add Rule to add new rules to the rule list. For existing rules, select the noncompliant rule from the rule list and choose Edit. \n3. On the Rules page, go to Edit Name and, in the Choose remediation action section, choose the appropriate remediation action from the recommended list. Depending on the selected remediation action, you can see either specific parameters or no parameters. \n4. (Optional) If you want to pass the resource ID of noncompliant resources to the remediation action, choose the Resource ID parameter. Once selected, during runtime, this parameter is substituted with the ID of the resource to be remediated. Each parameter has either a static value or a dynamic one. If you do not choose a specific resource ID parameter from the drop-down list, you can enter values for each key. If you choose a resource ID parameter from the drop-down list, you can enter values for all the other keys except the selected resource ID parameter.\n5. Choose Save. The Rules page is displayed.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-028":{"article":"This policy identifies security group rules that allow inbound traffic to the DNS port (53) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted DNS access can increase opportunities for malicious activity such as Denial of Service (DoS) attacks or Distributed Denial of Service (DDoS) attacks. \nAlso, DNS can be used by attackers as one of their reconnaissance techniques.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-546":{"article":"Retain audit logs across enterprise assets for a minimum of 90 days.\nRetaining historical audit logs is necessary because compromises often go unnoticed for significant lengths of time.\nBy having three months of logs immediately available, an entity can quickly identify and minimize impact of a data breach.","impact":"Without formal processes to detect exceptions and anomalies, the entity may be unaware of unauthorized and potentially malicious activities occurring within their network.\nHaving centrally stored log history allows investigators to better determine the length of time a potential breach was occurring, and the possible system(s) impacted.","report_fields":["StreamARN"],"remediation":"To update a data stream using the console:\n1. Open the Amazon Kinesis console at https://console.aws.amazon.com/kinesis/.\n2. In the navigation bar, expand the Region selector and choose a Region.\n3. Choose the name of your stream in the list and open Configuration tab.\n4. To edit the data retention period, choose Edit in the Data retention period section, and then enter a new data retention period.\n\nTo update a data stream using the CLI:\n1. Use the following command to increase the stream retention period. Replace the --stream-name with the name of the stream and --retention-period-hours according to your organization policy.\n  aws kinesis increase-stream-retention-period --stream-name samplestream --retention-period-hours 2160\n\nThis command produces no output. The retention period (the length of time data records are accessible after they are added to the stream) of the specified stream will be changed to 2160 hours.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-287":{"article":"To take advantage of the safety and reliability of geographic redundancy, span Auto Scaling group across multiple Availability Zones within a Region.\nWhen one Availability Zone becomes unhealthy or unavailable, Amazon EC2 Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Amazon EC2 Auto Scaling automatically redistributes the application instances evenly across all the Availability Zones for your Auto Scaling group.","impact":"When an Auto Scaling group (ASG) is not configured for multiple Availability Zones, in case of any issue with the Availability Zone, it will not be reachable. This deteriorates the availability and reliability of the ASG.","report_fields":["AutoScalingGroupARN"],"remediation":"Use the following procedure to expand Auto Scaling group to a subnet in an additional Availability Zone. \n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2autoscaling/.\n2. Select the Auto Scaling group that you want to modify.\n3. On the Details tab, choose Network, Edit.\n4. Choose the subnets corresponding to the Availability Zone that you want to add to the Auto Scaling group. Make sure that at least two subnets are selected.\n5. Choose Update.\n6. To update the Availability Zones for your load balancer so that it shares the same Availability Zones as your Auto Scaling group, complete the following steps:\n  a. On the navigation pane, under LOAD BALANCING, choose Load Balancers.\n  b. Choose your load balancer.\n  c. Do one of the following:\n    For Application Load Balancers and Network Load Balancers:\n      i. On the Description tab, for Availability Zones, choose Edit subnets.\n      ii. On the Edit subnets page, for Availability Zones, select the check box for the Availability Zone to add. If there is only one subnet for that zone, it is selected. If there is more than one subnet for that zone, select one of the subnets.\n    For Classic Load Balancers in a VPC:\n      i. On the Instances tab, choose Edit Availability Zones.\n      ii. On the Add and Remove Subnets page, for Available subnets, select the subnet using its add (+) icon. The subnet is moved under Selected subnets.\n    For Classic Load Balancers in EC2-Classic:\n      i. On the Instances tab, choose Edit Availability Zones.\n      ii. On the Add and Remove Availability Zones page, choose the Availability Zone to add.\n  d. Choose Save.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-056":{"article":"AWS console defaults to no check boxes selected when creating a new IAM user. When creating the IAM User credentials you have to determine what type of access they require. \nProgrammatic access: The IAM user might need to make API calls, use the AWS CLI, or use the Tools for Windows PowerShell. In that case, create an access key (access key ID and a secret access key) for that user. \nAWS Management Console access: If the user needs to access the AWS Management Console, create a password for the user.","impact":"Creating access keys at the same time as the user account, rather than when needed, increases the attack surface of access keys that can remain unused and lead to compromise. Additionally, unnecessary AWS IAM access keys generate unnecessary management work in auditing and rotating IAM credentials.","report_fields":["Arn"],"remediation":"1. Login to the AWS Management Console.\n2. Click on Services.\n3. Click on IAM.\n4. Click on Users.\n5. Click on Security Credentials.\n6. As an Administrator:\n  - Click on the X (Delete) for keys that were created at the same time as the user profile but have not been used. \n7. As an IAM User:\n  - Click on the X (Delete) for keys that were created at the same time as the user profile but have not been used.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-469":{"article":"Desync mitigation mode protects your application from issues due to HTTP Desync. The load balancer classifies each request based on its threat level, allows safe requests, and then mitigates risk as specified by the mitigation mode that you specify. The desync mitigation modes are monitor, defensive, and strictest. The default is the defensive mode, which provides durable mitigation against HTTP desync while maintaining the availability of your application.","impact":"HTTP Desync issues can lead to request smuggling and make applications vulnerable to request queue or cache poisoning. In turn, these vulnerabilities can lead to execution of unauthorized commands.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load balancers. \n3. Choose an Application Load Balancer. \n4. From Actions, choose Edit attributes. \n5. For Desync mitigation mode choose either Defensive or Strictest.\n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-079":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for changes made to Identity and Access Management (IAM) policies.","impact":"Lack of monitoring and logging of IAM policy changes calls can lead to insufficient response time to accidental or intentional changes that may result in unauthorized access.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName=DeleteGroupPolicy) || ($.eventName=DeleteRolePolicy) || ($.eventName=DeleteUserPolicy) || ($.eventName=PutGroupPolicy) || ($.eventName=PutRolePolicy) || ($.eventName=PutUserPolicy) || ($.eventName=CreatePolicy) || ($.eventName=DeletePolicy) || ($.eventName=CreatePolicyVersion) || ($.eventName=DeletePolicyVersion) || ($.eventName=AttachRolePolicy) || ($.eventName=DetachRolePolicy) || ($.eventName=AttachUserPolicy) || ($.eventName=DetachUserPolicy) || ($.eventName=AttachGroupPolicy) || ($.eventName=DetachGroupPolicy)}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-281":{"article":"A scaling cooldown helps you prevent your Auto Scaling group from launching or terminating additional instances before the effects of previous activities are visible.\nIf the cooldown period is set to 0, it means that scaling activities initiated by simple scaling policies can start at any moment and cause the initiation of an additional scaling activity.\nIt is recommended to set a cooldown period of at least 300 seconds, this is a value that AWS sets by default.","impact":"If the cooldown period is set to 0, it means that scaling activities initiated by simple scaling policies can start at any moment and cause the initiation of an additional scaling activity and repeatedly trigger unnecessary scaling actions.","report_fields":["AutoScalingGroupARN"],"remediation":"In order to update the default cooldown period, perform the following: \n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2autoscaling/.\n2. Choose auto scaling group that you want to modify.\n3. Under 'Advanced configurations', click 'Edit' button.\n4. Set a new value for 'Default cooldown'.\n5. Choose 'Update'.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-460":{"article":"For storing sensitive information, you can encrypt environment variable values prior to sending them to Lambda by using the console's encryption helpers. This adds an additional layer of encryption that obscures secret values in the Lambda console and API output, even for users who have permission to use the key. In your code, you retrieve the encrypted value from the environment and decrypt it by using the AWS KMS API.","impact":"Without encrypting environment variable values in transit, there is a possibility of unauthorized access or accidental exposure (in the Lambda console and API output) of sensitive and critical data stored in variables.","report_fields":["FunctionArn"],"remediation":"1. Use the AWS Key Management Service (AWS KMS) to create any customer managed keys for Lambda to use for server-side and client-side encryption. \n2. Login to the AWS Management Console and open the Amazon Lambda https://console.aws.amazon.com/lambda/.\n3. In the navigation pane click on the 'Functions'.\n4. Click on the required function.\n5. Click on the 'Configuration' and then 'Environment variables'.\n6. Click 'Edit'.\n7. Under 'Encryption in transit', choose 'Enable helpers for encryption in transit'.\n8. For each environment variable that you want to enable console encryption helpers for, choose 'Encrypt' next to the environment variable.\n9. Under 'AWS KMS key to encrypt in transit', choose a customer managed key that you created at the beginning of this procedure.\n10. Choose 'Execution role policy' and copy the policy. This policy grants permission to your function's execution role to decrypt the environment variables.\n11. Save this policy to use in the last step of this procedure.\n12. Add code to your function that decrypts the environment variables. Choose 'Decrypt secrets snippet' to see an example.\n13. Click 'Encrypt'.\n14. Choose 'Save'.\n15. Set up permissions. If you're enabling client-side encryption for security in transit, your function needs permission to call the 'kms:Decrypt' API operation. Add the policy that you saved previously in this procedure to the function's execution role.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-477":{"article":"Configuring an SNS notification with your CloudFormation stack helps immediately notify stakeholders of any events or changes occurring with the stack. The stack must not only have an SNS topic configured, but must also have at least one confirmed subscription to that topic.","impact":"Not enabling SNS notification can lead to slow or no response to a stack changes.","report_fields":["StackId"],"remediation":"1. Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/.\n2. Select stack for which you want to configure notifications.\n3. Click 'Update' on the top bar.\n4. Click 'Next' two times.\n5. On the 'Configure stack options' scroll down to the 'Notification options'.\n6. Select an existing SNS topic or create new.\n7. Click 'Next'.\n8. Click 'Update stack'.\n9. Navigate to the SNS console https://console.aws.amazon.com/sns/v3.\n10. Click 'Topics'.\n11. Select the topic that was configured at the step 6.\n12. On the 'Subscriptions' tab, click 'Create subscription'.\n13. Configure subscription and click 'Create subscription'.\n14. Confirm subscription'.","multiregional":false,"service":"AWS CloudFormation"},"ecc-aws-388":{"article":"This policy identifies the Transit gateway that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["TransitGatewayArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/.\n2. Click on the 'Transit gateway'.\n3. Click on the required transit gateway.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"AWS Transit Gateway"},"ecc-aws-070":{"article":"A security group should always have attached protected assets. Removing Unused Security Groups is the expected outcome of the firewall and router rule sets review.","impact":"Not cleaning out your security groups pose the risk that a forgotten security group policy will be used to accidentally open an attack surface.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Click on the reported Security group.\n4. Click on Actions.\n5. Click on Delete security group.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-057":{"article":"AWS access from within AWS instances can be performed by either encoding AWS keys into AWS API calls or by assigning the instance to a role which has an appropriate permissions policy for the access required. 'AWS Access' means accessing the APIs of AWS in order to use AWS resources or manage AWS account resources.","impact":"Not using AWS IAM roles for AWS resource access from instances increases the risks of unauthorized access to instances as compromised credentials can be used by the attacker to gain and maintain access to a specific instance to use the privileges associated with it.","report_fields":["InstanceId","OwnerId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. In AWS IAM create a new role. Assign a permissions policy if needed permissions are already known. \n2. From the AWS console, launch a new instance with settings identical to the existing one and ensure that the newly created role is selected. \n3. Shutdown both the existing instance and the new instance. \n4. Detach disks from both instances. \n5. Attach the existing instance disks to the new instance.\n6. Boot the new instance and you will have the same machine, but with the associated role.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-453":{"article":"To manage enterprise caching solution, it's important that you know how your clusters are performing and the resources they're consuming. It's also important that you know the events that are being generated and the costs of your deployment.","impact":"With disabled logs for Redis, it is harder to know how clusters are performing and the resources they're consuming, what events that are being generated and the costs of your deployment.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. Under 'Resources' click on the 'Redis cluster'.\n3. Click on the required cluster.\n4. Click on 'Logs'.\n5. Under the 'Slow logs' and 'Engine logs' click on the enable.\n6. Choose any format and choose any Log destination type.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-307":{"article":"The log_statement setting specifies the types of SQL statements that cause an error condition are recorded in the server log. \nValues are: debug5, debug4, debug3, debug2, debug1, info, notice, warning, error, log, fatal, panic. It is recommended to set on 'error' or stricter for better security.","impact":"If log_statement is not set correctly then few or too many SQL requests will be logged.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_min_error_statement\".\n6. Choose 'error' or stricter values according to your organization policy.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-473":{"article":"Desync mitigation mode protects your application from issues due to HTTP Desync. The load balancer classifies each request based on its threat level, allows safe requests, and then mitigates risk as specified by the mitigation mode that you specify. The desync mitigation modes are monitor, defensive, and strictest. The default is the defensive mode, which provides durable mitigation against HTTP desync while maintaining the availability of your application.","impact":"HTTP Desync issues can lead to request smuggling and make applications vulnerable to request queue or cache poisoning. In turn, these vulnerabilities can lead to credential stuffing or execution of unauthorized commands.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load balancers. \n3. Choose an Classic Load Balancer. \n4. On the Description tab, choose Configure desync mitigation mode. \n5. On the Configure desync mitigation mode page, choose either Defensive or Strictest.\n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-113":{"article":"Inline policies are policies that are attached directly to a single user, group, or role. It is recommend to use managed policies instead of inline policies. Managed policies provide reusability, central change management, versioning, and other capabilities.","impact":"Not using managed policies, you are missing out on such possibilities as reusability, versioning and rollback, automatic updates, larger policy size, and fine-grained control over policies assignment.","report_fields":["Arn"],"remediation":"1. On the IAM console, select Users from the Navigation pane and then select Permissions.  \n2. Remove any policies attached directly to the user (these are inline policies), and replace them with the equivalent managed policies (on the Policies page) that are assigned to users, groups or roles.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-048":{"article":"Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure passwords are comprised of different character sets. \nIt is recommended that the password policy require at least one symbol.","impact":"Not using a password policy with at least one symbol reduces security and account resiliency against brute-force attacks.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane. \n4. Check 'Require at least one non-alphanumeric character'. \n5. Click on 'Apply password policy'.","multiregional":true,"service":"AWS Account"},"ecc-aws-408":{"article":"This policy identifies the ELB that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["LoadBalancerArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon ECS at https://console.aws.amazon.com/ec2/v2.\n2. Under the 'Load Balancing' click on the 'Load Balancers'.\n3. Click on the required load balancer.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-495":{"article":"By specifying a hard memory limit for your tasks you avoid running out of memory because ECS stops placing tasks on the instance, and docker kills any containers that try to go over the hard limit.","impact":"If not specifying a hard memory limit, tasks can run out of memory because ECS will not stop placing tasks on the instance.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. From the navigation bar, choose the Region that contains your task definition.\n3. In the navigation bar, choose task definitions.\n4. Click on the 'Create new revision'.\n5. Under 'Container definitions' click on the 'Add container' or click on the existing container.\n6. Click on the 'Add Hard limit'.\n7. Set required value.\n8. Save.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-420":{"article":"This policy identifies the KMS that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["KeyArn","AWSAccountId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EFS at https://console.aws.amazon.com/kms.\n2. Click on the required customer managed key.\n3. Open 'Tags' and click on the 'Create tag'.\n4. Add new tag and save.","multiregional":false,"service":"AWS Key Management Service"},"ecc-aws-479":{"article":"If you do not specify a Customer managed key when creating your environment, CloudWatch log groups uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over KMS key management. Using AWS-managed KMS key may not meet your company's compliance requirements.","report_fields":["arn"],"remediation":"Use AWS CLI to associate KMS CMK with log group:\naws logs associate-kms-key --log-group-name my-log-group --kms-key-id 'key-arn'","multiregional":false,"service":"Amazon CloudWatch"},"ecc-aws-035":{"article":"This policy identifies security group rules that allow inbound traffic to the Oracle DB port (1521) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted Oracle Database access can increase opportunities for malicious activity such as unauthorized access, denial-of-service (DoS) attacks and loss of data. \nAlso, unrestricted Oracle Database access allows remote attackers to execute arbitrary database commands by performing a remote registration of a database instance or service name that already exists, then conducting a man-in-the-middle (MITM) attack to hijack database connections.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-577":{"article":"Reserved Instances provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account.","impact":"A billing discount is not applied to instances in your account.","report_fields":["ReservedInstancesId"],"remediation":"To retry a failed RI payment, contact AWS Support: https://console.aws.amazon.com/support.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-258":{"article":"With Amazon EMR versions 4.8.0 and later, you can use a security configuration to specify settings for encrypting data at rest, data in transit, or both.\nData encryption helps prevent unauthorized users from reading data on a cluster and associated data storage systems. This includes data saved to persistent media, known as data at rest, and data that may be intercepted as it travels the network, known as data in transit.","impact":"When encryption of data at rest is disabled, it can lead to unauthorized access to sensitive information available on a cluster and associated data storage systems.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nIn order to enable in-transit and at-rest encryption for existing AWS EMR clusters, you must create an EMR security configuration then clone clusters with the new security configuration. \nTo create a security configuration using the console:\n1. Open the Amazon EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, choose Security Configurations, Create security configuration.\n3. Type a Name for the security configuration.\n4. Enable S3 encryption, Local disk encryption and Data in transit encryption.\n5. Choose Create.\nFor more information on the encryption options follow the link: https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-create-security-configuration.html.\n\nTo clone a cluster using the console:\n1. Open the AWS EMR console at https://console.aws.amazon.com/elasticmapreduce/.\n2. In the navigation pane, under EMR on EC2, choose Clusters.\n3. Select the cluster that you want to clone.\n4. At the top of the Cluster Details page, click Clone.\n5. In the dialog box, choose Yes to include the steps from the original cluster in the cloned cluster. Choose No to clone the original cluster's configuration without including any of the steps.\n6. The Create Cluster page appears with a copy of the original cluster's configuration. Review the configuration, make any necessary changes, and on the 'Step 4: Security' select the security configuration that you created earlier.\n7. Then click Create Cluster.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-138":{"article":"With the creation of an AWS account, a root user is created that cannot be disabled or deleted. This user has unrestricted access to and control over all resources in the AWS account. It is highly recommended that the use of this account be avoided for everyday tasks.","impact":"The 'root user' has unrestricted access to and control over all account resources. Use of it is inconsistent with the principles of least privilege and separation of duties, and can lead to unnecessary harm due to error or account compromise.","report_fields":["account_id","account_name"],"remediation":"If you find out that the root user account is being used for daily activities including administrative tasks that do not require a root user, do the following:\n1. Change the root user password. \n2. Deactivate or delete any access keys associated with the root user.","multiregional":true,"service":"AWS Account"},"ecc-aws-454":{"article":"Monitoring is an essential part of maintaining the availability, reliability, and performance of your Elasticache clusters. When a notable event occurs within your cluster, Elasticache sends a message to the configured email address via Amazon SNS to keep you up-to-date on everything that's going on within your clusters. Notable events include cluster provisioning error, settings changes and more.","impact":"Not enabling Beanstalk notifications can lead to a slow or no response to significant cluster events.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' or 'Redis clusters' from the navigation pane.\n4. Choose required cluster.\n5. Click on the 'Modify' button.\n6. Under the 'Topic for Amazon SNS notification' choose existing sns topic or create a new one.\n7. Save changes.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-095":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes made to Config configurations.","impact":"Lack of monitoring and logging of AWS Config configuration changes can lead to undetected Config items changes.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{ ($.eventSource = config.amazonaws.com) && (($.eventName=StopConfigurationRecorder)||($.eventName=DeleteDeliveryChannel)||($.eventName=PutDeliveryChannel)||($.eventName=PutConfigurationRecorder)) }}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-167":{"article":"IMAP is a protocol used for the management of eMail storage on a mail server, it is commonly used by an email user/client to view, access and edit the messages on the remote server, and emulate an experience of the message actually been stored locally on the end user\u2019s computer.\nUnrestricted access (0.0.0.0/0) increases opportunities for  malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:\n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-429":{"article":"This policy identifies the Amazon Redshift clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ClusterIdentifier"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. On the navigation menu, choose Clusters.\n3. Choose the name of the cluster that you want to modify from the list. The cluster details page appears.\n4. Choose the 'Properties' tab, then in the 'Tags' section choose 'Add tags'.\n5. Add tags.\n6. Save.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-413":{"article":"This policy identifies the Glacier that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["VaultARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Glacier at https://console.aws.amazon.com/glacier.\n2. Click on the 'Vaults'.\n3. Click on the required vault.\n4. Add new tag and save.","multiregional":false,"service":"Amazon S3 Glacier"},"ecc-aws-436":{"article":"You can gain better insight into your AWS Kinesis streams usage (i.e. distribution of data throughput) by enabling shard-level metrics such as IncomingBytes and IncomingRecords \u2013 helpful metrics that identify which shard is receiving more data within a stream, WriteProvisionedThroughputExceeded \u2013 metric that determines if the writes are throttled within a stream shard over a specified period of time, ReadProvisionedThroughputExceeded \u2013 metric that returns the number of GetRecords calls throttled within a shard over a specified time frame, etc.","impact":"Without Shard Level Enhanced Monitoring, it is not possible to quickly respond to performance changes in the underlying infrastructure. This can lead to data unavailability.","report_fields":["StreamARN"],"remediation":"1. Login to the AWS Management Console at https://console.aws.amazon.com/kinesis.\n2. Choose 'Data streams'.\n3. Choose required data stream.\n4. Go to 'Configuration'.\n5. Under 'Enhanced (shard-level) metrics' click on the 'Edit'.\n6. Choose all metrics and save.","multiregional":false,"service":"Amazon Kinesis"},"ecc-aws-108":{"article":"Ensure that your AWS Cloudfront distributions logging is enabled. CloudFront distribution logging is used to track all the requests for the content delivered through the Content Delivery Network (CDN). It is helpful during investigation activities and provides audit trail used for audit purposes.","impact":"Lack of monitoring and logging of Cloudfront can lead to missing suspicious requests made for web content.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS console.\n2. On the console, select the specific region. \n3. Navigate to the CloudFront Distributions dashboard. \n4. Click on the reported distribution.\n5. On the 'General' tab, click on 'Edit'.\n6. On the 'Edit Distribution' page, set 'Logging' to 'On', select 'Bucket for Logs' and 'Log Prefix' as desired.\n7. Click on 'Yes', then click on 'Edit'.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-007":{"article":"When an RDS DB instance is enabled with Multi-AZ, the RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different availability zone. \nThese Multi-AZ deployments will improve primary node reachability by providing read replica in case of network connectivity loss or loss of availability in the primary availability zone for read/write operations.","impact":"When an RDS is not configured for multiple Availability Zones and in case of any issue with the Availability Zone availability, and during regular RDS maintenance, the data stored in a database will not be reachable. Multi-AZ deployments allow you to automate failover.","report_fields":["DBInstanceArn"],"remediation":"1. Sign in to the AWS console. \n2. On the console, select the specific region. \n3. Navigate to the Amazon RDS console. \n4. Select 'Instances', and then select the reported DB instance. \n5. Select 'Modify'  from the 'Instance Actions' drop-down list. \n6. In the 'Instance Specifications' section for the 'Multi-AZ Deployment', select 'Yes'. \n7. Click on 'Continue'. \n8. On the confirmation page, review the changes and click on 'Modify DB Instance' to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-074":{"article":"You can launch AWS resources, such as Amazon Elasticsearch Service domains, into a virtual private cloud (VPC). A VPC is a virtual network that's dedicated to your AWS account. It's logically isolated from other virtual networks in the AWS Cloud. Placing an Elasticsearch Service domain within a VPC enables secure communication between Elasticsearch Service and other services within the VPC without the need for an internet gateway, NAT device, or VPN connection. All traffic remains securely within the AWS Cloud.","impact":"Deploying Elasticsearch domains without a VPC reduces the security by allowing all the traffic between Elasticsearch Service and other services over the public Internet.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. If you create a domain with a public endpoint, you cannot later place it within a VPC. Instead, you must create a new domain and migrate your data.\n2. The reverse is also true. If you create a domain within a VPC, it cannot have a public endpoint. Instead, you must either create another domain or disable this control.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-224":{"article":"Instance metadata is used to configure or manage the running instance. The IMDS provides access to temporary, frequently rotated credentials. These credentials remove the need to hard code or distribute sensitive credentials to instances manually or programmatically.\nVersion 2 of the IMDS adds new protections for the following types of vulnerabilities. These vulnerabilities could be used to try to access the IMDS:\n  - Open website application firewalls; \n  - Open reverse proxies; \n  - Server-side request forgery (SSRF) vulnerabilities;\n  - Open Layer 3 firewalls and network address translation (NAT).","impact":"Instances that use IMDSv1 are exposed to the following vulnerabilities: \n- Open website application firewalls;\n- Open reverse proxies;\n- Server-side request forgery (SSRF) vulnerabilities;\n- Open Layer 3 firewalls and network address translation (NAT).","report_fields":["InstanceId","OwnerId"],"remediation":"Currently only the AWS SDK or AWS CLI support modifying the instance metadata options on existing instances. You can't use the Amazon EC2 console for modifying instance metadata options.\nUse the modify-instance-metadata-options CLI command and set the http-tokens parameter to required. When you specify a value for http-tokens, you must also set http-endpoint to enabled:\naws ec2 modify-instance-metadata-options --instance-id i-1234567898abcdef0 --http-tokens required --http-endpoint enabled","multiregional":false,"service":"Amazon EC2"},"ecc-aws-226":{"article":"A cluster security group is designed to allow all traffic from the control plane and managed node groups to flow freely between each other. By assigning the cluster security group to the elastic network interfaces created by Amazon EKS that allow the control plane to communicate with the managed node group instances, you don't need to configure complex security group rules to allow this communication. Any instance or network interface that is assigned this security group can freely communicate with other resources with this security group. \nSecurity group should allow only 443 and 10250. Any protocol and ports that you expect your nodes to use for inter-node communication should be included, if required.","impact":"Allowed inbound traffic on all ports enables attackers to use port scanners and other probing techniques to identify applications and services running on your EKS clusters and exploit their vulnerabilities.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Management Console.\n2. Navigate to Amazon EC2 dashboard at https://console.aws.amazon.com/ec2/.\n3. In the navigation panel, under NETWORK & SECURITY section, choose Security Groups.\n4. Select the security group that you want to reconfigure.\n5. Select the Inbound rules tab from the dashboard bottom panel and click the Edit button to update inbound rules configuration.\n6. Inside the Edit inbound rules dialog box, find the inbound rule(s) configured to allow access on ports different than TCP port 443 and 10250, then click on the x button next to each rule to remove it from the security group. Once all non-compliant inbound rules are deleted from the selected security group, click Save to apply the changes.\n7. Repeat steps no. 5 \u2013 6 , but for Outbound rules tab.\n8. Repeat steps no. 4 \u2013 7 to update other security groups with non-compliant access configurations, associated with your Amazon EKS clusters.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-193":{"article":"By default, ALBs are not configured to drop invalid HTTP header values. Removing these header values prevents HTTP desync attacks.","impact":"Not dropping invalid HTTP headers may result in data leakage or the execution of unwanted actions on back-end systems.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load balancers. \n3. Choose an Application Load Balancer. \n4. From Actions, choose Edit attributes. \n5. Under Drop Invalid Header Fields, choose Enable. \n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-199":{"article":"In Amazon RDS, Enhanced Monitoring enables a more rapid response to performance changes in underlying infrastructure. These performance changes could result in a lack of availability of the data. \nEnhanced Monitoring provides real-time metrics of the operating system that your RDS DB instance runs on. An agent is installed on the instance. The agent can obtain metrics more accurately than is possible from the hypervisor layer.","impact":"Without Enhanced Monitoring, it is not possible to quickly respond to performance changes in the underlying infrastructure. This can lead to data unavailability.","report_fields":["DBInstanceArn"],"remediation":"1. Open the IAM console at https://console.aws.amazon.com/iamv2 \n2. In the navigation pane, choose Roles. \n3. Choose Create role. \n4. Choose the AWS service tab, and then choose RDS from the list of services. \n5. Choose RDS - Enhanced Monitoring, and then choose Next: Permissions. \n6. Ensure that the Attached permissions policy page shows AmazonRDSEnhancedMonitoringRole, and then choose Next: Tags. \n7. On the Add tags page, choose Next: Review. \n8. For Role Name, enter a name for your role. For example, enter emaccess. The trusted entity for your role is the AWS service monitoring.rds.amazonaws.com. \n9. Choose Create role.  \nTo enable enhanced monitoring  \n1. Open the RDS console at https://console.aws.amazon.com/rds.\n2. In the navigation pane, choose Databases. \n3. Choose instance and press modify. \n4. Scroll to the Monitoring section. \n5. Choose Enable enhanced monitoring for your DB instance or read replica.\n6. Set the Monitoring Role property to the IAM role that you created to permit Amazon RDS to communicate with Amazon CloudWatch Logs for you, or choose Default to have RDS create a role for you named rds-monitoring-role. \n7. Set the Granularity property to the interval, in seconds, between points when metrics are collected for your DB instance or read replica. The Granularity property can be set to one of the following values: 1, 5, 10, 15, 30, or 60.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-328":{"article":"Removing unattached/orphaned Elastic Block Store volumes will help you avoid unexpected charges on your AWS bill and halt access to any sensitive data available on these volumes.","impact":"Not removing unattached/orphaned Elastic Block Store volumes still get charges on your AWS bill.","report_fields":["VolumeId"],"remediation":"To create snapshot before deleting the volume (optional):\n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ \n2. Under Elastic Block Store, click on Volumes. \n3. Click on the required volume.\n4. Under the Actions, click on Create snapshot.\n5. Click on the Create snapshot.\n\nTo delete EBS Volume \n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ \n2. Under Elastic Block Store, click on Volumes. \n3. Click on the required volume.\n4. Under the Actions, click on Delete volume.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-054":{"article":"IAM policies are the means by which privileges are granted to users, groups, or roles. It is recommended and considered a standard security advice to grant least privilege that is, granting only the permissions required to perform a task.\nDetermine what users need to do and then craft policies that allow them to perform only those tasks instead of allowing full administrative privileges.","impact":"Providing full administrative privileges instead of restricted ones to the minimum set of permissions can expose your AWS resources to potentially unwanted actions.","report_fields":["Arn"],"remediation":"1. Sign in to the AWS Management Console and open the IAM console.\n2. In the navigation pane, click Policies and then search for the policy name found in the audit report. \n3. Select the policy that needs to be deleted. \n4. In the policy action menu, select first Detach.\n5. Select all Users, Groups, Roles that have this policy attached 6. Click on Detach Policy.\n7. In the policy action menu, select Detach.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-476":{"article":"You can use drift detection to identify stack resources to which configuration changes have been made outside of CloudFormation management. You can then take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. Resolving drift helps to ensure configuration consistency and successful stack operations.","impact":"Configuration changes performed outside of CloudFormation can lead to problems and can complicate stack update or deletion operations that affects configuration consistency and successful stack operations.","report_fields":["StackId"],"remediation":"After drift detection identified stack resources to which configuration changes have been made outside of CloudFormation management. You can take corrective action so that your stack resources are again in sync with their definitions in the stack template, such as updating the drifted resources directly so that they agree with their template definition. \nAnother way to resolve drift of resources is to update CloudFormation stack template to match the current configurations of resources.","multiregional":false,"service":"AWS CloudFormation"},"ecc-aws-445":{"article":"Amazon MWAA can send Apache Airflow logs to Amazon CloudWatch. You can view logs for multiple environments from a single location to easily identify Apache Airflow task delays or workflow errors without the need for additional third-party tools.","impact":"If 'Scheduler logs' is not enabled and the 'log_level' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Click \"Next\".\n5. Enable 'Airflow scheduler logs'.\n6. Choose required log level.\n7. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-135":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements or compliance standards. It is highly recommended that access to data be restricted to encrypted protocols. This rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/.\n2. Go to Load Balancers.\n3. Click on the reported Load Balancer.\n4. In the 'Description' tab, under 'Security' select security group to edit.\n5. In the 'Inbound rules' tab, click 'Edit inbound rules'.\n6. Modify rules.\n7. Click on 'Save rules'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-405":{"article":"This policy identifies the EFS that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["FileSystemArn","OwnerId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EFS at https://console.aws.amazon.com/efs.\n2. Click on the required efs.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Elastic File System"},"ecc-aws-107":{"article":"Checks an ACM for unused certificates. It is recommended to delete unused certificates or associate them (use them).","impact":"Unused certificates can be invalid, failed, or expired. Their accidental use can lead to the risk of deploying an invalid certificate in resources which may trigger an error in the front end and cause a loss of credibility for the web application/website.","report_fields":["CertificateArn","DomainName"],"remediation":"To delete unused certificates: \n1. Sign in to the AWS console.\n2. On the console, select the region. \n3. Navigate to the Certificate Manager(ACM) service.\n4. Select the certificate that was reported. \n5. Under the 'Actions' drop-down list, click on 'Delete'.  \nAlternatively, you can associate/use the unused certificate with the resource that requires it.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-067":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for unauthorized API calls.","impact":"Lack of monitoring and logging of unauthorized API calls can lead to insufficient response time to threats from an attacker. In turn, it can lead to data loss and degradation of the service. Monitoring unauthorized API calls will help reveal application errors and reduce the time to detect malicious activity.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{(($.errorCode=\"*UnauthorizedOperation\") || ($.errorCode=\"AccessDenied*\")) && (($.sourceIPAddress!=\"delivery.logs.amazonaws.com\") && ($.eventName!=\"HeadBucket\"))}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-254":{"article":"Ensure that at-rest encryption is enabled for your AWS Glue job bookmarks in order to encrypt the bookmark data before it is sent to Amazon S3 for storage.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data.","report_fields":["Name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create and configure a new AWS Glue security configuration, perform the following actions: \n1. Sign in to AWS Management Console.\n2. Navigate to Glue service dashboard at https://console.aws.amazon.com/glue/.\n3. In the left navigation panel, under Security, select Security configurations.\n4. Click Add security configuration to start the setup process.\n5. On Add security configuration page, perform the following:\n  5.1 In the Security configuration name box, type a unique name for your new Glue security configuration.\n  5.2 Click the Advanced properties tab and select Job bookmark encryption checkbox to enable encryption at-rest for your Amazon Glue job bookmarks, then choose the ARN of the AWS KMS key that you want to use from AWS KMS key dropdown list.\n  5.3 Then click Finish to create your new Amazon Glue security configuration.\n6. Update the configuration of your existing AWS Glue ETL jobs to make use of the new security configuration created at the previous step.","multiregional":false,"service":"AWS Glue"},"ecc-aws-273":{"article":"Amazon DocumentDB engine logs (Audit, Profiler) should be enabled and sent to CloudWatch.\nRDS cluster logging provides detailed records of requests made to DocumentDB. DocumentDB cluster logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the DocumentDB cluster, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBClusterArn"],"remediation":"To change Cluster settings:\n1. Open the DocumentDB console at https://console.aws.amazon.com/docdb\n2. In the navigation pane, choose Clusters.\n3. Choose the cluster that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose Audit and Profiler log types to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify immediately.\n\nTo change Parameter groups settings:\n1. Open the DocumentDB console at https://console.aws.amazon.com/docdb\n2. In the navigation pane, choose Parameter groups.\n3. Choose and open the parameter group that you want to modify.\n4. Change 'audit_logs' and 'profiler' parameters.\n5. Choose Edit.\n6. Change to value to enabled.","multiregional":false,"service":"Amazon DocumentDB"},"ecc-aws-264":{"article":"Default ports are: 6379 for Redis and 11211 for Memcached. Changing default port helps with automated attacks and gives more information if it was a target attack or not.","impact":"Running AWS ElastiCache clusters on the default port is a potential security risk as it provides an attacker path to a service listening on that port and increases the attack vectors your organization is exposed to.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nChanging port on an existing ElastiCache Redis cluster:\n1. Create a manual backup of the cluster:\n  1.1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  1.2. From the list in the upper-right corner, choose the AWS Region that you want to launch this cluster in.\n  1.3. Choose 'Redis clusters' from the navigation pane.\n  1.4. Choose the cluster and choose 'Action', and then 'Backup'.\n  1.5. Make sure the cluster name is right and enter backup name. \n  1.6. Select 'Encryption key'. \n  1.7. Choose 'Create Backup'.\n2. Create a new cluster by restoring from the backup:\n  2.1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2.2. Choose 'Redis clusters' from the navigation pane.\n  2.3. Select 'Create Redis cluster'.\n  2.4. For 'Choose a cluster creation method', choose 'Restore from backups'.\n  2.5. For 'Source' select 'Amazon ElastiCache backups'. And select backup created on the previous step.\n  2.6. Under 'Redis settings' change 'Port' value from default.\n  2.7. Complete other settings and click 'Next' and 'Next'.\n  2.8. Review settings and click 'Create'.\n3. Update the endpoints in your application to the new cluster's endpoints.\n4. Delete the old cluster.\n\nTo create an ElastiCache for Memcached cluster:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region you want to launch this cluster in.\n3. Choose 'Memcached clusters' from the navigation pane.\n4. Choose 'Create Memcached cluster'.\n5. Under 'Cluster settings' change 'Port' value from default.\n6. Complete the 'Cluster settings' section. \n7. Click 'Next'.\n8. Complete the 'Advanced settings' section. \n9. Click 'Next'.\n10. Review all your entries and choices, then go back and make any needed corrections. When you're ready, choose 'Create' to launch your cluster.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-433":{"article":"An active/standby broker is comprised of two brokers in two different Availability Zones, configured in a redundant pair. These brokers communicate synchronously with your application, and with Amazon EFS.","impact":"When an MQ is not configured with active/standby broker, in case of any issue with the Availability Zone, MQ will not be reachable. \nActive/standby deployments allow you to automate failover.","report_fields":["BrokerArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Sign in to the AWS Management Console.\n2. Navigate to the Amazon MQ dashboard at https://console.aws.amazon.com/amazon-mq/.\n3. Under the 'Select deployment and storage type' choose 'Active/standby broker' or 'Cluster deployment'\n4. Create broker.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-203":{"article":"Amazon PostgreSQL database engine logs (Postgresql, Upgrade) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"To enable and publish PostgreSQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- log_statement=all\n- log_min_duration_statement=minimum query duration (ms) to log\n\nTo create a custom DB parameter group\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.\n\nTo publish PostgreSQL logs to CloudWatch Logs from the AWS Management Console\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose all of the log files to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-538":{"article":"An origin is the location where content is stored, and from which CloudFront gets content to serve to viewers.\nWhen a CloudFront distribution in your account is configured to point to a non-existent bucket, a malicious third party can create the referenced bucket and serve their own content through your distribution. It is recommended to check all origins regardless of routing behavior to ensure that your distributions are pointing to appropriate origins.","impact":"When a CloudFront distribution in your account is configured to point to a non-existent bucket, a malicious third party can create the referenced bucket and serve their own content through your distribution.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n2. Choose the ID of a distribution that has an S3 origin.\n3. Choose the 'Origins' tab.\n4. Choose the check box next to an origin, and then choose 'Edit'.\n5. For 'Origin domain' update value to a valid value (name of S3 bucket that exists or S3 website endpoint).\n8. Click 'Save changes'.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-176":{"article":"Encrypt Amazon RDS snapshot at rest by enabling the encryption option for your Amazon RDS DB snapshot.\nThe RDS snapshot encryption and decryption process is handled transparently and does not require any additional action from you or your application. The keys used for AWS RDS database snapshot encryption can be entirely managed and protected by the Amazon Web Services key management infrastructure or fully managed by the AWS customer through Customer Master Keys (CMKs).","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in snapshots.","report_fields":["DBSnapshotArn","DBInstanceIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n    \n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Snapshots.\n3. Select the Snapshot to modify.\n4. Choose Actions.\n5. Choose Copy Snapshot. \n6. Under Encryption options, check Enable encryption. \n7. Choose Copy snapshot.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-011":{"article":"If you have an HTTPS listener, you deployed an SSL server certificate on your load balancer when you created the listener. Each certificate comes with a validity period. You must ensure that you renew or replace the certificate before its validity period ends.\nCertificates provided by AWS Certificate Manager and deployed on your load balancer can be renewed automatically. ACM attempts to renew certificates before they expire. If you imported a certificate into ACM, you must monitor the expiration date of the certificate and renew it before it expires. After a certificate that is deployed on a load balancer is renewed, new requests use the renewed certificate.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and an AWS resource that implements the certificates is no longer secure.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to the AWS EC2 console at console.aws.amazon.com/ec2/.  \n2. Navigate to the 'Load Balancer' section, and then to the 'Listener' tab.  \n3. Select the listener (HTTPS), and the 'View/edit certificates' tab.  \n4. Add the new certificate (from IAM) for each instance that failed the rule.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-482":{"article":"From a security perspective, logging is an important feature to enable for future forensics efforts in the case of any security incidents. Correlating anomalies in CodeBuild projects with threat detections can increase confidence in the accuracy of those threat detections.","impact":"With disabled logs, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["arn"],"remediation":"1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Expand Build, choose Build project, and then choose the build project without logging. \n3. From Edit, choose Logs. \n4. Enable either CloudWatch logs or S3 Logs.\n5. Update logs.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-544":{"article":"CloudTrail provides a history of API activity within your AWS account, allowing you to track changes and troubleshoot issues. AWS CloudTrail delivery options provide flexibility and customization for logging, monitoring, and analyzing API activity within your AWS infrastructure.","impact":"If CloudTrail logs fail to deliver, you may lose important data related to API activity within your AWS account. This can make it difficult to track changes and troubleshoot issues, it may require additional effort to troubleshoot and resolve the delivery issues, which can increase your operational overhead. If CloudTrail logs are delayed or not delivered in real-time, it can delay your ability to detect and respond to security threats or other issues.","report_fields":["TrailARN"],"remediation":"1. Verify the delivery destination: Check that the S3 bucket, CloudWatch Logs log group, or SNS topic that is configured as the delivery destination for CloudTrail logs is active and accessible. You may need to confirm that the IAM roles and permissions are correctly configured.\n2. Check CloudTrail configuration: Review your CloudTrail configuration to ensure that it is correctly configured to deliver logs to the intended destination(s). Verify that the log file validation feature is enabled, which helps to detect and prevent data tampering.\n3. Check network connectivity: Confirm that there are no network connectivity issues between your AWS account and the delivery destination. Check for any firewall or network security group configurations that may be blocking traffic.\n4. Review delivery failure logs: Check the CloudTrail delivery failure logs to identify any specific errors or issues that may be causing the delivery failure. This may include issues with the AWS service that is delivering the logs, or issues with the delivery destination.\n5. Resend failed logs: Once you have identified the cause of the delivery failure, you can resend any failed logs using the CloudTrail console or the AWS Command Line Interface (CLI). This will help to ensure that you have a complete and accurate record of API activity within your AWS account.\n6. Monitor delivery: After resolving the delivery failure, monitor CloudTrail delivery to ensure that logs are being delivered as expected. Consider setting up alarms or notifications to alert you in the event of future delivery failures.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-444":{"article":"Amazon MWAA can send Apache Airflow logs to Amazon CloudWatch. You can view logs for multiple environments from a single location to easily identify Apache Airflow task delays or workflow errors without the need for additional third-party tools.","impact":"If 'Dag Processing logs' is not enabled and the 'log_level' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Click \"Next\".\n5. Enable 'Airflow DAG processing logs'.\n6. Choose required log level.\n7. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-146":{"article":"The Network Access Control List (NACL) function provides stateless filtering of ingress and egress network traffic to AWS resources. It is recommended that no NACL allow unrestricted ingress access to remote server administration ports, such as SSH to port 22 and RDP to port 3389.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["NetworkAclId","OwnerId"],"remediation":"1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home\n2. In the left pane, click on Network ACLs \n3. For each network ACL to remediate, perform the following:\n  - Select the network ACL\n  - Click on the Inbound Rules tab\n  - Click on Edit inbound rules\n  - Either \n    A) update the Source field to a range other than 0.0.0.0/0, or, \n    B) click on Delete to remove the offending inbound rule\n  - Click on Save","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-504":{"article":"When creating a RDS instance, you should change the default admin username 'admin' or 'postgres' to a unique value. Default usernames are public knowledge and should be changed upon configuration.","impact":"Well-known database Admin username could lead to unintended access.","report_fields":["DBInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose Create database.\n4. Under the 'Credentials Settings' change 'Master username'.\n5. Create database","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-112":{"article":"Enabling MFA delete for versioning is a good way to add extra protection to sensitive files stored in buckets.","impact":"Users can accidentally delete an S3 bucket, or an attacker/malicious user can do it deliberately to cause disruption.","report_fields":["Name"],"remediation":"Using AWS s3api CLI, enable MFA Delete for the S3 buckets that fail this rule, for example 'aws s3api put-bucket-versioning --bucket bucketname --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa 'your-mfa-serial-number mfa-code''.","multiregional":true,"service":"Amazon S3"},"ecc-aws-159":{"article":"RDS event notifications uses Amazon SNS to make you aware of changes in the availability or configuration of your RDS resources. These notifications allow for rapid response.\n'DBCluster: ['maintenance','failure']' Notification should be enabled for All Clusters.","impact":"Not enabling RDS cluster event notifications can lead to a slow response to a cluster failure.","report_fields":["account_id","account_name"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Event subscriptions.\n3. Under Event subscriptions, choose Create event subscription.\n4. In the Create event subscription dialog, do the following:\n4.1. For Name, enter a name for the event notification subscription.\n4.2. For Send notifications to, choose an existing Amazon SNS ARN for an SNS topic.\n     To use a new topic, choose Create topic to enter the name of a topic and a list of recipients.\n4.3. For Source type, choose Clusters.\n4.4. Under Instances to include, select All clusters.\n4.5. Under Event categories to include, select Specific event categories. The control also passes if you select All event categories.\n4.5.1 Select maintenance and failure.\n4.6. Choose Create.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-425":{"article":"This policy identifies the MWAA clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["Arn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon MWAA at https://console.aws.amazon.com/mwaa.\n2. Click on the required environment.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-185":{"article":"A failed finding indicates that an EC2 instance has not run for a significant period of time. This creates a security risk because the EC2 instance is not being actively maintained (analyzed, patched, updated). If it is later launched, the lack of proper maintenance could result in unexpected issues in your AWS environment. To safely maintain an EC2 instance over time in a nonrunning state, start it periodically for maintenance and then stop it after maintenance. Ideally this is an automated process.","impact":"EC2 instances that have not been running for a significant time pose a security risk because they were not actively maintained (analyzed, patched, updated). If they are launched later, the lack of proper maintenance may result in unexpected issues in the AWS environment.","report_fields":["InstanceId","OwnerId"],"remediation":"You can terminate an EC2 instance using either the console or the command line. \nBefore you terminate the EC2 instance, verify that you won't lose any data:\n  - Check that your Amazon EBS volumes will not be deleted on termination. \n  - Copy any data that you need from your EC2 instance store volumes to Amazon EBS or Amazon S3.  \n\nTo terminate an EC2 instance (console): \n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. \n2. In the navigation pane, under Instances, choose Instances. \n3. Select the instance, and then choose Actions, Instance State, Terminate. \n4. When prompted for confirmation, choose Yes, Terminate.   \n\nTo Terminate an EC2 instance (CLI): aws ec2 terminate-instances --instance-ids <value>","multiregional":false,"service":"Amazon EC2"},"ecc-aws-417":{"article":"This policy identifies the MSK clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ClusterArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon MSK at https://console.aws.amazon.com/msk\n2. Click on the required cluster.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Managed Streaming for Apache Kafka"},"ecc-aws-128":{"article":"Route53 conveniently allows you to register and manage domains alongside the rest of your AWS resources. When a domain is registered in Route53, it will automatically have automatic renewal enabled. In the case where automatic renewal is not enabled and domain was not renewed, it becomes expired.","impact":"An expired Amazon Route53 domain can cause website or application downtime or failure. An expired domain could be taken over by a malicious individual or deleted by the domain registrar and it will become available for others to register.","report_fields":["DomainName"],"remediation":"If you don't renew a domain before the end of the late-renewal period or if you accidentally delete the domain, some registries for top-level domains (TLDs) allow you to restore the domain before it becomes available for others to register.\nWhen a domain is deleted or it passes the end of the late-renewal period, it no longer appears in the Amazon Route53 console. \nTo restore any expired domain names registered with AWS Route 53, perform the following:\n1. Determine whether the TLD registry for the domain supports restoring domains and, if so, the period during which restoration is allowed.\n  a. Go to 'Domains that you can register with Amazon Route53': https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/registrar-tld-list.html.\n  b. Find the TLD for your domain, and review the values in the \"Deadlines for renewing and restoring domains\" section. \n2. Review the price for restoring a domain, which is often higher and sometimes much higher than the price for registering or renewing a domain. In 'Amazon Route53 Pricing for Domain Registration': https://d32ze2gidvkk54.cloudfront.net/Amazon_Route_53_Domain_Registration_Pricing_20140731.pdf, find the TLD for your domain (such as .com) and check the price in the \"Restoration Price\" column. If you still want to restore the domain, make note of the price; you'll need it in a later step.\n3. Using the AWS account that the domain was registered to, sign in to the AWS Support Center: https://console.aws.amazon.com/support/home?region=us-east-1#/case/create?issueType=customer-service&serviceCode=billing&categoryCode=domain-name-registration-issue.\n4. Specify the following values:\n  - Regarding: Accept the default value of 'Account and Billing Support'.\n  - Service: Accept the default value of 'Billing'.\n  - Category: Accept the default value of 'Domain name registration issue'.\n  - Subject: Enter 'Restore an expired domain' or 'Restore a deleted domain'.\n  - Description: \n    Provide the following information:\n      - The domain that you want to restore\n      - The 12-digit account ID of the AWS account that the domain was registered to\n      - Confirmation that you agree to the price for restoring the domain. Use the following text:\n        \"I agree to the price of $____ for restoring my domain.\"\n        Replace the blank with the price that you found in step 2.\n  - Contact method: Specify a contact method and, if you choose 'Phone', enter the applicable values.\n5. Choose 'Submit'.\n6. When AWS learn whether they were able to restore your domain, an AWS Support representative will contact you. In addition, if AWS were able to restore your domain, the domain will reappear in the console. The new expiration date is usually one or two years (depending on the TLD) after the old expiration date.\n  Note: The new expiration date is not calculated from the date that the domain was restored.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-399":{"article":"This policy identifies the Codebuild that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Codebuild at https://console.aws.amazon.com/codesuite/codebuild.\n2. Click on the 'Build', 'Build projects' and choose required project.\n3. Open 'Build details' and click on the 'Edit'.\n4. Under the 'Additional configuration' click on 'Tags'\n5. Add new tag and save.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-195":{"article":"Before you start to use your Application Load Balancer, you must add one or more listeners. A listener is a process that uses the configured protocol and port to check for connection requests. Listeners support both HTTP and HTTPS protocols. You can use an HTTPS listener to offload the work of encryption and decryption to your Application Load Balancer. You should use redirect actions with Application Load Balancer to redirect client HTTP request to an HTTPS request on port 443 to enforce encryption in-transit.","impact":"An HTTP protocol is not a secure method of transmitting data. Any person monitoring the Internet traffic can see unencrypted data, which leads to a breach of confidentiality.","report_fields":["LoadBalancerArn"],"remediation":"Create an HTTP listener rule that redirects HTTP requests to HTTPS\n  1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n  2. On the navigation pane, under LOAD BALANCING, choose Load Balancers.\n  3. Select a load balancer, and then choose Listeners, Add listener.\n    Note: Skip to step 6 if you already have an HTTP listener.\n  4. For Protocol: port, choose HTTP. You can either keep the default port or specify a custom port.\n  5. For Default actions, choose Add action, redirect to, and then enter port 443 (or a different port if you\u2019re not using the default). For more details, see Rule action types. To save, choose the checkmark icon.\n    Note: If you created a new HTTP listener following steps 3-5 above, skip to Create an HTTPS listener.\n  6. Select a load balancer, and then choose HTTP Listener.\n  7. Under Rules, choose View/edit rules.\n  8. Choose Edit Rule to modify the existing default rule to redirect all HTTP requests to HTTPS. Or, insert a rule between the existing rules (if appropriate for your use case).\n  9. Under Then, delete the existing condition. Then, add the new condition with the Redirect to action.\n  10. For HTTPS, enter 443 port.\n  11. Keep the default for the remaining options.\n  Note: If you want to change the URL or return code, you can modify these options as needed.\n  12. To save, choose the checkmark icon.\n\nCreate an HTTPS listener\nNote: If you already have an HTTPS listener with a rule to forward requests to the respective target group, skip to Verify that the security group of the Application Load Balancer allows traffic on 443.\n  1. Choose Listeners, Add listener.\n  2. For Protocol: port, choose HTTPS. Keep the default port or specify a custom port.\n  3. For Default actions, choose Add action, Forward to.\n  4. Select a target group that hosts application instances.\n  5. Select a predefined security policy that's best suited for your configuration.\n  6. Choose Default Security Certificate. (If you don\u2019t have one, you can create a security certificate.) \n  7. Choose Save.\n\nVerify that the security group of the Application Load Balancer allows traffic on 443\n  1. Choose the load balancer's Description.\n  2. Under Security, choose Security group ID.\n  3. Verify the inbound rules. The security group must have an inbound rule that permits traffic on HTTP and HTTPS. If there are no inbound rules, complete the following steps to add them.\n\nTo add inbound rules (if you don't already have them):\n  1. Choose Actions, Edit Inbound Rules to modify the security group.\n  2. Choose Add rule.\n  3. For Type, choose HTTPS.\n  4. For Source, choose Custom (0.0.0.0/0 or Source CIDR).\n  5. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-507":{"article":"Logging is an important part of maintaining the reliability, availability, and performance of services. Logging message delivery status helps provide operational insights, such as the following: \n  - Knowing whether a message was delivered to the Amazon SNS endpoint. \n  - Identifying the response sent from the Amazon SNS endpoint to Amazon SNS.\n  - Determining the message dwell time (the time between the publish timestamp and the hand off to an Amazon SNS endpoint).","impact":"With disabled logs, it is harder to analyze operational insights.","report_fields":["TopicArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon SNS at https://console.aws.amazon.com/sns/v3.\n2. Click on the required sns topic.\n3. On the Topics page, choose a topic and then choose Edit.\n4. On the Edit MyTopic page, expand the Delivery status logging section.\n5. Choose the protocol for which you want to log delivery status; for example AWS Lambda.\n6. Enter the Success sample rate (the percentage of successful messages for which you want to receive CloudWatch Logs.\n7. In the IAM roles section, do one of the following:\n  To choose an existing service role from your account, choose Use existing service role and then specify IAM roles for successful and failed deliveries.\n  To create a new service role in your account, choose Create new service role, choose Create new roles to define the IAM roles for successful and failed deliveries in the IAM console.\n  To give Amazon SNS write access to use CloudWatch Logs on your behalf, choose Allow.\n8. Choose Save changes.","multiregional":false,"service":"Amazon Simple Notification Service"},"ecc-aws-131":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements or compliance standards. It is highly recommended that access to data be restricted to encrypted protocols. This rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/\n2. Go to Security groups\n3. Click on the reported security group\n3. Go to the SG rules\n4. Click on the reported rule\n5. Click on Edit \n6. Modify the rule\n7. Click on Save","multiregional":false,"service":"Amazon EC2"},"ecc-aws-285":{"article":"AWS X-Ray always encrypts traces and related data at rest. When you need to audit and disable encryption keys for compliance or internal requirements, you can configure X-Ray to use an AWS Key Management Service (AWS KMS) key to encrypt data.\nX-Ray provides an AWS managed key named aws/xray. Use this key when you just want to audit key usage in AWS CloudTrail and don't need to manage the key itself. When you need to manage access to the key or configure key rotation, you can create a customer managed key.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["account_id","account_name"],"remediation":"To configure X-Ray to use a KMS key for encryption\n1. Open the X-Ray console: https://console.aws.amazon.com/xray/home#/service-map.\n2. Choose Encryption.\n3. Choose Use a KMS key.\n4. Choose a Customer Managed Key from the dropdown menu or manually enter a key ARN (usually used for a Customer Managed Key in a different account).\nNote: X-Ray does not support asymmetric KMS keys.\n5. Choose Apply.","multiregional":false,"service":"AWS X-Ray"},"ecc-aws-154":{"article":"An Elasticsearch domain requires at least three data nodes for high availability and fault-tolerance. Deploying an Elasticsearch domain with at least three data nodes ensures cluster operations if a node fails.","impact":"Using only one data node can lead to low availability and fault tolerance. Deploying an Elasticsearch domain with at least three data nodes ensures cluster operations if a node fails.","report_fields":["ARN"],"remediation":"From Console:\n1. Login to the AWS Management Console and open the Amazon Elasticsearch Service console using https://console.aws.amazon.com/esv3. \n2. Under 'My domains', choose the name of the domain to edit. \n3. Choose 'Edit domain'. \n4. Under 'Data nodes', set 'Number of nodes' to a number greater than or equal to three.\nFor three Availability Zone deployments, set to a multiple of three to ensure equal distribution across Availability Zones. \n5. Choose Submit.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-255":{"article":"Ensure that CloudWatch logs encryption is enabled for your Amazon Glue security configurations in order to meet regulatory requirements and prevent unauthorized users from getting access to the logging data published to AWS CloudWatch Logs. A security configuration is a set of encryption properties that are used by Amazon Glue service to configure encryption for crawlers, jobs and development endpoints.","impact":"Disabled encryption of CloudWatch logs allows a user to get unauthorized access to sensitive data.","report_fields":["Name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo enable encryption at rest for Amazon Glue logging data published to AWS CloudWatch Logs, you need to re-create the necessary security configurations with the CloudWatch Logs encryption mode enabled. To create and configure a new AWS Glue security configuration, perform the following actions: \n1. Sign in to AWS Management Console.\n2. Navigate to Glue service dashboard at https://console.aws.amazon.com/glue/.\n3. In the left navigation panel, under Security, choose Security configurations.\n4. Click Add security configuration to initiate the setup process.\n5. On Add security configuration page, perform the following:\n  5.1 Enter a unique name for your new configuration within Security configuration name box.\n  5.2 Select CloudWatch logs encryption checkbox to enable at-rest encryption when writing logs to AWS CloudWatch, then choose the ARN of the AWS KMS key that you want to use for encryption from AWS KMS key dropdown list.\n6. Reconfigure (update) your existing Amazon Glue crawlers, jobs and development endpoints to make use of the new security configuration created at the previous step.","multiregional":false,"service":"AWS Glue"},"ecc-aws-496":{"article":"If host is specified, all containers within the tasks that specified the host PID mode on the same container instance share the same process namespace with the host Amazon EC2 instance. If task is specified, all containers within the specified task share the same process namespace. If no value is specified, the default is a private namespace.","impact":"If the host's PID namespace is shared with containers, it would allow containers to see all of the processes on the host system. This reduces the benefit of process level isolation between the host and the containers. These circumstances could lead to unauthorized access to processes on the host itself, including the ability to manipulate and terminate them.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. From the navigation bar, choose the Region that contains your task definition.\n3. In the navigation bar, choose task definitions.\n4. Click on the 'Create new revision'.\n5. Under 'Volumes' click on the 'Configure via JSON'.\n6. Set value in 'pidMode' to 'task' or 'null'.\n7. Save changes.\n8. If your task definition is used in a service, update your service with the updated task definition.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-179":{"article":"A user might sometimes request the distribution's root URL instead of an object in the distribution. When this happens, specifying a default root object can help you to avoid exposing the contents of your web distribution.","impact":"When the root object for CloudFront distributions is not configured, the content of CloudFront is exposed to the internet.","report_fields":["ARN"],"remediation":"1. Upload the default root object to the origin that your distribution points to. The file can be any type supported by CloudFront.\n2. Confirm that the permissions for the object grant CloudFront at least read access.\n3. Update your distribution to refer to the default root object using the CloudFront console or the CloudFront API. To specify a default root object using the CloudFront console\n3.1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n3.2. In the list of distributions in the top pane, select the distribution to update.\n3.3. In the Distribution Details pane, on the General tab, choose Edit.\n3.4. In the Edit Distribution dialog box, in the Default Root Object field, enter the file name of the default root object.\n3.5. Enter only the object name, for example, index.html. Do not add a / before the object name.\n3.6. To save your changes, choose Yes, Edit. To update your configuration using the CloudFront API,  you specify a value for the DefaultRootObject element in your distribution. For information about using the CloudFront API to specify a default root object, see PUT Distribution Config in the Amazon CloudFront API Reference.\n4. Confirm that you have enabled the default root object by requesting your root URL. If your browser doesn't display the default root object, perform the following steps.\n4.1. Confirm that your distribution is fully deployed by viewing the status of your distribution in the CloudFront console.\n4.2. Repeat steps 2 and 3 to verify that you granted the correct permissions and that you correctly updated the configuration of your distribution to specify the default root object.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-293":{"article":"In AWS Backup, a Backup vault is a container that stores and organizes your backups. When creating a backup vault, you must specify the AWS Key Management Service (AWS KMS) encryption key that encrypts some of the backups placed in this vault. Encryption for other backups is managed by their source AWS services. By default backup vault is encrypted with AWS managed-key.\nKMS CMK customer-managed keys provide more granular control over your data-at-rest encryption/decryption process than AWS managed-keys.","impact":"Without a KMS CMK customer-managed keys, you do not have full and granular control over who can access encrypted backups in a Backup vault data.","report_fields":["BackupVaultArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt backup data using AWS KMS Customer Managed Keys, you have to recreate a Backup vault and enable encryption using KMS CMK.\n1. Open the Amazon Backup console at https://console.aws.amazon.com/backup/.\n2. In the navigation panel, choose Backup vaults.\n3. Click 'Create Backup vault' button from the dashboard top menu.\n4. On the 'Create Backup vault' page, perform the following:\n    4.1    Provide a unique name for a new vault.\n    4.2 Select the ID of the AWS Customer Managed Key (CMK) from the KMS encryption master key dropdown list.\n    4.3 (Optional) Within Backup vault tags section, configure the required tags for the new resource.\n    4.4 Click Create Backup vault.\n5. In the navigation panel select 'Backup plans'.\n6. Choose the backup plan associated with the non-compliant backup vault.\n7. In the Backup rules section, select the rule associated with the non-compliant backup vault and click Edit.\n8. On Edit 'Backup rule' configuration page, select the backup vault created earlier in the process from the Backup vault dropdown list, then click Save to apply the changes. The future backups taken using the selected backup plan will be encrypted with your own AWS KMS CMK configured for the newly created backup vault.","multiregional":false,"service":"AWS Backup"},"ecc-aws-198":{"article":"You should enable error logs for Elasticsearch domains and send those logs to CloudWatch Logs for retention and response. \nDomain error logs can assist with security and access audits, and can help to diagnose availability issues.","impact":"With disabled logs for Elasticsearch, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["ARN"],"remediation":"1. Open ElasticSearch at https://console.aws.amazon.com/esv3 \n2. Choose Domains. \n3. Select the domain you want to update. \n4. On the Logs tab, select a log type 'Error logs' and choose Enable. \n5. Create a CloudWatch log group, or choose an existing one. \n6. Choose an access policy that contains the appropriate permissions, or create a policy using the JSON that the console provides:\n  {{\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n        {{\n          \"Effect\": \"Allow\",\n          \"Principal\": {{\n              \"Service\": \"es.amazonaws.com\"\n          }},\n          \"Action\": [\n            \"logs:PutLogEvents\",\n            \"logs:CreateLogStream\"\n          ],\n          \"Resource\" : cw_log_group_arn\n        }}\n      ]\n  }}","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-205":{"article":"Amazon MariaDB database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"To enable and publish MariaDB logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo create a custom DB parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.\n\nTo create a new option group for MariaDB logging by using the console:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Option groups.\n3. Choose Create group.\n4. In the Create option group window, do the following:\n  4.1. For Name, type a name for the option group that is unique within your AWS account. The name can contain only letters, digits, and hyphens.\n  4.2. For Description, type a brief description of the option group. The description is used for display purposes.\n  4.3. For Engine, choose the mariadb engine.\n  4.4. For Major engine version, choose the major version of the DB engine that you want.\n5. To continue, choose Create.\n6. Choose the name of the option group you just created.\n7. Choose Add option.\n8. Choose MARIADB_AUDIT_PLUGIN from the Option name list.\n9. Set SERVER_AUDIT_EVENTS to CONNECT, QUERY, TABLE, QUERY_DDL, QUERY_DML, QUERY_DCL.\n10. Choose Add option.\n\nTo publish MariaDB logs to CloudWatch Logs from the AWS Management Console\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose all of the log files to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-016":{"article":"Ensure HARDWARE MFA is enabled for the 'root' account","impact":"Virtual MFA might not provide the same level of security as hardware MFA devices. Also, one layer of protection (a password) for a root account is not enough to prevent credentials from being compromised. The likelihood of a password being cracked and an account being compromised is much higher without MFA enabled.","report_fields":["account_id","account_name"],"remediation":"Perform the following to establish a hardware MFA for the root account:\n1. Sign in to the AWS Management Console and open the IAM console.\n2. Choose Dashboard, and under Security Status, expand Activate MFA on your root account.\n3. Choose Activate MFA\n4. In the wizard, choose a hardware MFA device and then choose Next Step.\n5. In the Serial Number box, enter the serial number found on the back of the MFA device.\n6. In the Authentication Code 1 box, enter the six-digit number displayed by the MFA device. You might need to press the button on the front of the device to display the number.\n7. Wait 30 seconds for the device to refresh the code and then enter the next six-digit number in the Authentication Code 2 box. You might need to press the button on the front of the device again to display the second number.\n8. Choose Next Step. \nThe MFA device is now associated with the AWS account. The next time you use your AWS account credentials to sign in, you must type a code from the hardware MFA device.","multiregional":true,"service":"AWS Account"},"ecc-aws-030":{"article":"This policy identifies security group rules that allow inbound traffic to the HTTP port (80) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Port 80 is a frequent target for attacks. Allowing unrestricted HTTP access can increase opportunities for malicious activity such as unauthorized access, denial-of-service (DoS) attacks and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group. \n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-006":{"article":"RDS instance should have Retention Policies for Backups configured to retain at least 7 days of backups.","impact":"A retention period of fewer than 7 days set for RDS database instances can result in data loss and the inability to recover it in the event of failure.","report_fields":["DBInstanceArn"],"remediation":"Configure your RDS backup retention policy to be at least 7 days.  \n1. Navigate to the the AWS console RDS dashboard. \n2. In the navigation pane, select Instances. \n3. Select the database instance you wish to configure. \n4. From the 'Instance actions' menu, select Modify. \n5. Scroll down to 'Backup Retention' options and set the retention period to at least 7 days. \n6. Click on Continue.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-142":{"article":"Amazon S3 provides Block public access (bucket settings) and Block public access (account settings) to help you manage public access to Amazon S3 resources. By default, S3 buckets and objects are created with public access disabled.\nHowever, an IAM principal with sufficient S3 permissions can enable public access at the bucket and/or object level. While enabled, Block public access (bucket settings) prevents an individual bucket and its contained objects from becoming publicly accessible. Similarly, Block public access (account settings) prevents all buckets and their contained objects from becoming publicly accessible across the entire account.","impact":"Everyone and everything on the Internet can connect to publicly accessible S3 buckets, which may increase the opportunity for malicious activities or loss of data confidentiality.","report_fields":["Name"],"remediation":"If utilizing Block Public Access (bucket settings):\nFrom Console:\n1. Login to AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/\n2. Select the Check box next to Bucket.\n3. Click on the 'Edit public access settings'.\n4. Click on 'Block all public access'\n5. Repeat for all the buckets in your AWS account that contain sensitive data.\nFrom Command Line:\n1. List all of the S3 Buckets 'aws s3 ls'\n2. Set the Block Public Access to true on that bucket\n'aws s3api put-public-access-block --bucket <name-of-bucket> --public-accessblock-configuration 'BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true''\n\nIf utilizing Block Public Access (account settings):\nFrom console:\nIf the output reads true for separate configuration settings, then it is set on the account.\n1. Login to the AWS Management Console and open the Amazon S3 console using https://console.aws.amazon.com/s3/\n2. Choose Block public access (account settings)\n3. Choose Edit to change the block public access settings for all the buckets in your AWS account\n4. Choose the settings you want to change and then choose Save. For details about each setting, pause on the icons.\n5. When you're asked for confirmation, enter Confirm. Then Click on Confirm to save your changes.\nFrom Command Line:\nTo set Block Public access settings for this account, run the following command:\n'aws s3control put-public-access-block --public-access-block-configuration BlockPublicAcls=true, IgnorePublicAcls=true, BlockPublicPolicy=true, RestrictPublicBuckets=true --account-id <value>'","multiregional":true,"service":"Amazon S3"},"ecc-aws-114":{"article":"This policy identifies Firewall rules attached to the cluster network which allow inbound traffic on all protocols from the public internet. Doing so, may allow a bad actor to bruteforce their way into the system and potentially get access to the entire cluster network.","impact":"Allowed inbound traffic on all protocols from the public internet enables attackers to use port scanners and other probing techniques to identify applications and services running on your EKS clusters and exploit their vulnerabilities.","report_fields":["arn"],"remediation":"1. Login to the AWS Console. \n2. Go to Security group. \n3. Go to the SG rules.\n4. Click on the reported Firewall rule. \n5. Click on Edit. \n6. Modify the rule. \n7. Click on Save.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-318":{"article":"Allowing an unlimited number of login attempts for a user connection can facilitate both brute-force login attacks and the occurrence of denial-of-service.","impact":"Unlimited number of login attempts for a user connection can facilitate brute-force login attacks.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"sec_max_failed_login_attempts\".\n6. Choose '3' or less\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-132":{"article":"Services and databases store data that may be sensitive, protected by law, subject to regulatory requirements or compliance standards. \nIt is highly recommended that access to data will be restricted to encrypted protocols. This rule detects network settings that may expose data via unencrypted protocol over the public internet or to an overly wide local scope.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["InstanceId","OwnerId"],"remediation":"1. Login to AWS Console at https://console.aws.amazon.com/ec2/.\n2. Go to 'Security groups'.\n3. Click on the reported security group.\n3. Go to the SG rules.\n4. Click on the reported rule.\n5. Click on 'Edit'.\n6. Modify the rule.\n7. Click on 'Save'.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-304":{"article":"You can grant additional permissions to an event bus by attaching a resource-based policy to it. With a resource-based policy, you can allow PutEvents, PutRule, and PutTargets API calls from another account. You can also use IAM conditions in the policy to grant permissions to an organization, apply tags, or filter events to only those from a specific rule or account. You can set a resource-based policy for an event bus when you create it or afterward.","impact":"A CloudWatch event bus with a policy that allows anyone to access your resource (\"Principal\": \"*\") can lead to data leaks and can allow unauthorized AWS users to send their CloudWatch events.","report_fields":["Arn"],"remediation":"Use the following procedure to modify the permissions for an existing event bus:\n1. Open the Amazon EventBridge console at https://console.aws.amazon.com/events/.\n2. In the left navigation pane, choose Event buses.\n3. In Name, choose the name of the event bus to manage permissions for.\n4. If a resource policy is attached to the event bus, the policy displays.\n5. Choose Manage permissions.\n6. Modify the policy that includes the permissions to grant unrestricted access to anyone (\"Principal\": \"*\") for the event bus by removing this permissions and granting new one for a specific AWS account, group, user etc.\n7. Choose Update.","multiregional":false,"service":"Amazon EventBridge"},"ecc-aws-387":{"article":"This policy identifies the Subnet that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["SubnetId","OwnerId","VpcId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/.\n2. Click on the 'Subnets'.\n3. Click on the required subnet.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-368":{"article":"Amazon FSx for Lustre can take an automatic daily backup of your file system. These automatic daily backups occur during the daily backup window that was established when you created the file system. \nAutomatic daily backups are kept for a certain period of time, known as a retention period.\nIt is highly recommended to use automatic daily backups set at least to 7 days for file systems that have any level of critical functionality associated with them.","impact":"A retention period of fewer than 7 days set for FSx file systems can result in data loss and the inability to recover it in the event of failure.","report_fields":["ResourceARN"],"remediation":"To update your Amazon FSx Lustre file systems configuration in order to set up a sufficient backup retention period, perform the following actions: \n1. Open the Amazon FSx for Lustre console at https://console.aws.amazon.com/fsx/.\n2. From the console dashboard, choose the name of the file system that reconfigure.\n3. In the navigation pane, choose Backups.\n4. In the Settings tab, click Update.\n5. Select 'Yes' to enable automatic backups.\n6. For 'Automatic Backup Retention' set number of days to retain a backup at least to 7 days.\n7. Click update.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-576":{"article":"Because dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts, these are more expensive than the ones running on the shared (default) environment. Dedicated EC2 instances must be regularly reviewed for cost optimization.\nUnless there is a business need to have EC2 instances with dedicated tenancy, you should change tenancy to less expensive type to avoid escalating costs. In some cases, Dedicated Hosts or Dedicated Instances can help you address compliance requirements or regulatory requirements and reduce costs by using your existing server-bound software licenses.","impact":"Keeping unnecessary EC2 instances with dedicated tenancy may lead to increased costs.","report_fields":["InstanceId","OwnerId"],"remediation":"If you decided to migrate a dedicated instance to default shared tenancy, follow the next steps:\n\nI. Create an AMI from an Amazon EC2 Instance:\n  1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n  2. In the navigation pane, choose 'Instances'.\n  3. Right-click the instance you want to use as the basis for your AMI, and choose 'Create Image' from the context menu.\n  4. In the 'Create Image' dialog box, type a unique name and description, and then choose 5'Create Image'. By default, Amazon EC2 shuts down the instance, takes snapshots of any attached volumes, creates and registers the AMI, and then reboots the instance. Choose 'No reboot' if you don't want your instance to be shut down.\n  Warning: If you choose 'No reboot', AWS can't guarantee the file system integrity of the created image.\n  5. It may take a few minutes for the AMI to be created. After it is created, it will appear in the 'AMIs' view in AWS Explorer. \n\nII. Relaunch EC2 instance with the correct tenancy type:\n  1. From the Amazon EC2 the navigation bar, choose 'AMIs'.\n  2. Select 'Owned by me' and find the AMI created at step no. I.\n  3. Select the AMI, and then choose 'Launch instance from AMI'.\n  4. In 'Advanced details' tab, for 'Tenancy' leave the default value 'Shared - Run a shared hardware instance'.\n  5. Make all the other necessary configurations and click 'Launch Instance'.\n  6. Choose 'View Instances' to check the status of your instance.\n\nIII. Terminate the instance with dedicated tenancy in order to stop incurring charges for the resource:\n  1. In the navigation pane, choose 'Instances'.\n  2. Select the instance, and choose 'Instance state', 'Terminate instance'.\n  3. Choose 'Terminate' when prompted for confirmation.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-194":{"article":"Enable deletion protection to protect your Load Balancers from deletion. If you enable deletion protection for your load balancer, you must disable delete protection before you can delete the load balancer.","impact":"It is quite possible to accidentally delete an ELB that was not supposed to be deleted. This can result in the loss of website or application availability.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, under LOAD BALANCING, choose 'Load Balancers'. \n3. Choose the load balancer. \n4. On the 'Description' tab, choose 'Edit' attributes. \n5. On the 'Edit load balancer attributes' page, select 'Enable for Delete Protection', and then choose 'Save'. \n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-394":{"article":"This policy identifies the AppFlow that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["flowArn"],"remediation":"Use AWS CLI to add tags to appflow:\naws appflow tag-resource --resource-arn arn --tags example=example","multiregional":false,"service":"Amazon AppFlow"},"ecc-aws-218":{"article":"Secrets Manager helps you improve the security posture of your organization. Secrets include database credentials, passwords, and third-party API keys. You can use Secrets Manager to store secrets centrally, encrypt secrets automatically, control access to secrets, and rotate secrets safely and automatically. \nSecrets Manager can rotate secrets. You can use rotation to replace long-term secrets with short-term ones. Rotating your secrets limits how long an unauthorized user can use a compromised secret. For this reason, you should rotate your secrets frequently.","impact":"Compromised secrets can lead to unauthorized access to different AWS resources. Rotating your secrets limits the amount of time an unauthorized user can use a compromised secret.","report_fields":["Name","ARN"],"remediation":"1. Open the Secrets Manager console at https://console.aws.amazon.com/secretsmanager/.\n2. To find the secret that requires rotating, enter the secret name in the search field.\n3. Choose the secret you want to rotate, which displays the secrets details page.\n4. Under Rotation configuration, choose Edit rotation.\n5. From Edit rotation configuration, choose Enable automatic rotation.\n6. For Select Rotation Interval, choose a rotation interval.\n7. Choose a Lambda function for rotation. For information about customizing your Lambda rotation function, see 'Understanding and customizing your Lambda rotation function' in the AWS Secrets Manager User Guide: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_how.html\n8. To configure the secret for rotation, choose Next.","multiregional":false,"service":"AWS Secrets Manager"},"ecc-aws-082":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms.\nIt is recommended that a metric filter and alarm be established for customer created CMKs that have changed state to disabled or scheduled deletion. Data encrypted with disabled or deleted keys will no longer be accessible.","impact":"Lack of monitoring and logging of CMK key disabling or deletion can lead to insufficient response time to detect accidental or intentional deletion of a customer key. This may result in a loss of all data encrypted using this key.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for disabled or scheduled for deletion CMK's and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<disable_or_delete_cmk_changes_metric>` --metrictransformations metricName= `<disable_or_delete_cmk_changes_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filter-pattern '{($.eventSource = kms.amazonaws.com) && (($.eventName=DisableKey)||($.eventName=ScheduleKeyDeletion)) }'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<disable_or_delete_cmk_changes_alarm>` --metric-name `<disable_or_delete_cmk_changes_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluationperiods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-275":{"article":"Amazon Aurora-MySQL database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nRDS cluster logging provides detailed records of requests made to RDS databases. RDS cluster logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the RDS DB cluster, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBClusterArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB cluster that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Audit, Error, General, SlowQuery log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify.\n\nTo enable and publish Aurora-MySQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom Cluster Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo create a custom cluster parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Cluster Parameter Group.\n6. In Group name, enter the name of the new DB cluster parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo apply a new DB cluster parameter group to an RDS DB cluster:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB cluster that you want to modify.\n4. Choose Modify. The Modify DB cluster page appears.\n5. Under Database options, change the DB cluster parameter group.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. Choose Modify CLuster to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-117":{"article":"API keys are string tokens that you provide to client application developers to grant access to your APIs. You can use API keys together with usage plans or Lambda authorizers to control access to your APIs. API Gateway can generate API keys on your behalf, or you can import them from a CSV file.","impact":"Without API keys, your APIs are vulnerable to unauthorized access.","report_fields":["id","path"],"remediation":"1. Sign in to the AWS Management Console and open the API Gateway console at https://console.aws.amazon.com/apigateway/. \n2. In the API Gateway main navigation pane, choose Resources.\n3. Under Resources, create a new method or choose the existing one.\n4. Choose Method Request.\n5. Under the Authorization Settings section, choose true for API Key Required.\n6. Select the checkmark icon to save the settings.\n7. Deploy or redeploy the API for the requirement to take effect.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-005":{"article":"Firewall and router configurations should be used to restrict connections between untrusted networks and any system components in the cloud environment.","impact":"When the VPC security group associated with an RDS instance allows unrestricted access (0.0.0.0/0 or ::/0), everyone and everything on the Internet can establish a connection to your database. This can increase the opportunity for malicious activities such as brute-force attacks, SQL injections, or DoS/DDoS attacks.","report_fields":["DBInstanceArn"],"remediation":"1. Login to the AWS Console at console.aws.amazon.com/vpc/ .\n2. Go to security group.\n3. Click on the reported Firewall rule ID.\n4. Click on Edit inbounds rules.\n5. Modify the rule\n6. Click on Save.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-034":{"article":"This policy identifies security group rules that allow inbound traffic to the NetBIOS-SSN port (139) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted NetBIOS access can increase opportunities for malicious activity such as man-in-the-middle attacks (MITM), Denial of Service (DoS) attacks or BadTunnel exploits.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-366":{"article":"You can configure the log levels that Amazon FSx logs; that is, whether Amazon FSx will log only error events, only warning events, or both error and warning events. Error and warning events can be logged from the following data repository operations: Automatic export, Data repository tasks. CloudWatch Logs allows you to store, view, and search audit event logs in the Amazon CloudWatch console, run queries on the logs using CloudWatch Logs Insights, and trigger CloudWatch alarms or Lambda functions.","impact":"With disabled monitoring and logging for FSx Lustre, it may be difficult to troubleshoot any issues that might happen with FSx Lustre.","report_fields":["ResourceARN"],"remediation":"To update your Amazon FSx Lustre file systems configuration in order to set up a logging, perform the following actions: \n1. Open the Amazon FSx for Lustre console at https://console.aws.amazon.com/fsx/.\n2. From the console dashboard, choose the name of the file system that you want to reconfigure.\n3. In the navigation pane at the bottom of the page, choose 'Monitoring' tab.\n4. In the 'Logging tab', click 'Update'.\n5. The 'Update logging configuration' window opens, select which types of logs to log.\n6. Choose CloudWatch Logs destination.\n7. Click on 'Update'. It may take a few minutes before changes will apply.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-305":{"article":"The same SQL query can be executed in multiple ways and still produce different results. The PostgreSQL planner/optimizer is responsible for creating an optimal execution plan for each query. The 'log_planner_stats' flag controls the inclusion of PostgreSQL planner performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_planner_stats' flag enables a crude profiling method for logging PostgreSQL planner performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_planner_stats\".\n6. Choose 0.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-183":{"article":"Backups help you to recover more quickly from a security incident. They also strengthen the resilience of your systems. DynamoDB point-in-time recovery (PITR) automates backups for DynamoDB tables. It reduces the time to recover from accidental delete or write operations. DynamoDB tables that have PITR enabled can be restored to any point in time in the last 35 days.","impact":"Without enabling DynamoDB point-in-time recovery, it is impossible to quickly recover data in the event of failure from accidental delete or write operations.","report_fields":["TableArn"],"remediation":"1. Open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n2. Choose the table that you want to work with, and then choose Backups.\n3. In the Point-in-time Recovery section, under Status, choose Enable.\n4. Choose Enable again to confirm the change.","multiregional":false,"service":"Amazon DynamoDB"},"ecc-aws-510":{"article":"Removing EFS without mount target will help you to avoid unexpected charges on your AWS bill and make cloud more organized.","impact":"Having an EFS without any mount targets can lead to wasted costs, unused resources, cluttered cloud environment.","report_fields":["FileSystemArn","OwnerId"],"remediation":"1. Login to the AWS Management Console and Navigate to Elastic File System (EFS) dashboard.\n2. Select File Systems from the left navigation panel.\n3. Click on the required file system.\n4. Click 'Attach' and then click on the 'Manage mount targets'.\n5. Choose 'VPC' and click on the 'Add mount target'.\n6. Choose 'Availability zone', 'Subnet ID', 'IP address', 'Security Groups'.\n7. Save.","multiregional":false,"service":"Amazon Elastic File System"},"ecc-aws-063":{"article":"Security groups provide stateful filtering of ingress and egress network traffic to AWS resources. It is recommended that no security group allows unrestricted ingress access to remote server administration ports, such as SSH to port 22 and RDP to port 3389.","impact":"Exposing port 3389 (RDP) to public access can increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromising.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home\n2. In the left pane, click on 'Security Groups'.\n3. For each security group, perform the following:\n4. Select the security group.\n5. Click on the 'Inbound Rules' tab.\n6. Click on the 'Edit inbound rules' button.\n7. Identify the rules to be edited or removed.\n8. Either:\n  A) Update the Source field to a range other than 0.0.0.0/0 or ::/0 \n  B) Click 'Delete' to remove the offending inbound rule.\n9. Click on 'Save rules'.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-292":{"article":"Application Load Balancers should have access logs enabled to capture detailed information about requests sent to load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues.","impact":"Disabled access logs for Application Load Balancers make it harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["EnvironmentArn"],"remediation":"To enable access logs Elastic Beanstalk need to be configured with Application load balancer:\n1. Open the Elastic Beanstalk console at console.aws.amazon.com/elasticbeanstalk/ and in the Regions list, select your AWS Region. \n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list. \n3. In the navigation pane, choose Configuration. \n4. In the 'Load balancer' category, choose Edit. \n5. Under Access log files, click on the 'Enabled'.\n6. Choose bucket with bucket policy that have permission for Beanstalk to write in bucket.\n7. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-127":{"article":"Encrypt Amazon RDS Cluster at rest by enabling the encryption option for your Amazon RDS DB cluster.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in RDS clusters.","report_fields":["DBClusterArn"],"remediation":"On the AWS RDS console, for each RDS cluster that failed the rule, enable encryption in the Details section. You can choose either to use your default AWS encryption key or supply an AWS KMS key.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-148":{"article":"Server access logging provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. For example, access log information can be useful for security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.","impact":"Lack of auditing and logging can lead to insufficient response time to threats from an attacker, which can result in data loss and degradation of the service.","report_fields":["Name"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/. \n2. From the Buckets list, choose the name of the bucket that you want to enable server access logging for. \n3. Choose Properties.\n4. In the Server access logging section, choose Edit. \n5. Under Server access logging, select Enable. \n6. For Target bucket, enter the name of the bucket that you want to receive the log record objects. \n7. The target bucket must be in the same Region as the source bucket and must not have a default retention period configuration.\n8. Choose Save changes.","multiregional":true,"service":"Amazon S3"},"ecc-aws-390":{"article":"This policy identifies the peering connections that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["VpcPeeringConnectionId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Click on the 'Peering connections'.\n3. Click on the required peering connection.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-099":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Routing tables are used to route network traffic between subnets and to network gateways. It is recommended that a metric filter and alarm be established for detecting changes made to route tables.","impact":"Lack of monitoring and logging of route table changes can result in insufficient response time to detect accidental or intentional modifications that may lead to uncontrolled network traffic.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = CreateRoute) || ($.eventName = CreateRouteTable) || ($.eventName = ReplaceRoute) || ($.eventName = ReplaceRouteTableAssociation) || ($.eventName = DeleteRouteTable) || ($.eventName = DeleteRoute) || ($.eventName = DisassociateRouteTable)}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-207":{"article":"Amazon Aurora database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB instance that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Audit, Error, General, SlowQuery log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify.\n\nTo enable and publish Aurora logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo create a custom DB parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Parameter Group.\n6. In Group name, enter the name of the new DB parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo apply a new DB parameter group or DB options group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify. The Modify DB Instance page appears.\n5. Under Database options, change the DB parameter group and DB options group as needed.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases. \n8. Choose Modify DB Instance to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-488":{"article":"Enabling CloudWatch retention establishes how long log events are kept in AWS CloudWatch Logs. Retention settings are assigned to CloudWatch log groups and the retention period assigned to a log group is applied to their log streams. Any data older than the current retention setting is deleted automatically.","impact":"Log data is stored in CloudWatch Logs indefinitely by default. This may incur high unexpected costs, especially when combined with other forms of logging.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon CloudWatch at https://console.aws.amazon.com/cloudwatch.\n2. Click on the 'Log groups'.\n3. Click on the required log group.\n4. Click on the 'ACtions' and ' Edit retention setting'.\n5. Choose required retention period.\n6. Save.","multiregional":false,"service":"Amazon CloudWatch"},"ecc-aws-398":{"article":"This policy identifies the Cloudtrail trails that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["TrailARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Cloudtrail at https://console.aws.amazon.com/cloudtrail.\n2. Click on the required trail.\n3. Click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-217":{"article":"Enhanced VPC routing forces all COPY and UNLOAD traffic between the cluster and data repositories to go through your VPC. You can then use VPC features such as security groups and network access control lists to secure network traffic.","impact":"Without enhanced VPC routing, you cannot be sure that traffic is not allowed from the Internet. Public access increases the opportunity for malicious activity such as hacking, denial-of-service attacks, and data loss.","report_fields":["ClusterIdentifier"],"remediation":"To update a cluster and enable enhanced VPC routing:\n1. Sign in to the AWS Management Console and open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. On the navigation menu, choose Clusters, then choose a cluster that you want to modify.\n3. Go to 'Properties' tab.\n4. At the 'Network and security settings' section, click 'Edit'.\n5. To enable Enhanced VPC routing for 'Enhanced VPC routing' select 'Turn on'.\n5. Choose 'Save changes'.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-026":{"article":"Ensure backups on the RDS instance are enabled.","impact":"Failure to enable backups for RDS instances can result in data loss and inability to recover it in the event of a user error on the source database, an unsuccessful major change to the instance database, or other issues.","report_fields":["DBInstanceArn"],"remediation":"1. Login to the AWS Console. \n2. Choose the RDS Service.\n3. Select your RDS DB without backups and click on its name.\n4. Click on the 'Backups' tab and choose retention period of 1 day and more.\n5. Click on 'Save'.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-391":{"article":"This policy identifies the VPC that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["VpcId","OwnerId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Click on the 'Your VPCs'.\n3. Click on the required vpc.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-421":{"article":"This policy identifies the Lambda functions that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["FunctionArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Lambda https://console.aws.amazon.com/lambda/ .\n2. In the navigation pane click on the 'Functions'.\n3. Click on the required function.\n4. Click on the 'Configuration' and then 'Tags'.\n5. Click 'Manage tags'.\n6. Click 'Add new tag'.\n7. Enter values.\n8. Save.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-177":{"article":"You can use API Gateway to generate an SSL certificate and then use its public key in the backend to verify that HTTP requests to your backend system are from API Gateway. This allows your HTTP backend to control and accept only requests that originate from Amazon API Gateway, even if the backend is publicly accessible.","impact":"Without SSL certificates, the HTTP backend will accept all requests, not just those that originate from Amazon API Gateway.","report_fields":["stageName","restApiId"],"remediation":"Step 1. Generate a client certificate using the API Gateway console\n1. Open the API Gateway console at https://console.aws.amazon.com/apigateway/.\n2. Choose a REST API.\n3. In the main navigation pane, choose Client Certificates.\n4. From the Client Certificates pane, choose Generate Client Certificate.\n5. Optionally, for Edit, choose to add a descriptive title for the generated certificate and choose Save to save the description. API Gateway generates a new certificate and returns the new certificate GUID, along with the PEM-encoded public key.\n\nStep 2. Configure an API to use SSL certificates\n1. In the API Gateway console, create or open an API for which you want to use the client certificate. Make sure that the API has been deployed to a stage.\n2. Choose Stages under the selected API and then choose a stage.\n3. In the Stage Editor panel, select a certificate under the Client Certificate section.\n4. To save the settings, choose Save Changes. If the API has been deployed previously in the API Gateway console, you'll need to redeploy it for the changes to take effect.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-341":{"article":"SageMaker Model containers are internet-enabled by default. This allows containers to access external services and resources on the public internet as part of your training and inference workloads.","impact":"Disabled network isolation could provide an avenue for unauthorized access to your data. For example, a malicious user or code that you accidentally install on the container (in the form of a publicly available source code library) could access your data and transfer it to a remote host.","report_fields":["ModelArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create new model with enabled network isolation:\n1. Open the Sagemaker console at https://console.aws.amazon.com/sagemaker/\n2. In the navigation pane, under Inference choose Models.\n3. Choose create model.\n4. Under Network, choose Enable network isolation\n5. Create model.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-228":{"article":"Amazon ECR Tag Immutability enables customers to rely on the descriptive tags of an image as a reliable mechanism to track and uniquely identify images. Prior to this enhancement, tags could be overwritten requiring developers to use the Image SHA to know which image was being deployed. By setting an image tag as immutable, developers can now rely on the tag to correlate the deployed image version with the build that produced the image.","impact":"Disabled tag immutability can result in image tags in the repository being overwritten, for this reason users cannot rely on the descriptive tags of an image as a mechanism to track and uniquely identify images.","report_fields":["repositoryArn"],"remediation":"To change the policy using the AWS Console, follow these steps:\n1. Log in to the AWS Management Console at https://console.aws.amazon.com/.\n2. Open the Amazon ECR console.\n3. Select a repository using the radio button.\n4. Click Edit.\n5. Enable the Tag immutability toggle.","multiregional":false,"service":"Amazon Elastic Container Registry"},"ecc-aws-467":{"article":"Multi-AZ file systems support all the availability and durability features of Single-AZ file systems. In addition, they are designed to provide continuous availability to data, even during file system maintenance, infrastructure component replacement, and when an Availability Zone is unavailable. In a Multi-AZ deployment, Amazon FSx automatically provisions and maintains a standby file server in a different Availability Zone. Any changes written to disk in your file system are synchronously replicated across Availability Zones to the standby. If there is planned file system maintenance or unplanned service disruption, Amazon FSx automatically fails over to the secondary file server, allowing you to continue accessing your data without manual intervention.\nMulti-AZ file systems are recommended for most production workloads that require high availability to shared Windows file data. Single-AZ file systems offer a lower price point for workloads that don\u2019t require the high availability of a Multi-AZ solution and that can recover from the most recent file system backup if data is lost.","impact":"When an FSx file system is not configured to use multiple Availability Zones and Availability Zone in which file system deploy becomes unavailable, your data also can become unavailable. Using Single-AZ does not provide data resiliency and is not recommended for use cases such as business-critical production workloads that require high availability.","report_fields":["ResourceARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create FSx system with enabled Multi-AZ, perform the following actions: \n1. Open the Amazon FSx console at https://console.aws.amazon.com/fsx/.\n2. On the dashboard, choose 'Create file system 'to start the file system creation wizard.\n3. On the 'Select file system type' page, choose ' FSx for Windows File Server', and then choose 'Next'. The 'Create file system' page appears.\n4. For 'File system name - optional', enter a name for your file system. \n5. For Deployment type choose 'Multi-AZ'.\n6. Set up all other necessary configurations.\n7. Choose 'Next'.\n8. Review the file system configuration shown on the 'Create file system' page. For your reference, note which file system settings you can modify after the file system is created.\n9. Choose 'Create file system'.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-438":{"article":"'Allow all' a legacy permissions mode that enables access control with API-level granularity for ledgers. 'Standard' a permissions mode that enables access control with finer granularity for ledgers, tables, and PartiQL commands.\nBy default, this mode denies all requests to run any PartiQL commands on any tables in this ledger. To allow PartiQL commands, you must create IAM permissions policies for specific table resources and PartiQL actions, in addition to the SendCommand API permission for the ledger.","impact":"All users with SendCommand API permission can run all PartiQL commands on any table.","report_fields":["EnvironmentArn"],"remediation":"1. Open the AWS QLDB console at https://console.aws.amazon.com/qldb.\n2. Click on the required ledger.\n3. Click on the 'Edit'.\n4. Choose 'Standard access'.","multiregional":false,"service":"Amazon QLDB"},"ecc-aws-440":{"article":"To monitor your AWS AppSync GraphQL API and help debug issues related to requests, you can turn on logging to Amazon CloudWatch Logs.","impact":"With disabled logging of API Gateway, it may be difficult to troubleshoot any issues that might happen with APIs or find who accessed API and how they accessed it.","report_fields":["arn","name"],"remediation":"To turn on automatic logging on a GraphQL API, use the AWS AppSync console.\n1. Sign in to the AWS AppSync console.\n2. On the APIs page, choose the name of a GraphQL API.\n3. On your API's homepage, in the navigation pane, choose 'Settings'.\n4. Under 'Logging', do the following:\n  a. Turn on 'Enable Logs'.\n  b. (Optional) For detailed request-level logging, select the check box under 'Include verbose content'.\n  c. Optional) Under 'Field resolver log level', choose your preferred field-level logging level (None, Error, or All).\n  d. Under 'Create or use an existing role', choose 'New role' to create a new AWS Identity and Access Management (IAM) that allows AWS AppSync to write logs to CloudWatch. Or, choose 'Existing role' to select the Amazon Resource Name (ARN) of an existing IAM role in your AWS account.\n5. Choose 'Save'.\nIf you choose to use an existing IAM role, the role must grant AWS AppSync the required permissions to write logs to CloudWatch. To configure this manually, you must provide a service role ARN so that AWS AppSync can assume the role when writing the logs.","multiregional":false,"service":"AWS AppSync"},"ecc-aws-571":{"article":"Identifying unused Amazon RDS instances will lower the cost of your AWS bill. An RDS instance is considered to be unused when it is stopped for more than 3 days. \nUnless there is a business need to retain unused RDS instances, you should remove them to maintain an accurate inventory of system components.\nWhile your DB instance is stopped, you are charged for provisioned storage (including Provisioned IOPS). You're also charged for backup storage, including manual snapshots and automated backups within your specified retention window. However, you're not charged for DB instance hours.\nIf you don't manually start your DB instance after it is stopped for seven consecutive days, RDS automatically starts your DB instance for you. This way, it doesn't fall behind any required maintenance updates.","impact":"Keeping unused RDS instances can result in escalating costs and cluttered AWS accounts.","report_fields":["DBInstanceArn"],"remediation":"To start a DB instance:\n  1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n  2. In the navigation pane, choose 'Databases', and then choose the DB instance that you want to start.\n  3. For 'Actions', choose 'Start'.\n\nTo delete a DB instance:\n  1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n  2. In the navigation pane, choose 'Databases', and then choose the DB instance that you want to delete.\n  3. For 'Actions', choose 'Delete'.\n  4. To create a final DB snapshot for the DB instance, choose 'Create final snapshot?'.\n  5. If you chose to create a final snapshot, enter the 'Final snapshot name'.\n  6. To retain automated backups, choose 'Retain automated backups'.\n  7. Enter 'delete me' in the box.\n  8. Choose 'Delete'.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-276":{"article":"Amazon Aurora-PostgreSQL database engine logs (Postgresql) should be enabled and sent to CloudWatch.\nRDS cluster logging provides detailed records of requests made to RDS. This logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the RDS DB cluster, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBClusterArn"],"remediation":"To publish PostgreSQL logs to CloudWatch Logs from the AWS Management Console\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose all of the log files to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify DB Instance.\n\nTo enable and publish Aurora-PostgreSQL logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom DB Cluster Parameter Group:\n- log_statement=all\n- log_min_duration_statement='minimum query duration (ms) to log'\n\nTo create a custom DB cluster parameter group\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Cluster Parameter Group.\n6. In Group name, enter the name of the new DB cluster parameter group.\n7. In Description, enter a description for the new DB cluster parameter group.\n8. Choose Create.\n\nTo apply a new DB cluster parameter group to an RDS DB instance:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB cluster that you want to modify.\n4. Choose Modify. The Modify DB cluster page appears.\n5. Under Database options, change the DB cluster parameter group.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. Choose Modify CLuster to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-344":{"article":"Route53 conveniently allows you to register and manage domains alongside the rest of your AWS resources.  When a domain is registered in Route53, it will automatically have automatic renewal enabled. In the case where automatic renewal is not enabled, you must renew it manually.\nTo avoid the possibility of a domain expiring accidentally, you can enable automatic renewal on the domain, if it's not done yet. Avoid waiting until the last minute to renew your domain. Some TLDs disallow renewals as much as 25 days before the expiration date, and it can take a day or more to process a renewal.","impact":"If you do not renew your domain name, it will become expired. An expired Amazon Route53 domain can cause website or application downtime or failure. An expired domain could be taken over by a malicious individual or deleted by the domain registrar and it will become available for others to register.","report_fields":["DomainName"],"remediation":"If automatic renewal is enabled, here's what happens:\n  - 45 days before expiration\n    AWS sends an email to the registrant contact that tells you that automatic renewal is currently enabled and gives instructions about how to disable it. Keep your registrant contact email address current so you don't miss this email.\n  - 35 or 30 days before expiration\n    For all domains except .com.ar, .com.br, and .jp domains, AWS renews domain registration 35 days before the expiration date so AWS has time to resolve any issues with your renewal before the domain name expires.\n    The registries for .com.ar, .com.br, and .jp domains require that AWS renews the domains no more than 30 days before expiration. You'll get a renewal email from Gandi, our registrar associate, 30 days before expiration, which is the same day that AWS renews your domain if you have automatic renewal enabled.\n\nIf automatic renewal is disabled, here's what happens as the expiration date for a domain name approaches:\n  - 45 days before expiration\n    AWS sends an email to the registrant contact for the domain that tells you that automatic renewal is currently disabled and gives instructions about how to enable it. Keep your registrant contact email address current so you don't miss this email.\n  - 30 days and 7 days before expiration\n    If automatic renewal is disabled for the domain, ICANN, the governing body for domain registration, requires the registrar to send you an email.\n    If you enable automatic renewal less than 30 days before expiration, and the renewal period has not passed, AWS renews the domain within 24 hours.\n\nWhen you want to change whether Amazon Route53 automatically renews registration for a domain shortly before the expiration date, or you want to see the current setting for automatic renewal, perform the following procedure.\nTo enable automatic renewal for a domain:\n1. Sign in to the AWS Management Console and open the Route53 console at https://console.aws.amazon.com/route53/.\n2. In the navigation pane, choose 'Registered Domains'.\n3. Choose the name of the domain that you want to update.\n4. Choose 'Enable' for 'Auto renew'.\n5. If you encounter issues while enabling automatic renewal, you can contact AWS Support for free. \n   For more information, see 'Contacting AWS Support about domain registration issues': https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-contact-support.html.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-319":{"article":"Bad packets received from the client can potentially indicate packet-based attacks on the system, such as \"TCP SYN Flood\" or \"Smurf\" attacks, which could result in a denial-of-service condition.","impact":"Bad packets received from the client can potentially indicate packet-based attacks on the system, such as \"TCP SYN Flood\" or \"Smurf\" attacks, which could result in a denial-of-service condition.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"sec_protocol_error_further_action\".\n6. Type '(DROP,3)'\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-081":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for failed console authentication attempts. Monitoring failed console logins may decrease lead time to detect an attempt to bruteforce a credential, which may provide an indicator, such as source IP, that can be used in other event correlations.","impact":"Lack of monitoring and logging of Console authentication failures can lead to insufficient response time to detect an attempt of brute-forcing a user credential or no response at all.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = ConsoleLogin) && ($.errorMessage = \"Failed authentication\")}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-380":{"article":"This policy identifies the EIP that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["AllocationId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Under 'Virtual private cloud' click on the 'Elastic IPs'.\n3. Open required eip.\n4. Click on 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-486":{"article":"This rule checks if the deployment group for Lambda Compute Platform is not using the default deployment configuration 'CodeDeployDefault.LambdaAllAtOnce'. This deployment configuration shifts all traffic to the updated Lambda functions at once.","impact":"In some cases, it may be not acceptable to shifts all traffic to the updated Lambda functions at once, as this can lead to interruptions caused by changing application versions.","report_fields":["applicationName","deploymentGroupId","deploymentGroupName"],"remediation":"To remediate this issue you can use the predefined configurations available for AWS Lambda deployments: \n  1. Navigate to CodeDeploy Applications https://console.aws.amazon.com/codesuite/codedeploy/applications.\n  2. Select an application to which you need to update deployment group.\n  3. Select 'Deployment groups' tab.\n  4. Select deployment group that you want to modify.\n  5. Click 'Edit' at top right corner.\n  6. Select one of the predefined configurations available\n    - CodeDeployDefault.LambdaCanary10Percent5Minutes - Shifts 10 percent of traffic in the first increment. The remaining 90 percent is deployed five minutes later.\n    - CodeDeployDefault.LambdaCanary10Percent10Minutes - Shifts 10 percent of traffic in the first increment. The remaining 90 percent is deployed 10 minutes later.\n    - CodeDeployDefault.LambdaCanary10Percent15Minutes - Shifts 10 percent of traffic in the first increment. The remaining 90 percent is deployed 15 minutes later.\n    - CodeDeployDefault.LambdaCanary10Percent30Minutes - Shifts 10 percent of traffic in the first increment. The remaining 90 percent is deployed 30 minutes later.\n    - CodeDeployDefault.LambdaLinear10PercentEvery1Minute - Shifts 10 percent of traffic every minute until all traffic is shifted.\n    - CodeDeployDefault.LambdaLinear10PercentEvery2Minutes - Shifts 10 percent of traffic every two minutes until all traffic is shifted.\n    - CodeDeployDefault.LambdaLinear10PercentEvery3Minutes - Shifts 10 percent of traffic every three minutes until all traffic is shifted.\n    - CodeDeployDefault.LambdaLinear10PercentEvery10Minutes - Shifts 10 percent of traffic every 10 minutes until all traffic is shifted.\n  7. Click 'Save changes'.\n\nAnother way to remediate this issue is to create a custom configurations:\n  1. Navigate to the CodeDeploy Deployment configurations https://console.aws.amazon.com/codesuite/codedeploy/deployment-configs\n  2. Click at the top right corner on the 'Create deployment configuration' button.\n  3. Enter name for the deployment configuration.\n  4. For 'Compute platform' select 'AWS Lambda'.\n  5. For 'Type' select the one that you need and configure the rest according to the needs of the project.\n  6. After you finished, click 'Create deployment configuration'.\n  7. Navigate to CodeDeploy Applications https://console.aws.amazon.com/codesuite/codedeploy/applications.\n  8. Select an application to which you need to update deployment group.\n  9. Select 'Deployment groups' tab.\n  10. Select deployment group that you want to modify.\n  11. Click 'Edit' at top right corner.\n  12. Select the deployment configurations that was created at the step 6.\n  13. Click 'Save changes'.","multiregional":false,"service":"AWS CodeDeploy"},"ecc-aws-520":{"article":"The Time To Live (TTL) field in the IP packet is reduced by one on every hop. This reduction can be used to ensure that the packet does not travel outside EC2. IMDSv2 protects EC2 instances that may have been misconfigured as open routers, layer 3 firewalls, VPNs, tunnels, or NAT devices, which prevents unauthorized users from retrieving metadata. With IMDSv2, the PUT response that contains the secret token cannot travel outside the instance because the default metadata response hop limit is set to 1.","impact":"If hop limit value is greater than 1, the token can leave the EC2 instance which can lead to unauthorized access to metadata.","report_fields":["LaunchConfigurationName"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo limit launch configuration hops, create new launch configuration:\n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2/.\n2. Under Auto Scaling, click on the Launch Configurations.\n3. Click on the Create launch configuration.\n4. Additional configuration, click on the Advanced details.\n5. Under \"Metadata response hop limit\" type \"1\".\n6. Create launch configuration.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-271":{"article":"Redis auth enables Redis to require a password before allowing clients to run commands, thereby improving data security. \nBy default redis does not require a password to run commands.","impact":"Without Redis auth, clients can run commands without a password, which makes the cluster more vulnerable.","report_fields":["ARN"],"remediation":"If encryption in transit is already enabled on your cluster follow next steps:\n1. Sign in to the AWS Management Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the list in the upper-right corner, choose the AWS Region where the cluster that you want to modify is located.\n3. In the navigation pane, choose 'Redis clusters'.\n4. In the list of clusters, for the cluster that you want to modify, choose its name.\n5. For 'Actions', choose 'Modify'. The Modify Cluster window appears.\n6. If 'Transit encryption mode' is 'Preferred' change it to 'Required' and apply changes before enabling Auth. Then repeat steps 1-5 and move on to the next step.\n   If it is already set to 'Required' skip this step.\n7. For 'Access Control' option, choose 'Redis AUTH Default User' and set a new token.\n8. Click 'Preview changes'.\n9. If you want to perform the update right away, choose 'Apply immediately'. If Apply immediately is not chosen, the update process is performed during the cluster's next maintenance window.\n10. Choose 'Modify'.\n\nIf encryption in transit is disabled on your cluster follow next steps:\nStep 1: Set your Transit encryption mode to Preferred\n  1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2. Choose 'Redis clusters' from the ElastiCache 'Resources' listed on the navigation pane, present on the left hand.\n  3. Choose the Redis cluster you want to update.\n  4. Choose the 'Actions' dropdown, then choose 'Modify'.\n  5. Choose 'Enable' under 'Encryption in transit' in the 'Security' section.\n  6. Choose 'Preferred' as the 'Transit encryption mode'.\n  7. Choose 'Preview changes' and save your changes.\n\nAfter you migrate all your Redis clients to use encrypted connections:\nStep 2: Set your Transit encryption mode to Required\n  1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2. Choose 'Redis clusters' from the ElastiCache 'Resources' listed on the navigation pane, present on the left hand.\n  3. Choose the Redis cluster you want to update.\n  4. Choose the 'Actions' dropdown, then choose 'Modify'.\n  5. Choose 'Required' as the 'Transit encryption mode', in the 'Security' section.\n  6. For 'Access Control' option, choose 'Redis AUTH Default User' and set a new token.\n  7. Choose 'Preview changes' and save your changes.\n\nStep 3: Enable Auth for cluster\n  1. Sign in to the AWS Management Console and open the Amazon ElastiCache console at https://console.aws.amazon.com/elasticache/.\n  2. Choose 'Redis clusters' from the ElastiCache 'Resources' listed on the navigation pane, present on the left hand.\n  3. Choose the Redis cluster you want to update.\n  4. Choose the 'Actions' dropdown, then choose 'Modify'.\n  5. In the 'Security' section for 'Access Control' option, choose 'Redis AUTH Default User' and set a new token.\n  6. Choose 'Preview changes' and save your changes.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-309":{"article":"Ensure that AWS Config does not have status \"FAILURE\". \nThe most common case is when AWS Config recorder fails to deliver logs to S3 bucket, the service is unable to send the recorded information to the designated bucket, therefore you lose the ability to audit the configuration changes made within your AWS account. This usually happens due to misconfigurations of access policies defined for the associated IAM role.\nAWS Config delivers configuration snapshots and configuration history files to S3 bucket that is configured in a delivery channel.","impact":"When AWS Config recorder fails, it means there are some misconfigurations that impact the AWS Config ability to work properly. \nWhen AWS Config recorder fails to deliver logs to S3 bucket, the service is unable to send the recorded information to the designated bucket, therefore you lose the ability to audit the configuration changes made within your AWS account.","report_fields":["name","roleARN"],"remediation":"Follow this AWS Guide on how to troubleshoot AWS Config console error messages: https://aws.amazon.com/premiumsupport/knowledge-center/config-console-error/.","multiregional":false,"service":"AWS Config"},"ecc-aws-084":{"article":"S3 Bucket Access Logging generates a log that contains access records for each request made to your S3 bucket. An access log record contains details about the request, such as the request type, the resources specified in the request worked, and the time and date the request was processed. \nIt is recommended that bucket access logging be enabled on the CloudTrail S3 bucket. By enabling S3 bucket logging on target S3 buckets, it is possible to capture all events which may affect objects within target buckets. Configuring logs to be placed in a separate bucket allows access to log information which can be useful in security and incident response workflows.","impact":"Lack of monitoring and logging of requests to an S3 bucket can lead to undetected changes made to the S3 bucket.","report_fields":["TrailARN"],"remediation":"1. Sign in to the AWS Management Console and open the S3 console at https://console.aws.amazon.com/s3.\n2. Under All Buckets, click on the target S3 bucket.\n3. Click on Properties in the top right of the console.\n4. Under Bucket  <s3_bucket_for_cloudtrail> , click on Logging.\n5. Configure bucket logging: \n  5.1. Click on Enabled checkbox.\n  5.2. Select Target Bucket from the list.\n  5.3. Enter a Target Prefix \n6. Click Save.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-023":{"article":"Logging is crucial for detecting intrusion attempts, monitoring access, as well as for helping debug access errors and system failures during and after the fact. Almost every compliance framework will mandate logging to be enabled.","impact":"Disabled access logs for CLB make it harder to analyze statistics, diagnose issues or detect different types of attacks, as well as retain data for regulatory or legal purposes.","report_fields":["LoadBalancerArn"],"remediation":"To enable access logging using the console, do the following:\n1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n2. In the navigation pane, choose 'Load Balancers'.  \n3. Select your load balancer.  \n4. On the 'Description' tab, choose 'Edit attributes'.  \n5. On the 'Edit load balancer attributes' page, do the following:  \n  - For 'Access logs', select 'Enable'.  \n  - For 'S3 location', type the name of your S3 bucket, including any prefix (for example, my-loadbalancer-logs/my-app). You can specify the name of an existing bucket or a name for a new bucket. If you specify an existing bucket, be sure that you own this bucket and that you configured the required bucket policy. \n  - (Optional) If the bucket does not exist, choose 'Create this location for me'. You must specify a name that is unique across all existing bucket names in Amazon S3 and follows the DNS naming conventions. \n  - Choose 'Save'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-259":{"article":"Ensure that your Amazon Elastic MapReduce (EMR) clusters are provisioned using the AWS EC2-VPC platform instead of EC2-Classic platform (outdated from 2013.12.04) for better flexibility and control over security, better traffic routing and availability.\nWhen you launch an Amazon EMR cluster within a VPC, you can launch it within either a public, private, or shared subnet. There are slight but notable differences in configuration, depending on the subnet type you choose for a cluster.","impact":"Launching and managing AWS EMR clusters using EC2-Classic (no VPC) platform instead of EC2-VPC can bring multiple disadvantages such as worse networking infrastructure (you will not be able to take advantage of network isolation, private subnets and private IP addresses), much less flexible control over access security and no access to newer and powerful EC2 instance types (C4, M4, R4, etc) for your clusters.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\n1. Login to the AWS Management Console.\n2. Navigate to EMR dashboard at https://console.aws.amazon.com/elasticmapreduce/.\n3. In the navigation panel, under Amazon EMR, click Clusters to access your AWS EMR clusters page.\n4. Select the EMR cluster that you want to relaunch into a VPC then click on the Clone button from the dashboard top menu.\n5. Inside the Cloning dialog box, choose Yes to include the steps from the original cluster in the cloned cluster or No to clone the original cluster's configuration without including any of the existing steps. Click Clone to start the cloning process.\n6. On the Create Cluster page, select Step 1: Software and Steps from the left navigation panel and configure the software that will be installed on the new cluster. Click Next to continue the setup process.\n7. On the Hardware Configuration panel, select the VPC network and the EC2 subnet where the new EMR cluster instances will be provisioned, set the EBS volume size for the root device and configure the cluster nodes (instances) as needed. Click the Next button until your reach Step 4: Security page, without changing any other configuration attributes.\n8. Review the security options, then click Create Cluster to provision your new Amazon EMR cluster.\n9. Once you have moved the existing data and verified that your new EMR cluster is working 100% within the selected VPC network, terminate the original cluster in order to stop incurring charges for it.","multiregional":false,"service":"Amazon EMR"},"ecc-aws-392":{"article":"This policy identifies the VPC endpoint that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["VpcEndpointId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc.\n2. Click on the 'Endpoints'.\n3. Click on the required endpoint.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-475":{"article":"Enabling Cross-Zone Load Balancing makes it easier to deploy and manage applications that run across multiple subnets in different Availability Zones. This would also guarantee better fault tolerance and more consistent traffic flow. If one of the availability zones registered with the ELB fails (as result of network outage or power loss), the load balancer with the Cross-Zone Load Balancing activated would act as a traffic guard, stopping any request being sent to the unhealthy zone and routing it to the other zone(s).","impact":"When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. If the number of registered targets is not same across the Availability Zones, traffic wont be distributed evenly and the instances in one zone may end up over utilized compared to the instances in another zone.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Load balancers. \n3. Choose an Classic Load Balancer. \n4. On the Description tab, choose Change cross-zone load balancing setting. \n5. On the Configure Cross-Zone Load Balancing page, select Enable.\n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-278":{"article":"Access Analyzer generates a finding for each instance of a resource-based policy that grants access to a resource within your zone of trust to a principal that is not within your zone of trust. The status of all findings remains Active until you archive them or remove the access that generated the finding. When you remove the access, the finding status is updated to Resolved. You should review all of the findings in your account to determine whether the sharing is expected and approved.","impact":"Do not resolved findings pose a security risk that leads to open unintended access to your resources and data.","report_fields":["account_id","account_name"],"remediation":"Review any findings to determine whether the access identified in the finding is intentional or unintentional.\n\nTo review findings:\n1. Open the IAM console at https://console.aws.amazon.com/iam/.\n2. Choose Access analyzer. \n    All Active findings that were generated are displayed for the analyzer.\n\nWhen you get a finding for access to a resource that is intentional, you can archive the finding.\nTo archive findings from the Findings page:\n1. Select the check box next to one or more findings to archive.\n2. Choose Archive. A confirmation is displayed at the top of the screen.\n\nTo resolve findings generated from access that you did not intend to allow, modify the policy statement to remove the permissions that allow access to the identified resource.\nAfter you make a change to resolve a finding, if the resource is no longer shared outside of your zone of trust, the status of the finding is changed to Resolved. The finding is no longer displayed in the Active findings table, and instead is displayed in the Resolved findings table.\nIf the changes you made resulted in the resource being shared outside of your zone of trust, but in a different way, such as with a different principal or for a different permission, Access Analyzer generates a new Active finding.","multiregional":false,"service":"AWS Identity and Access Management Access Analyzer"},"ecc-aws-360":{"article":"Ensure that CloudWatch logs encryption is enabled for your Amazon ECS Exec in order to meet regulatory requirements and prevent unauthorized users from getting access to the logging data published to CloudWatch or S3. Amazon ECS provides a default configuration for logging commands run using ECS Exec by sending logs to CloudWatch Logs using the awslogs log driver that's configured in your task definition.","impact":"Disabled encryption of logs allows a user to get unauthorized access to sensitive data.","report_fields":["clusterArn"],"remediation":"To enable encryption for logging data published to CloudWatch, perform the following actions:\nExecute command :\naws ecs update-cluster \\\n    --cluster $CLUSTER_NAME \\\n    --region $AWS_REGION \\\n    --configuration executeCommandConfiguration=\"{{logging=OVERRIDE,\\\n                                                kmsKeyId=$KMS_KEY_ARN,\\\n                                                logConfiguration={{cloudWatchLogGroupName=\"$CLOUDWATCH_GROUP_NAME\",\\\n                                                                cloudWatchEncryptionEnabled=true}}\\\n                                                                }}\"\n\nTo enable encryption for logging data published to s3, perform the following actions:\nExecute command:\naws ecs update-cluster \\\n    --cluster $CLUSTER_NAME \\\n    --region $AWS_REGION \\\n    --configuration executeCommandConfiguration=\"{{logging=OVERRIDE,\\\n                                                logConfiguration={{s3EncryptionEnabled=true,\\\n                                                                s3BucketName=$ECS_EXEC_BUCKET_NAME,\\\n                                                                s3KeyPrefix=$PREFIX}} \\\n                                                                }}\"\nIn addition, the task role will need to have IAM permissions to log the output to S3 and/or CloudWatch and decrypt the data with KMS key.\nTo update your task role follow the instructions that are described in the 'IAM permissions required for Amazon CloudWatch Logs or Amazon S3 Logging' and 'IAM permissions required for encryption using your own AWS KMS key (KMS key)' sections in Amazon ECS User Guide: https://docs.aws.amazon.com/AmazonECS/latest/userguide/ecs-exec.html.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-294":{"article":"Monitoring is an essential part of maintaining the availability, reliability, and performance of your Amazon Elastic Beanstalk applications. When a notable event occurs within your application environment, Elastic Beanstalk sends a message to the configured email address via Amazon SNS to keep you up-to-date on everything that's going on within your environment. Notable events include environment creation errors, environment changes, and instance health events.","impact":"Not enabling Beanstalk notifications can lead to a slow response to a environment changes, and instance health events.","report_fields":["EnvironmentArn"],"remediation":"To enable notifications perform following steps:\n1. Login to the AWS Management Console and open the Amazon Elastic Beanstalk console using https://console.aws.amazon.com/elasticbeanstalk/\n2. Click on the required environment.\n3. Click on Configuration. \n4. Click Edit on 'Notifications' parameter.\n5. Provide email.\n6. Save changes.\n7. Open email and confirm subscription.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-022":{"article":"Regularly managing and removing outdated snapshots is crucial to maintain efficiency and cost-effectiveness in your backup strategy.","impact":"Keeping snapshots more than 30 days can result in escalating storage costs, cluttered AWS accounts, and prolonged data restoration times.","report_fields":["SnapshotId","OwnerId"],"remediation":"1. Navigate to console.aws.amazon.com/ec2/. \n2. Select Snapshots under Elastic Block Store. \n3. Find snapshots that were created more than 14 days ago.\n4. Update snapshots:\n4.1. Press Create snapshot.\n4.2. Select Volume in the Resource  type.\n4.3. Choose the volume with the expired period  under the Volume tab.\n4.4. Press Create snapshot.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-150":{"article":"Encrypting data at rest reduces the risk of data stored on disk being accessed by a user not authenticated to AWS. It adds another set of access controls to limit unauthorized users ability to access the data. For example, API permissions are required to decrypt the data before it can be read. API Gateway REST API caches should be encrypted at rest to ensure an added layer of security.","impact":"Disabled encryption allows users not authenticated in AWS or unauthorized users to gain access to API Gateway containing sensitive data.","report_fields":["stageName","restApiId"],"remediation":"From Console:\n1. Login to AWS Management Console and open the API Gateway console using https://console.aws.amazon.com/apigateway/ \n2. Choose the API. \n3. Choose Stages. \n4. In the Stages list for the API, choose the stage to add caching to. \n5. Choose Settings. \n6. Choose Enable API cache. \n7. Update the desired settings, then select Encrypt cache data. \n8. Choose Save Changes.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-121":{"article":"Security groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that no security group allow unrestricted egress access.","impact":"Allowing unrestricted outbound access can increase opportunities for malicious activity such as unauthorized access.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"Reduce the scope of the outbound rules to just the necessary scope, protocols, and ports. \n1. Login to the AWS Console. \n2. Go to Security group.\n3. Go to the SG rules. \n4. Click on the reported Firewall rule. \n5. Click on Edit. \n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-522":{"article":"Using environmental variables to store credentials in container task definition may violate the requirement to use strong cryptography to render authentication credentials unreadable. AWS Systems Manager Parameter Store can help you improve the security posture of your organization","impact":"Using environmental variables to store credentials in container task definition may violate the requirement to use strong cryptography to render authentication credentials unreadable. This could lead to unintended data exposure and unauthorized access.","report_fields":["taskDefinitionArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo pass secrets securely to containers use Parameter Store or Secrets Manager:\nUsing Parameter Store: \n1. aws ssm put-parameter --type SecureString --name awsExampleParameter --value awsExampleValue\nUsing Secrets Manager:\n1.1. aws secretsmanager create-secret --name awsExampleParameter --secret-string awsExampleValue\n2. Open the IAM console, and then create a role with a trust relation for ecs-tasks.amazonaws.com. For example:\n  {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n      {\n        \"Sid\": \"\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n          \"Service\": \"ecs-tasks.amazonaws.com\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n      }\n    ]\n  }\n3. To create an inline policy for your role in the IAM console, choose Roles, select the role that you created in step 2, and then choose Add inline policy on the Permissions tab. Choose the JSON tab, and then create a policy with the following code. Replace us-east-1 and awsExampleAccountID with the AWS Region and account where your parameters are stored. Replace awsExampleParameter with the name of the parameters that you created in step 1:\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Action\": [\n            \"ssm:GetParameters\",\n            \"secretsmanager:GetSecretValue\"\n          ],\n          \"Resource\": [\n            \"arn:aws:ssm:us-east-1:awsExampleAccountID:parameter/awsExampleParameter\",\n            \"arn:aws:secretsmanager:us-east-1:awsExampleAccountID:secret:awsExampleParameter*\"\n          ]\n        }\n      ]\n    }\n4.  (Optional) Attach the managed policy AmazonECSTaskExecutionRolePolicy to the role that you created in step 2.\n\nNote that when you update a task definition, it does not update running tasks that were launched from the previous task definition. To update a running task, you must redeploy the task with the new task definition.\n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/.\n2. In the left navigation pane, choose Task Definitions.\n3. Select the task definition that needs to be updated.\n4. For Task execution role, choose the task execution IAM role that you created earlier.\n5. In the Container Definitions section, choose Add container.\n6. In the Environment variables section under ENVIRONMENT, for Key, enter a key for your environment variable.\n7. On the Value dropdown list, choose ValueFrom.\n8. In the text box for the key, enter the Amazon Resource Name (ARN) of your Parameter Store or Secrets Manager resource.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-284":{"article":"To maintain the availability of the compute resources in the event of a failure and provide an evenly distributed application load, ensure that your Amazon Auto Scaling Groups (ASGs) have associated Elastic Load Balancers or Target Groups.\nElastic Load Balancing automatically distributes your incoming application traffic across all the EC2 instances that you are running. Elastic Load Balancing helps to manage inbound requests by optimally routing traffic so that no instance is overloaded. When you attach an Application Load Balancer, Network Load Balancer, or Gateway Load Balancer, you attach a Target Group.","impact":"Auto Scaling Group not attached to Elastic Load Balancer or Target Group may cause problems with availability and performance of instances.","report_fields":["AutoScalingGroupARN"],"remediation":"Use the following procedure to attach a load balancer to an existing Auto Scaling group.\n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2autoscaling/.\n2. Select the check box next to an existing group. A split pane opens up in the bottom part of the Auto Scaling groups page, showing information about the group that is selected.\n3. On the Details tab, choose Load balancing, Edit.\n4. Under Load balancing, do one of the following:\n  a. For Application, Network or Gateway Load Balancer target groups, select its check box and choose a target group.\n  b. For Classic Load Balancers, select its check box and choose a load balancer.\n5. Choose Update.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-303":{"article":"Management events provide visibility into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Example management events include: Configuring security (for example, IAM AttachRolePolicy API operations), Registering devices (for example, Amazon EC2 CreateDefaultVpc API operations), Configuring rules for routing data (for example, Amazon EC2 CreateSubnet API operations), Setting up logging (for example, AWS CloudTrail CreateTrail API operations).\nManagement events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the ConsoleLogin event.","impact":"If security critical information is not recorded, there will be no trail for forensic analysis, and discovering the cause of problems or the source of attacks may become more difficult or impossible.","report_fields":["TrailARN"],"remediation":"To enable Management events for all CloudTrail trails available within your AWS account, perform the following: \n  1. Open the Amazon CloudTrail console at https://console.aws.amazon.com/cloudtrail/. \n  2. In the navigation pane, choose Trails.\n  3. Choose the trail that you want to reconfigure.\n  4. Click the Edit button, next to the Management events section.\n  5. Choose if you want your trail to log Read events, Write events, or both.\n  6. Click Save to apply the changes and save the trail configuration.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-426":{"article":"This policy identifies the QLDB that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["EnvironmentArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon QLDB at https://console.aws.amazon.com/qldb.\n2. Click on the required QLDB ledger.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon QLDB"},"ecc-aws-324":{"article":"The Oracle 'resource_limit' parameter determines whether resource limits are enforced in database profiles.","impact":"If 'resource_limit' is set to FALSE, there will be no resource limits in database profiles.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type 'resource_limit'.\n6. Choose TRUE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-524":{"article":"A WAF Regional web ACL can contain a collection of rules and rule groups that inspect and control web requests.\nA web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, AWS AppSync, and Amazon Cognito resources.","impact":"If a web ACL is empty, the web traffic can pass without being detected or acted upon by WAF depending on the default action.","report_fields":["Name","WebACLId"],"remediation":"To add rules or rule groups to an empty web ACL:\n1. Sign in to the AWS Management Console and open the AWS WAF console at https://console.aws.amazon.com/wafv2/.\n  If you see 'Switch to AWS WAF Classic' in the navigation pane, select it. \n  For 'Filter', choose the 'Region' where the empty web ACL is located.\n2. In the navigation pane, choose 'Web ACLs'.\n3. Choose the name of the web ACL that you want to edit. This opens a page with the web ACL's details in the right pane.\t\n4. On the 'Rules' tab in the right pane, choose 'Edit web ACL'.\n5. To add rules to the web ACL, perform the following steps:\n  a. In the 'Rules' list, choose the rule that you want to add.\n  b. Choose 'Add rule to web ACL'. \n  c. Repeat steps a and b until you've added all the rules that you want.\n6. If you want to change the order of the rules in the web ACL, use the arrows in the 'Order' column. AWS WAF Classic inspects web requests based on the order in which rules appear in the web ACL. \n7. Choose 'Update'.","multiregional":false,"service":"AWS Web Application Firewall"},"ecc-aws-098":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. Network gateways are required to send/receive traffic to a destination outside of a VPC. It is recommended that a metric filter and alarm be established for detecting changes made to network gateways.","impact":"Lack of monitoring and logging of Internet Gateway changes can result in insufficient response time to accidental or intentional modifications. This may lead to unrestricted network access, loss of connection between your AWS VPC and Internet, or loss of VPN connection between your VPC and on-premises datacenter(s) linked to it.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{ ($.eventName = CreateCustomerGateway) || ($.eventName = DeleteCustomerGateway) || ($.eventName = AttachInternetGateway) || ($.eventName = CreateInternetGateway) || ($.eventName = DeleteInternetGateway) || ($.eventName = DetachInternetGateway) }}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-033":{"article":"This policy identifies security group rules that allow inbound traffic to the MySQL DB port (3306) from public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted MySQL access can increase opportunities for malicious activity such as unauthorized access, denial-of-service (DoS) attacks and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-197":{"article":"HTTPS (TLS) can be used to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks. Only encrypted connections over HTTPS (TLS) should be allowed. \nEnabling node-to-node encryption for Elasticsearch domains ensures that intra-cluster communications are encrypted in transit. There can be a performance penalty associated with this configuration. You should be aware of and test the performance trade-off before enabling this option.","impact":"Without Node-to-node encryption, potential attackers can manipulate or intercept network traffic between ElasticSearch cluster nodes using man-in-the-middle attacks (MITM) or similar attacks.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nFor a new Opensearch domain. When creating new domain make sure Node-to-node encryption under Encryption field is checked. \nFor a existing domain, first create a new domain with the Node-to-node encryption check box selected. Then follow this link to migrate your data to the new domain https://docs.aws.amazon.com/opensearch-service/latest/developerguide/version-migration.html#snapshot-based-migration.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-069":{"article":"Misconfigured S3 buckets can leak private information to the entire internet or allow unauthorized data tampering / deletion.","impact":"Granted anonymous access to S3 buckets can allow malicious users to view, upload, modify and delete S3 objects, actions that can lead to severe security issues such as data loss and unexpected charges on your AWS bill.","report_fields":["Name"],"remediation":"1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/. \n2. Choose the name of the bucket, the Bucket policy of which contains  [Effect='Allow' and Principal='*' and Action = '*']. \n3. Change the value of the parameters (Principal or Action ) to the specifically required ones. \n4. Enter Save button.","multiregional":true,"service":"Amazon S3"},"ecc-aws-068":{"article":"CloudTrail logs a record of every API call made in your AWS account. These log files are stored in an S3 bucket. It is recommended that the bucket policy or access control list (ACL) apply to the S3 bucket that CloudTrail logs to prevent public access to the CloudTrail logs.","impact":"Public access to CloudTrail log content can help an attacker in identifying weaknesses in the use or configuration of the affected account.","report_fields":["TrailARN"],"remediation":"Perform the following to remove any public access that has been granted to the bucket via an ACL or S3 bucket policy: \n1. Go to Amazon S3 console at https://console.aws.amazon.com/s3/home.\n2. Right-click on the bucket and then click on Properties.\n3. In the Properties pane, click on the Permissions tab. \n4. The tab shows a list of grants, one row per grant, in the bucket ACL. Each row identifies the grantee and the permissions granted. \n5. Select the row that grants permission to Everyone or Any Authenticated User.\n6. Uncheck all the permissions granted to Everyone or Any Authenticated User (click on x to delete the row). \n7. Click on Save to save the ACL. \n8. If the Edit bucket policy button is present, click on it. \n9. Remove any Statement having an Effect set to Allow and a Principal set to \"*\" or {\"AWS\":  \"*\"}.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-155":{"article":"An Elasticsearch domain requires at least three dedicated master nodes for high availability and fault-tolerance. Dedicated master node resources can be strained during data node blue/green deployments as there are additional nodes to manage. Deploying an Elasticsearch domain with at least three dedicated master nodes ensures sufficient master node resource capacity and cluster operations if a node fails. Using more than three master nodes might be unnecessary to mitigate the availability risk, and will result in additional cost.","impact":"Using only one master node can lead to low availability and fault tolerance. Deploying an Elasticsearch domain with at least three dedicated master nodes ensures sufficient master node resource capacity and cluster operations if a node fails.","report_fields":["ARN"],"remediation":"From Console:\n1. Login to the AWS Management Console and open the Amazon Elasticsearch Service console using https://console.aws.amazon.com/esv3. \n2. Under 'My domains', choose the name of the domain to edit.  \n3. Choose 'Edit domain'. \n4. Under 'Dedicated master nodes', set 'Instance type' to the desired instance type. \n5. Set 'Number of master nodes' equal to three or greater. \n6. Choose Submit.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-039":{"article":"This policy identifies security group rules that allow inbound traffic to the Telnet port (23) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted Telnet access can increase opportunities for malicious activity such as IP address spoofing, man-in-the-middle attacks (MITM) and brute-force attacks.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-430":{"article":"This policy identifies the Sagemaker instances that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["NotebookInstanceArn"],"remediation":"1. Open the Sagemaker console at https://console.aws.amazon.com/sagemaker/\n2. Choose Notebook instances.\n3. Choose required notebook instance.\n4. Under the 'Tags' click on the 'Edit'.\n5. Add tags and save.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-527":{"article":"A WAF global web ACL can contain a collection of rules and rule groups that inspect and control web requests.\nA web access control list (web ACL) gives you fine-grained control over all of the HTTP(S) web requests that your protected resource responds to. You can protect Amazon CloudFront, Amazon API Gateway, Application Load Balancer, AWS AppSync, and Amazon Cognito resources.","impact":"If a web ACL is empty, the web traffic can pass without being detected or acted upon by WAF depending on the default action.","report_fields":["WebACLId","Name"],"remediation":"To add rules or rule groups to an empty web ACL:\n1. Open the AWS WAF console at https://console.aws.amazon.com/wafv2/.\n2. In the navigation pane, choose 'Switch to AWS WAF Classic', and then choose 'Web ACLs'.\n3. For 'Filter', choose 'Global (CloudFront)'.\n4. Choose the name of the empty web ACL.\n5. Choose 'Rules', and then choose 'Edit web ACL'.\n6. For 'Rules', choose a rule or rule group, and then choose 'Add rule to web ACL'.\n7. At this point, you can modify the rule order within the web ACL if you are adding multiple rules or rule groups to the web ACL.\n8. Choose 'Update'.","multiregional":true,"service":"AWS Web Application Firewall"},"ecc-aws-513":{"article":"In order to adhere to security best practices and protect certificates from cryptographic algorithm hacking attacks using brute-force methods it is highly recommended upgrading 1024-bit server certificates to 2048-bit or 4096-bit RSA certificates which are using stronger encryption algorithms.","impact":"Any server certificate that is using 1024-bit keys can no longer be considered secure. All major web browsers dropped support for 1024-bit RSA certificates at the end of 2013. If your AWS IAM server certificates are still using 1024-bit keys, you should raise their bit length to 2048 or higher in order to increase its security level.","report_fields":["CertificateArn","DomainName"],"remediation":"1. Open the ACM console at https://console.aws.amazon.com/acm/home. If this is your first time using ACM, look for the AWS Certificate Manager heading and choose the Get started button under it.\n2. Choose Import a certificate.\n3. Do the following:\n  3.1. For Certificate body, paste the PEM-encoded certificate to import. It should begin with -----BEGIN CERTIFICATE----- and end with -----END CERTIFICATE-----.\n  3.2. For Certificate private key, paste the certificate's PEM-encoded, unencrypted private key. It should begin with -----BEGIN PRIVATE KEY----- and end with -----END PRIVATE KEY-----.\n  3.3. (Optional) For Certificate chain, paste the PEM-encoded certificate chain.\n4. Choose Review and import.\n5. On the Review and import page, check the displayed metadata about your certificate to ensure that it is what you intended. The fields include:\n  5.1. Domains \u2014 A list of fully qualified domain names (FQDN) authenticated by the certificate\n  5.2. Expires in \u2014 The number of days until the certificate expires\n  5.3. Public key info \u2014 The cryptographic algorithm used to generate the key pair\n  5.4. Signature algorithm \u2014 The cryptographic algorithm used to create the certificate's signature\n  5.5. Can be used with \u2014 A list of ACM integrated services that support the type of certificate you are importing\n6. If everything is correct, choose Import.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-358":{"article":"AWS CloudTrail provides a number of security features to consider as you develop and implement your own security policies. At least one AWS CloudTrail trail in the account should be defined with these security best practices:\n  - records global service events\n  - is a multi-region trail\n  - has Log file validation enabled\n  - encrypted with a KMS key\n  - records events for reads and writes\n  - records management events\n  - does not exclude any management events","impact":"If security critical information is not recorded, there will be no trail for forensic analysis, and discovering the cause of problems or the source of attacks may become more difficult or impossible.\nWithout enabling CloudTrail across different regions, it is hard to monitor the infrastructure and maintain security across all regions.\nWith disabled log file validation, you miss out on additional integrity checks of CloudTrail logs. CloudTrail log file validation is used to determine whether a log file was changed, deleted, or unchanged after CloudTrail delivered the log. \nWithout CloudTrail configured to use SSE-KMS, you do not have full control over who can read the log files in your organization. This can lead to unauthorized access to CloudTrail logs.","report_fields":["account_id","account_name"],"remediation":"To create a CloudTrail trail that comply with security best practices, perform the following actions:\nNote: If you have multiple single region trails, consider configuring your trails so that global service events and  are delivered in only one of the trails.\n1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail/\n2. On the CloudTrail service home page, the Trails page, or the Trails section of the Dashboard page, choose Create trail.\n3. On the Create Trail page, for Trail name, type a name for your trail.\n4. For Storage location, choose Create new S3 bucket to create a bucket. When you create a bucket, CloudTrail creates and applies the required bucket policies. To make it easier to find your logs, create a new folder (also known as a prefix) in an existing bucket to store your CloudTrail logs. Enter the prefix in Prefix.\nNote: If you chose 'Use existing S3 bucket', specify a bucket in Trail log bucket name, or choose Browse to choose a bucket. The bucket policy must grant CloudTrail permission to write to it.\n5. For Log file SSE-KMS encryption, choose Enabled. Choose 'New' AWS KMS key. In AWS KMS Alias, specify an alias, in the format alias/MyAliasName. The key policy must allow CloudTrail to use the key to encrypt your log files, and allow the users you specify to read log files in unencrypted form. \n6. In Additional settings, configure the following. For 'Log file validation', choose 'Enabled' to have log digests delivered to your S3 bucket.\n7. Optionally, configure CloudTrail to send log files to CloudWatch Logs by choosing 'Enabled' in 'CloudWatch Logs'.\n8. For Tags, add one or more custom tags (key-value pairs) to your trail. \n9. On the 'Choose log events' page, 'Management events' already selected by default, do not change it.\n10. For 'API activity', leave both boxes 'Read events' and 'Write events' selected. The default setting is to include all AWS KMS events.\n11. Do not exclude AWS events.\n12. Choose 'Next'.\n13. After a new trail was created, you should go to the 'AWS Key Management Service (AWS KMS)' console at https://console.aws.amazon.com/kms and give decrypt permissions to all users who require them. Users who have encrypt permissions but no decrypt permissions will not be able to read encrypted logs. \n\nDuring this configuration process, there was no need to enable 'Global service events' and a 'Multi-region trail'. Because 'Global service events' are delivered by default to trails that are created using the CloudTrail console. Also, in the console, by default, you create a trail that logs events in all AWS Regions.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-478":{"article":"Server Name Indication (SNI) is an extension to the TLS protocol that is supported by browsers and clients released after 2010. If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location. When a viewer submits an HTTPS request for your content, DNS routes the request to the IP address for the correct edge location. The IP address to your domain name is determined during the SSL/TLS handshake negotiation; the IP address isn't dedicated to your distribution.\nIf you configured CloudFront to use a custom SSL/TLS certificate with dedicated IP addresses, you can switch to using a custom SSL/TLS certificate with SNI instead and eliminate the charge that is associated with dedicated IP addresses.\nThis control fails if a custom SSL/TLS certificate is associated but the SSL/TLS support method is a dedicated IP address.","impact":"The SSL/TLS negotiation occurs early in the process of establishing an HTTPS connection. If CloudFront can't immediately determine which domain the request is for, it drops the connection if using 'vip' instead of 'sni'.","report_fields":["ARN"],"remediation":"To switch from a custom SSL/TLS certificate with dedicated IP addresses to SNI\n1. Sign in to the Amazon Web Services Management Console and open the CloudFront console at https://console.amazonaws.cn/cloudfront/v3/home.\n2. Choose the ID of the distribution that you want to view or update.\n3. Choose Distribution Settings.\n4. On the General tab, choose Edit.\n5. Change the setting of Custom SSL Client Support to Only Clients that Support Server Name Indication (SNI).\n6. Choose Yes, Edit.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-002":{"article":"Access keys consist of an access key ID and secret access key, which are used to sign programmatic requests that you make to AWS. AWS users need their own access keys to make programmatic calls to AWS from the AWS Command Line Interface (AWS CLI), Tools for Windows PowerShell, the AWS SDKs, or to make direct HTTP calls using the APIs for individual AWS services. It is recommended that all access keys be regularly rotated.","impact":"Not rotating access keys periodically increases the probability of access keys being compromised. It can lead to unauthorized access to your AWS cloud resources. Access keys rotation shortens the period an access key is active and reduces the business impact if the key is compromised.","report_fields":["Arn"],"remediation":"1. Go to the Management Console (https://console.aws.amazon.com/iam). \n2. Click on Users. \n3. Click on Security Credentials. \n4. As an Administrator - Click on Make Inactive for keys that have not been rotated in 90 Days. \n5. As an IAM User - Click on Make Inactive or Delete for keys which have not been rotated or used in 90 Days. \n6. Click on Create Access Key. \n7. Update programmatic call with new Access Key credentials.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-464":{"article":"Amazon ECS provides a default configuration for logging commands run using ECS Exec. You can audit which user accessed the container using AWS CloudTrail and log each command (and their output) to Amazon S3 or Amazon CloudWatch Logs. This configuration only handles the logging of the execute-command session. It doesn't affect logging of your application.","impact":"With disabled logging of ECS Exec, it may be difficult to troubleshoot any issues that might happen with container or find who accessed container and how they accessed it.","report_fields":["clusterArn"],"remediation":"To enable logging and publish data to CloudWatch, perform the following actions:\nExecute command :\naws ecs update-cluster \\\n    --cluster $CLUSTER_NAME \\\n    --region $AWS_REGION \\\n    --configuration executeCommandConfiguration=\"{{logging=OVERRIDE,\\\n                                                kmsKeyId=$KMS_KEY_ARN,\\\n                                                logConfiguration={{cloudWatchLogGroupName=\"$CLOUDWATCH_GROUP_NAME\",\\\n                                                                cloudWatchEncryptionEnabled=true}}\\\n                                                                }}\"\n\nTo enable logging and publish data to s3, perform the following actions:\nExecute command:\naws ecs update-cluster \\\n    --cluster $CLUSTER_NAME \\\n    --region $AWS_REGION \\\n    --configuration executeCommandConfiguration=\"{{logging=OVERRIDE,\\\n                                                logConfiguration={{s3EncryptionEnabled=true,\\\n                                                                s3BucketName=$ECS_EXEC_BUCKET_NAME,\\\n                                                                s3KeyPrefix=$PREFIX}} \\\n                                                                }}\"\nIn addition, the task role will need to have IAM permissions to log the output to S3 and/or CloudWatch and decrypt the data with KMS key.\nTo update your task role follow the instructions that are described in the 'IAM permissions required for Amazon CloudWatch Logs or Amazon S3 Logging' and 'IAM permissions required for encryption using your own AWS KMS key (KMS key)' sections in Amazon ECS User Guide: https://docs.aws.amazon.com/AmazonECS/latest/userguide/ecs-exec.html.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-037":{"article":"This policy identifies security group rules that allow inbound traffic to the PostgreSQL port (5432) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted PostgreSQL Database access can increase opportunities for malicious activity such as unauthorized access, denial-of-service (DoS) attacks, and loss of data. \nFor example, if a user account has a weak password, the attacker can execute a brute-force attack or dictionary-based password attack and then perform privilege escalation to root.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-500":{"article":"Deploying resources across multiple Availability Zones is an AWS best practice to ensure high availability within your architecture. Availability is a core pillar in the confidentiality, integrity, and availability triad security model. All Lambda functions should have a multi-Availability Zone deployment to ensure that a single zone of failure does not cause a total disruption of operations.\nWhen you create a Lambda function without a VPC configuration, it\u2019s automatically available in all Availability Zones within the Region. When you set up VPC access, you choose which Availability Zones the Lambda function can use. As a result, to provide continued high availability, ensure that the function has access to at least two Availability Zones.","impact":"Deploying Lambda function in only one Availability Zone, can cause a total disruption of operations.","report_fields":["FunctionArn"],"remediation":"To deploy a Lambda function in multiple Availability Zones through console:\n1. Open the AWS Lambda console at https://console.aws.amazon.com/lambda/\n2. From the 'Functions' page on the Lambda console choose a function.\n3. Choose 'Configuration' and then choose 'VPC'.\n4. Choose 'Edit'.\n5. For 'Subnets' attach at least one additional subnet.\n6. Click 'Save'.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-545":{"article":"Logging and monitoring are important for maintaining the reliability, availability, and performance of Step Functions and your AWS solutions. You can choose from OFF, ALL, ERROR, or FATAL. No event types log when set to OFF and all event types do when set to ALL.","impact":"Disabling logging in AWS Step Functions can result in inefficient resource usage, reduced performance, limited scalability, and additional operational overhead.","report_fields":["stateMachineArn"],"remediation":"To enable logging for AWS Step Functions, you can follow these steps:\n1. Open the AWS Management Console and navigate to the Step Functions console.\n2. Click on the state machine for which you want to enable logging.\n3. In the state machine details page, click on the \"Edit\" button.\n4. In the \"Log level\" dialog box choose the log level you want to capture in the log stream. The available log levels are: OFF, ERROR, WARN, INFO, and DEBUG.\n5. Select the desired CloudWatch log group. You can create a new log group or select an existing one.\n6. Click on the \"Save\" button.","multiregional":false,"service":"AWS Step Functions"},"ecc-aws-181":{"article":"A private replication instance has a private IP address that you cannot access outside of the replication network. A replication instance should have a private IP address when the source and target databases are in the same network. The network must also be connected to the replication instance's VPC using a VPN, AWS Direct Connect, or VPC peering.","impact":"When DMS replication instances are publicly accessible and have public IP addresses, any machine outside the VPC can establish a connection to these instances, increasing the attack surface and the opportunity for malicious activity.","report_fields":["ReplicationInstanceIdentifier","ReplicationInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\n1. Open the AWS Database Migration Service console at https://console.aws.amazon.com/dms/.\n2. Navigate to Replication instances, then delete the public instance. Choose the instance, choose Actions, then choose delete.\n3. Choose Create replication instance. Provide the configuration details.\n4. To disable public access, make sure that Publicly accessible is not selected.\n5. Choose Create.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-279":{"article":"Redis authentication tokens, or passwords, enable Redis to require a password before allowing clients to run commands, thereby improving data security.\nAmazon ElastiCache for Redis allows you to modify authentication tokens by setting and rotating new tokens. You can now modify active tokens while in use, or add brand-new tokens to existing encryption-in-transit enabled clusters that were previously setup without authentication tokens. \nWith support for rotating authentication token, ElastiCache for Redis now provides you more control and flexibility to meet your security requirements and password rotation policies.","impact":"Not rotating tokens periodically increases the probability of tokens being compromised. It can lead to unauthorized access to your AWS cloud resources. Tokens rotation shortens the period a token is active and reduces the business impact if it is compromised.","report_fields":["ARN"],"remediation":"To update a Redis server with a new AUTH token:\n1. Sign in to the Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the navigation pane, choose 'Redis clusters'.\n3. From the list of clusters, choose the cluster you want to update.\n4. Choose 'Actions' and then choose 'Modify'.\n5. Under 'Redis Auth Token strategy' select 'Rotate token' and set new token.\n6. Click 'Preview changes'.\n7. If you want to perform the update right away, choose 'Apply immediately'. If Apply immediately is not chosen, the update process is performed during the cluster's next maintenance window.\n8. Choose 'Modify'.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-093":{"article":"Ensure that your AWS SageMaker notebook instances are placed to the VPC to access the VPC resources only and 'Direct internet access' is disabled.\nWhen your notebook allows direct internet access, SageMaker provides a network interface that allows the notebook to communicate with the internet through a VPC managed by SageMaker. \nIt is recommended to keep your resources inside a custom VPC whenever possible to ensure secure network protection of your infrastructure. An Amazon VPC is a virtual network dedicated to your AWS account. With an Amazon VPC, you can control the network access and internet connectivity of your SageMaker Studio and notebook instances.","impact":"When AWS SageMaker notebook instances are publicly accessible, any machine outside the VPC can establish a connection to these instances, increasing the attack surface and the opportunity for malicious activity.","report_fields":["NotebookInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nTo ensure that your AWS SageMaker notebook instances are running inside a VPC, you need to re-create these:\n1. Log in to the AWS Management Console. \n2. Go to the SageMaker service dashboard at https://console.aws.amazon.com/sagemaker/. \n3. Create a notebook Instance.\n4. Under 'Network' section, select 'VPC' and ensure that 'Direct internet access' is set to 'Disable Access the internet through a VPC'.\n5. Configure other settings as required for your particular case.\n6. Click 'Create notebook instance'.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-164":{"article":"Amazon Redshift audit logging provides additional information about connections and user activities in your cluster. This data can be stored and secured in Amazon S3 and can be helpful for security audits and investigations.","impact":"If security critical information is not recorded, there will be no trail for forensic analysis, and discovering the cause of problems or the source of attacks may become more difficult or impossible.","report_fields":["ClusterIdentifier"],"remediation":"1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. In the navigation menu, choose Clusters, then choose the name of the cluster to modify.\n3. Choose Maintenance and monitoring.\n4. Under Audit logging, choose Edit.\n5. Set Enable audit logging to Yes, then enter the log destination bucket details.\n6. Open required cluster and click on 'Properties'.\n7. Click on the 'Parameter group'.\n8. Click on 'Parameters' and choose 'Edit parameters'.\n9. Change 'enable_user_activity_logging' to true.\n10. Save chages\n11. Choose Confirm.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-354":{"article":"Changing default port (5439) helps with automated attacks and gives more information if it was a target attack or not.","impact":"Running AWS Redshift clusters on the default port is a potential security risk as it provides an attacker path to a service listening on that port and increases the attack vectors your organization is exposed to.","report_fields":["ClusterIdentifier"],"remediation":"To remediate this issue from the AWS CLI, use the Amazon Redshift modify-cluster command to set the --port attribute:\naws redshift modify-cluster --cluster-identifier clustername --port port","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-352":{"article":"Encrypting data at rest reduces the risk of data stored on disk being accessed by a user not authenticated to AWS. It also adds another set of access controls to limit the ability of unauthorized users to access the data. \nA customer master key (CMK) is the primary resource used by AWS Key Management Service (KMS) to create data encryption keys and can be customer managed or AWS managed. SNS topic offers encryption of data at rest, a security feature that helps prevent unauthorized access to AWS SNS topic data. Use a customer managed CMK to encrypt SNS topics so that only users in the account with access to the CMK can view or manage the data.","impact":"Unauthorized users can read confidential information available on SNS topics. Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["TopicArn"],"remediation":"1. Open the Amazon SNS console at https://console.aws.amazon.com/sns/v3/home.\n2. In the navigation pane, choose Topics. \n3. Choose the name of the topic to encrypt.\n4. Choose Edit. \n5. Under Encryption, choose Enable Encryption. \n6. Choose the KMS CMK key to use to encrypt the topic. \n7. Choose Save changes.","multiregional":false,"service":"Amazon Simple Notification Service"},"ecc-aws-085":{"article":"You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your AWS account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access private resources while the function is running.","impact":"Lambda without a VPC is open to the internet. It can increase opportunities for malicious activity such as spamming and Denial-of-Service (DoS) attacks. Also, Lambda without a VPC cannot access AWS resources.","report_fields":["FunctionArn"],"remediation":"1. Open the AWS Lambda console at https://console.aws.amazon.com/lambda/.\n2. Navigate to Functions and then select your Lambda function.\n3. Scroll to Network and then select a VPC with the connectivity requirements of the function.\n4. To run your functions in high availability mode, Security Hub recommends that you choose at least 2 subnets. \n5. Choose at least one security group that has the connectivity requirements of the function \n6. Choose Save.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-021":{"article":"Identifies volumes that have not had a backup or snapshot in the past 14 days. If a snapshot is not recent, it could be missing crucial patches and software updates. It is recommended to keep backups as recent as you can.","impact":"Without a recently taken snapshot, it is impossible to recover data in the event of failure quickly. It has a significant impact on the resilience of your system.","report_fields":["VolumeId"],"remediation":"1. Navigate to console.aws.amazon.com/ec2/. \n2. Select Snapshots under Elastic Block Store. \n3. Find snapshots that were created more than 14 days ago.\n4. Update snapshots:\n4.1. Press Create snapshot.\n4.2. Select Volume in the Resource  type.\n4.3. Choose the volume with the expired period  under the Volume tab.\n4.4. Press Create snapshot.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-215":{"article":"Backups help you to recover more quickly from a security incident. They strengthen the resilience of your systems. Amazon Redshift takes periodic snapshots by default. \nAutomatic snapshots should be enabled and retained for at least seven days.","impact":"A retention period of fewer than 7 days set for Redshift clusters can result in data loss and the inability to recover it in the event of failure.","report_fields":["ClusterIdentifier"],"remediation":"1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. In the navigation menu, choose Clusters, then choose the name of the cluster to modify. \n3. Choose Edit. \n4. Under Backup, set Snapshot retention to a value of 7 or greater. \n5. Choose Modify Cluster.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-102":{"article":"When your AWS SageMaker notebook instances are publicly accessible, any machine outside the VPC can establish a connection to these instances, increasing the attack surface and the opportunity for malicious activity. It is recommended that the Amazon SageMaker notebook instances are not publicly accessible.","impact":"Allowing direct public access to your notebook instance might violate the requirement to only allow access to system components that provide authorized publicly accessible services, protocols, and ports. When AWS SageMaker notebook instances are publicly accessible, any machine outside the VPC can establish a connection to these instances, increasing the attack surface and the opportunity for malicious activity.","report_fields":["NotebookInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Log in to the AWS Management Console. \n2. Go to the SageMaker service dashboard at https://console.aws.amazon.com/sagemaker/. \n3. Create a notebook Instance.\n4. Under Network, select VPC and ensure that 'Direct internet access' is set to 'Disable Access the internet through a VPC'.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-362":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon MWAA uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["Arn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nIn order to encrypt a new Airflow environment with KMS CMK: \n1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Click the \"Create environment\" button.\n4. On the \"Specify details\" page configure \"Environment details\" and \"DAG code in Amazon S3\". \n5. Click \"Next\".\n6. On the \"Configure advanced settings\" make all the necessary configurations.\n7. Under the \"Encryption\" section, check \"Customize encryption settings (advanced)\".\n8. Select a KMS Customer managed key.\n9. Click \"Next\".\n10. On the \"Review and create\" page, review the configuration and click \"Create environment\".\n\nMake sure that all necessary permissions are configured for the execution role and KMS CMK key policy. Because policies should provide the right permissions for MWAA and the workers to be able to access the keys to decrypt/encrypt messages and logs.\nYou must add permissions to your execution role if your Airflow DAGs require access to any other AWS services.","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-365":{"article":"With this feature enabled, you can encrypt AWS Glue Data Catalog connection password. These passwords are stored in the Data Catalog connection and are used when AWS Glue connects to a Java Database Connectivity (JDBC) data store.","impact":"Not encrypted connection password can be compromised. Without AWS KMS Customer Master Keys (CMKs), you do not have full and granular control over who can use the encryption keys to access AWS Glue data.","report_fields":["CatalogId"],"remediation":"1. Sign in to the AWS Management Console and open the AWS Glue console at https://console.aws.amazon.com/glue/.\n2. Choose 'Settings' in the navigation pane.\n3. On the 'Data catalog settings' page, select 'Encrypt connection passwords', and choose an AWS KMS Customer Master Key.","multiregional":false,"service":"AWS Glue"},"ecc-aws-160":{"article":"RDS event notifications use Amazon SNS to make you aware of changes in the availability or configuration of your RDS resources. These notifications allow for rapid response.\nDBInstance:['maintenance','configuration change', 'failure'] Notification should be enabled for All Instances.","impact":"Not enabling RDS instance event notifications can lead to slow or no response to an instance configuration change or a failure.","report_fields":["account_id","account_name"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Event subscriptions.\n3. Under Event subscriptions, choose Create event subscription.\n4. In the Create event subscription dialog, do the following.\n4.1. For Name, enter a name for the event notification subscription.\n4.2. For Send notifications to, choose an existing Amazon SNS ARN for an SNS topic.\n     To use a new topic, choose Create topic to enter the name of a topic and a list of recipients.\n4.3. For Source type, choose Instances.\n4.4. Under Instances to include, select All Instances.\n4.5. Under Event categories to include, select Specific event categories.\n     The control also passes if you select All event categories.\n4.5.1. Select maintenance, configuration change, and failure.\n4.6. Choose Create.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-075":{"article":"The encryption of data at rest is a security feature that helps prevent unauthorized access to your data. When the feature is enabled, it encrypts sensitive information on your Elasticsearch domains and their storage systems such as Indices, Elasticsearch Logs, Swap files, automated snapshots and all other data in the application directory. The ElasticSearch at-rest encryption feature uses AWS KMS service to store and manage the encryption keys.","impact":"When encryption of data at rest is not enabled, it can lead to unauthorized access to sensitive information available on ES domains (clusters) and their storage systems.","report_fields":["ARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. By default, domains do not encrypt data at rest, and you cannot configure existing domains to use the feature. \n2. To enable the feature, you must create another domain and migrate your data.  \n3. Encryption of data at rest requires Amazon ES 5.1 or later.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-076":{"article":"EBS snapshots are used to back up the data on your EBS volumes to Amazon S3 at a specific point in time. You can use the snapshots to restore previous states of EBS volumes. It is rarely acceptable to share a snapshot with the public. \nTo avoid accidental exposure of your company's sensitive data, Amazon EBS snapshots should not be publicly restorable by everyone unless you explicitly allow it.","impact":"Publicly restorable AWS Elastic Block Store (EBS) volume snapshots can lead to exposure of personal and sensitive data.","report_fields":["SnapshotId","OwnerId"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, choose Snapshots and then choose your public snapshot.\n3. Choose Actions, then choose Modify permissions. \n4. Choose Private. \n5. Optionally, add AWS account numbers for authorized accounts to share your snapshot with. \n6. Choose Save.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-029":{"article":"This policy identifies security group rules that allow inbound traffic to the FTP port (21) from the public internet. Allowing access from arbitrary internet IP addresses to this port increases the attack surface of your network.","impact":"Unrestricted FTP access can increase opportunities for malicious activity such as brute-force attacks, FTP bounce attacks, spoofing attacks and packet capture.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console.\n2. Go to Security Group.\n3. Go to the SG rules.\n4. Click on the reported Firewall rule.\n5. Click on Edit.\n6. Modify the rule.\n7. Click on Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-338":{"article":"By default, when you create a notebook instance, users that log into that notebook instance have root access.","impact":"Users with root access have administrator privileges, users can access and edit all files on a notebook instance with root access enabled.","report_fields":["NotebookInstanceArn"],"remediation":"To disable root access:\n1. Open the Sagemaker console at https://console.aws.amazon.com/sagemaker/\n2. Choose Notebook instances, and then choose the notebook instance which you want to update.\n3. Choose Stop.\n4. Once the status of the Notebook instance is Stopped, choose Edit.\n5. In the Permissions and encryption settings disable Root access.\n6. Choose Update notebook instance.\n7. Choose Start to start the instance.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-089":{"article":"You can use CodeBuild in your PCI DSS environment to compile your source code, run unit tests, or produce artifacts that are ready to deploy. If you do, never store the authentication credentials AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in clear text. \nUsing environmental variables to store credentials in your CodeBuild project may violate the requirement to use strong cryptography to render authentication credentials unreadable.","impact":"Using environmental variables to store credentials in your CodeBuild project may violate the requirement to use strong cryptography to render authentication credentials unreadable. This could lead to unintended data exposure and unauthorized access.","report_fields":["arn"],"remediation":"1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Expand Build, choose Build project, and then choose the build project that contains plaintext credentials. \n3. From Edit, choose Environment. \n4. Expand Additional configuration and then scroll to Environment variables. \n5. Choose Remove next to the environment variable. \n6. Choose Update environment.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-236":{"article":"Enabling debug_pretty_print indents the messages produced by debug_print_parse, debug_print_rewritten, or debug_print_plan making them significantly easier to read. \nIf this setting is disabled, the \"compact\" format is used instead, significantly reducing readability of the DEBUG statement log messages. \nUnless directed otherwise by your organization's logging policy, it is recommended this setting be disabled by setting it to on.","impact":"If this setting is disabled, the \"compact\" format is used instead, significantly reducing readability of the DEBUG statement log messages.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type 'debug_pretty_print'.\n6. Choose 1 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-343":{"article":"Check whether Amazon MQ brokers are not publicly accessible. The rule is NON_COMPLIANT if the publicly accessible field is true.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["BrokerArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon MQ console at https://console.aws.amazon.com/amazon-mq/\n2. In the navigation pane, choose Brokers.\n3. Click on the 'Create brokers'\n4. Inside 'Configure settings' under 'Additional settings' click on the 'Private access'.\n5. Create broker.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-116":{"article":"API Gateway private endpoints are made available via AWS PrivateLink interface VPC endpoints. Interface endpoints work by creating elastic network interfaces in subnets that you define inside your VPC. Those network interfaces then provide access to services running in other VPCs, or to AWS services such as API Gateway.\nWhen configuring your interface endpoints, you specify which service traffic should go through them. API Gateway as a fully managed service runs its infrastructure in its own VPCs. When you interface with API Gateway publicly accessible endpoints, it is done through public networks. \nWhen configured as private, the public networks are not made available to route your API. Instead, your API can only be accessed using the interface endpoints that you have configured.","impact":"When a private API endpoint has access from the public internet, it increases the attack surface.","report_fields":["id","name"],"remediation":"1. Sign in to the API Gateway console and choose APIs in the primary navigation pane https://console.aws.amazon.com/apigateway. \n2. Under + Create API, choose the API settings (gear icon).\n3. Under Endpoint Configuration, change the Endpoint Type option from Edge Optimized to Regional or from Regional to Edge Optimized. \n4. Choose Save to start the update.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-165":{"article":"A public IP address is an IP address that is reachable from the internet. If you launch your Amazon ECS instances with a public IP address, then your Amazon ECS instances are reachable from the internet. Amazon ECS services should not be publicly accessible, as this may allow unintended access to your container application servers.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["serviceArn"],"remediation":"To disable automatic public IP assignment, configure VPC and security group settings for your service in the Amazon Elastic Container.  \nFrom Console:\n1. Login to the AWS Management Console and open the ECS console using https://console.aws.amazon.com/ecs/\n2. In the navigation pane, choose Clusters and open the needed cluster.\n3. In the navigation pane, choose Services and then choose Create.\n4. Configure network by setting Auto-assign public IP to DISABLED.\n5. Complete service configuration.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-270":{"article":"You can enable Multi-AZ only on Redis (cluster mode disabled) clusters that have at least one available read replica. \nClusters without read replicas do not provide high availability or fault tolerance.","impact":"When an ElastiCache is not configured for multiple Availability Zones and in case of any issue with the Availability Zone, elasticache will not be reachable. \nMulti-AZ deployments allow you to automate failover.","report_fields":["ARN"],"remediation":"Enabling Multi-AZ on an existing cluster:\n1. Sign in to the Console and open the ElastiCache console at https://console.aws.amazon.com/elasticache/.\n2. From the navigation pane, choose 'Redis clusters'.\n3. From the list of clusters, choose the cluster you want to update.\n4. Choose 'Actions' and then choose 'Modify'.\n5. Choose 'Enable' for 'Multi-AZ'. And click 'Preview changes'.\n6. If you want to perform the update right away, choose 'Apply immediately'. If Apply immediately is not chosen, the update process is performed during the cluster's next maintenance window.\n7. Choose 'Modify'.","multiregional":false,"service":"Amazon ElastiCache"},"ecc-aws-326":{"article":"When you create and use your own KMS CMK customer-managed keys to protect the contents of your EBS volumes, you obtain full control over who can use the CMK keys and access the data encrypted within Volume. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs) for EBS volumes.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["VolumeId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt existing EBS Volume with KMS CMK:\n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/.\n2. Under Elastic Block Store, click on Volumes.\n3. Click on required volume.\n4. Click on the Actions.\n5. Click on the Create snapshot.\n6. Create snapshot.\n7. Under Elastic Block Store, click on Snapshots\n8. Click on the required snapshot.\n9. Click on the Actions, and Create volume from snapshot.\n10. Under the encryption click on the 'Encrypt this volume'.\n11. Choose existing KMS CMK (not default) key or create new one.\n12. Click on the Create volume.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-372":{"article":"For any WorkSpaces API requests setup the connection through an interface endpoint in your VPC. Utilizing a VPC interface endpoint for WorkSpaces API requests keeps the communication within the AWS network.\nThe VPC interface endpoint connects your VPC directly to the Amazon WorkSpaces API endpoint without an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. The instances in your VPC don't need public IP addresses to communicate with the Amazon WorkSpaces API endpoint.","impact":"Without using VPC Endpoint network connection exposed to the public network, this increases the opportunity for malicious activities and attacks.","report_fields":["DirectoryId"],"remediation":"Perform the following steps to create a VPC interface endpoint:\n1. Log in to the VPC console at https://console.aws.amazon.com/vpc/.\n2. In the left pane, click 'Endpoints'.\n3. Click 'Create Endpoint'.\n4. For 'Service category', ensure that AWS services is selected.\n5. For 'Service Name', choose 'Workspaces'. For Type, ensure that it indicates 'Interface.\n6. Complete the following information:\n  - For 'VPC', select a VPC in which to create the endpoint. \n  - For 'Subnets', select the subnets (Availability Zones) in which to create the endpoint network interfaces. Not all Availability Zones may be supported for all AWS services. \n  - To enable private DNS for the interface endpoint, for 'Enable DNS Name', select the check box. \n  - For Security group, select the security groups to associate with the endpoint network interfaces.\n8. Click Create endpoint.\n\nAfter you have created a VPC endpoint, you can use the following example CLI commands that use the endpoint-url parameter to specify interface endpoints to the Amazon WorkSpaces API endpoint:\naws workspaces copy-workspace-image --endpoint-url VPC_Endpoint_ID.workspaces.Region.vpce.amazonaws.com\n\naws workspaces delete-workspace-image --endpoint-url VPC_Endpoint_ID.api.workspaces.Region.vpce.amazonaws.com\n\naws workspaces describe-workspace-bundles --endpoint-url VPC_Endpoint_ID.workspaces.Region.vpce.amazonaws.com  \\\n  --endpoint-name Endpoint_Name \\\n  --body \"Endpoint_Body\" \\\n  --content-type \"Content_Type\" \\\n      Output_File","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-336":{"article":"Using Amazon KMS Customer Master Keys (CMKs) to protect data in SageMaker endpoint configurations gives full control over who can use the encryption keys to access SageMaker data. Amazon KMS service allows to easily create, rotate, disable and audit Customer Master Keys created for SageMaker.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["EndpointConfigArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create a new Endpoint configuration, perform the following actions:\n1. Open the Sagemaker console at https://console.aws.amazon.com/sagemaker/.\n2. In the navigation pane, under 'Inference' choose 'Endpoint configurations'.\n3. Choose 'Create Endpoint configurations'.\n4. Under 'Encryption key' choose existing key or create new key.\n5. Choose 'Create Endpoint configurations'.","multiregional":false,"service":"Amazon SageMaker"},"ecc-aws-536":{"article":"This control checks function settings for the following runtimes: nodejs18.x,, nodejs16.x, nodejs14.x, nodejs12.x, python3.9, python3.8, python3.7, ruby2.7, java11, java8, java8.al2, go1.x, dotnetcore3.1, and dotnet6.\nLambda runtimes are built around a combination of operating system, programming language, and software libraries that are subject to maintenance and security updates. When a runtime component is no longer supported for security updates, Lambda deprecates the runtime. Even though you cannot create functions that use the deprecated runtime, the function is still available to process invocation events. Make sure that your Lambda functions are current and do not use out-of-date runtime environments.\nIt is strongly recommended that you migrate functions to a supported runtime version so that you continue to receive security patches and remain eligible for technical support.","impact":"When security updates are no longer available for a component of a runtime, Lambda deprecates the runtime. Because of this, using deprecated Lambda runtimes can pose a security risk. Moreover, functions that use a deprecated runtime are no longer eligible for technical support.","report_fields":["FunctionArn"],"remediation":"To update a function, you need to migrate it to a supported runtime version. \n1. Login to the AWS Management Console and open the Amazon Lambda https://console.aws.amazon.com/lambda/.\n2. In the navigation pane click on the 'Functions'.\n3. Scroll down in the 'Code' tab to the 'Runtime settings' section.\n4. Click 'Edit'.\n5. Select a supported runtime version.\n6. Click 'Save'.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-512":{"article":"Check whether Amazon ELB (Application, Network and Gateway load balancers) are not internet facing. The rule is NON_COMPLIANT if the Scheme field is 'internet-facing' in the configuration.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["LoadBalancerArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Sign in to the AWS Admin Console and access the Amazon EC2 at https://console.aws.amazon.com/ec2/v2.\n2. Under the 'Load Balancing' click on the 'Load Balancers'.\n3. Create Network, Gateway or Application load balancer.\n4. Under the 'Scheme' choose 'Internal'.\n5. Create load balancer.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-367":{"article":"AWS creates a security group and attaches it to your directory\u2019s domain controller elastic network interfaces. This security group blocks unnecessary traffic to the domain controller and allows traffic that is necessary for Active Directory communications. AWS configures the security group to open only the ports that are required for Active Directory communications. In the default configuration, the security group accepts traffic to these ports from any IP address.\nIf you want to increase the security of your directories\u2019 security groups, you can modify them to accept traffic from a more restrictive list of IP addresses. For example, you could change the accepted addresses from 0.0.0.0/0 to a CIDR range that is specific to a single subnet or computer. Make such changes only if you fully understand how security group filtering works. Improper changes can result in loss of communications to intended computers and instances. AWS recommends that you do not attempt to open additional ports to the domain controller as this decreases the security of your directory.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["DirectoryId"],"remediation":"To edit a security group of directories: \n1. Go to https://console.aws.amazon.com/directoryservicev2.\n2. Click on a Directory which Security Group you want to edit.\n3. Note the VPC ID.\n4. Go to https://console.aws.amazon.com/ec2/v2/home.\n5. In the left panel, select Security Groups.\n6. Filter security groups based on VPC ID.\n7. Click on a security group that ends on \"_controllers\".\n8. Edit all inbound rules that have Source \"0.0.0.0/0\" or \"::/0\".","multiregional":false,"service":"AWS Directory"},"ecc-aws-282":{"article":"Amazon Elasticsearch Service allows you to configure your domains to require that all traffic be submitted over HTTPS. This ensures communications between your clients and your domain are encrypted.  ElasticSearch domains should be configured to enforce HTTPS connections for all clients to ensure encryption of data in transit.","impact":"Without enforcing HTTPS connection, Elasticsearch Service accepts a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["ARN"],"remediation":"To change the policy using the AWS Console, follow these steps:\n1. Open the Amazon Elasticsearch console at https://console.aws.amazon.com/esv3.\n2. Open a domain tab.\n3. Choose a domain that you want to edit. Select 'Actions' and 'Edit security configuration'.\n4. Select 'Require HTTPS' for all traffic to the domain.\n5. Click Submit.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-452":{"article":"Enabling connection draining on Elastic Beanstalk ensures that the load balancer stops sending requests to instances that are de-registering or unhealthy. It keeps the existing connections open. This is particularly useful for instances in Auto Scaling groups to ensure that connections aren't severed abruptly.","impact":"If connection draining is not enabled, Elastic Beanstalk can send requests to instances that are de-registering or unhealthy which results in no connection to Beanstalk.","report_fields":["EnvironmentArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Elastic Beanstalk console using https://console.aws.amazon.com/elasticbeanstalk/\n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list.\n3. In the navigation pane, choose Configuration.\n4. In the 'Load balancer' category, choose Edit.\n5. Enable 'Connection draining'.\n6. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-348":{"article":"TLS can be used to help prevent potential attackers from using person-in-the-middle or similar attacks to eavesdrop on or manipulate network traffic. Only encrypted connections over TLS should be allowed. Encrypting data in transit can affect performance. You should test your application with this feature to understand the performance profile and the impact of TLS.","impact":"Without enabled encryption in transit, MSK accept a connection whether it uses TLS or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["ClusterArn"],"remediation":"Perform the following steps to enable log delivery for MSK:\n1. Login to the MSK console at https://console.aws.amazon.com/msk/.\n2. Click on the Clusters.\n3. Choose required cluster.\n4. Click on the Actions and 'Edit Security Settings'.\n5. Under the 'Encrypt data in transit' choose only 'TLS encryption'.\n6. Save changes.","multiregional":false,"service":"Amazon Managed Streaming for Apache Kafka"},"ecc-aws-019":{"article":"IAM password policies can prevent the reuse of a given password by the same user. It is recommended that the password policy prevent the reuse of passwords. Preventing the password reuse increases account resiliency against brute force login attempts.","impact":"Using the same password can increase the risk of a brute force attack, dictionary-based password attack, etc.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane.\n4. Check 'Prevent password reuse'.\n5. Set 'Number of passwords to remember' to 24.","multiregional":true,"service":"AWS Account"},"ecc-aws-301":{"article":"SQS allows users to specify a redrive policy for a particular queue. This policy describes what to do when a message fails to be consumed from a queue, including how many times to try to consume the message. It also allows you to specify a Dead Letter Queue, a queue that the message is sent to after it fails too many times. Specifying a redrive policy allows you to avoid \u201cpoison pills\u201d (messages that cannot be handled and will clog up your consumer) and provides a place for failed messages to be stored for further inspection.","impact":"Messages crucial to your applications pipeline may be discarded, causing loss of data. Bugs in code that produce failing messages may remain undetected.","report_fields":["QueueArn"],"remediation":"To create SQS Queue:\n1. Open the Amazon SQS console at https://console.aws.amazon.com/sqs/.\n2. In the navigation pane, choose Queues.\n3. Create new queue.\n4. Scroll to the \"Redrive allow policy\" and enable it.\n5. Choose required \"Redrive permission\".\n6. Create Queue.\n\nTo configure a dead-letter queue:\n1. Open the Amazon SQS console at https://console.aws.amazon.com/sqs/.\n2. In the navigation pane, choose Queues.\n3. Choose a queue and choose Edit.\n4. Scroll to the Dead-letter queue section and choose Enabled.\n5. Choose the Amazon Resource Name (ARN) of an existing Dead Letter Queue that you want to associate with this source queue.\n6. To configure the number of times that a message can be received before being sent to a dead-letter queue, set Maximum receives to a value between 1 and 1,000.\n7. When you finish configuring the dead-letter queue, choose Save.","multiregional":false,"service":"Amazon Simple Queue Service"},"ecc-aws-450":{"article":"Instance metadata is used to configure or manage the running instance. The IMDS provides access to temporary, frequently rotated credentials. These credentials remove the need to hard code or distribute sensitive credentials to instances manually or programmatically.\nVersion 2 of the IMDS adds new protections for the following types of vulnerabilities. These vulnerabilities could be used to try to access the IMDS:\n  - Open website application firewalls;\n  - Open reverse proxies;\n  - Server-side request forgery (SSRF) vulnerabilities;\n  - Open Layer 3 firewalls and network address translation (NAT).","impact":"Instances that use IMDSv1 are exposed to the following vulnerabilities:\n- Open website application firewalls;\n- Open reverse proxies;\n- Server-side request forgery (SSRF) vulnerabilities;\n- Open Layer 3 firewalls and network address translation (NAT).","report_fields":["EnvironmentArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Elastic Beanstalk console using https://console.aws.amazon.com/elasticbeanstalk/\n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list.\n3. In the navigation pane, choose Configuration.\n4. In the Instances configuration category, choose Edit.\n5. Set Disable IMDSv1 to enforce IMDSv2. Clear Disable IMDSv1 to enable both IMDSv1 and IMDSv2.\n6. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-024":{"article":"Implement a mechanism to encrypt and decrypt electronic protected health information.","impact":"Disabled encryption allows unauthorized or anonymous users to gain access to messages containing sensitive data.","report_fields":["QueueArn"],"remediation":"1. Navigate to https://console.aws.amazon.com/sqs/. \n2. Choose Queues.\n3. Click on the reported Queue.\n4. Press Edit.\n5. Under Encryption, enable Server-side encryption.","multiregional":false,"service":"Amazon Simple Queue Service"},"ecc-aws-097":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. NACLs are used as a stateless packet filter to control ingress and egress traffic for subnets within a VPC. It is recommended that a metric filter and alarm be established for detecting changes made to NACLs.","impact":"Lack of monitoring and logging of Network Access Control Lists (NACL) changes can result in insufficient response time to detect accidental or intentional modifications that may lead to unrestricted or unauthorized access.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{ ($.eventName = CreateNetworkAcl) || ($.eventName = CreateNetworkAclEntry) || ($.eventName = DeleteNetworkAcl) || ($.eventName = DeleteNetworkAclEntry) || ($.eventName = ReplaceNetworkAclEntry) || ($.eventName = ReplaceNetworkAclAssociation) }}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-103":{"article":"Custom SSL certificates give you full control over your CloudFront content. Custom certificates allow users to access content by using an alternate domain name. You can store custom certificates in AWS Certificate Manager (ACM) or in IAM. \nIt is recommended to use a custom SSL Certificate to access CloudFront content to have more control over your data.","impact":"Without custom SSL certificates, you will not be able to deliver content over HTTPS using your own domain name, you will be able to use only CloudFront distribution domain name.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS console \n2. Select the region from the drop-down list in which the issue has occured \n3. Navigate to the CloudFront Distributions Dashboard \n4. Click the reported distribution \n5. On the 'General' tab, click on the 'Edit' button \n6. On the 'Edit Distribution' page set 'SSL Certificate' to 'Custom SSL Certificate (example.com)', select a certificate or type your certificate ARN and other parameters as per your requirements in their respective fields.\n7. Click on 'Yes', then click on 'Edit'","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-424":{"article":"This policy identifies the MQ brokers that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["BrokerArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon MQ at https://console.aws.amazon.com/amazon-mq.\n2. Click on the required broker.\n3. Under the 'Tags' click on the 'Create tag'.\n4. Click 'Add new tag' and fill the form.\n5. Save changes.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-043":{"article":"The information lifecycle includes information creation, collection, use, processing, storage, maintenance, dissemination, disclosure, disposition. With S3 Lifecycle configuration rules, you can tell Amazon S3 to move objects to less expensive storage classes or archive or delete them. Defining your data lifecycle policies to perform automated storage class migration and deletion will reduce the overall storage costs during its lifetime.\nSetting up an S3 bucket lifecycle policy, will help to cover the \"COST04-BP05 Enforce data retention policies\" best practice from the 'Cost Optimization Pillar' of the 'AWS Well-Architected Framework'.","impact":"Without the S3 lifecycle configuration, you are missing out on the opportunity to manage S3 objects so that they are stored cost-effectively throughout their lifecycle by moving data to more economical storage classes over time or expiring data based on the object age.","report_fields":["Name"],"remediation":"1. Login to the AWS Console. \n2. Find the S3 service. \n3. Choose your S3 bucket. \n4. Click on the Management tab. \n5. Click on the Lifecycle button and add the lifecycle rule. \n6. Follow instructions and add the rule according to your bucket content and mission. \n7. Save it.","multiregional":true,"service":"Amazon S3"},"ecc-aws-051":{"article":"IAM password policies can require passwords to be rotated or expired after a given number of days. It is recommended that the password policy expire passwords after 90 days or less.","impact":"Not reducing the password lifetime decreases account resiliency against brute-force login attempts.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane. \n4. Check 'Enable password expiration'.\n5. Set 'Password expiration period (in days)' to 90 or less.","multiregional":true,"service":"AWS Account"},"ecc-aws-080":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for detecting changes to CloudTrail's configurations. Monitoring changes to CloudTrail's configuration will help ensure sustained visibility to activities performed in the AWS account.","impact":"Lack of monitoring and logging of CloudTrail changes calls can lead to insufficient response time to accidental or intentional changes. In turn, it may lead to disabling the monitoring of all actions taken by a user, role, or AWS service.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = CreateTrail) || ($.eventName = UpdateTrail) || ($.eventName = DeleteTrail) || ($.eventName = StartLogging) || ($.eventName = StopLogging) }}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-468":{"article":"To help you manage your file systems and other Amazon FSx resources, you can assign your own metadata to each resource in the form of tags. Tags enable you to categorize AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type \u2014 you can quickly identify a specific resource based on the tags that you've assigned to it. This topic describes tags and shows you how to create them.\nAmazon FSx can copy tags of the volume to snapshots. If this parameter is set to true, all tags for the volume are copied to snapshots where the user doesn't specify tags. If this value is true and you specify one or more tags, only the specified tags are copied to snapshots. If you specify one or more tags when creating the snapshot, no tags are copied from the volume, regardless of this value.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ResourceARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nYou can create an FSx for OpenZFS file system using the Amazon FSx console:\n1. Open the Amazon FSx console at https://console.aws.amazon.com/fsx/.\n2. On the dashboard, choose 'Create file system' to start the file system creation wizard.\n3. On the 'Select file system type' page, choose 'FSx for OpenZFS', and then choose 'Next'. The 'Create file system' page appears.\n4. For 'Creation method', choose 'Standard create'. Begin your configuration with the File system details section.\n5. For 'File system name - optional', enter a name for your file system.\n6. For 'Storage capacity', enter the storage capacity of your file system, in GiB. Enter any whole number in the range of 64\u2013524288.\n7. For 'Provisioned SSD IOPS', you have two options to provision the number of IOPS for your file system:\n    - Choose 'Automatic' (the default) if you want Amazon FSx to automatically provision 3 IOPS per GB of SSD storage.\n    - Choose 'User-provisioned' if you want to specify the number of IOPS.\n8. For 'Throughput capacity', you have two options to provide your desired throughput capacity in MB per second (MBps).\n    - Choose 'Recommended throughput capacity' (the default) if you want Amazon FSx to automatically choose the throughput capacity. \n  - Choose 'Specify throughput capacity' if you want to specify the throughput capacity value. \n9. In the 'Network & security' section, provide networking and security group information:\n    For 'Virtual Private Cloud (VPC)', choose the Amazon VPC that you want to associate with your file system.\n    For 'VPC Security Groups', the ID for the default security group for your VPC should be already added.\n    For 'Subnet', choose any value from the list of available subnets.\n10. In the 'Encryption' section, for 'Encryption key', choose the AWS Key Management Service (AWS KMS) encryption key that protects your file system's data at rest.\n11. In Root volume configuration, you can set the following options for the file system's root volume:\n    For 'Data compression type', choose the type of compression to use for your volume, either 'Z-Standard', 'LZ4', or 'No compression'. Z-Standard compression provides more data compression and higher read throughput than LZ4 compression. LZ4 compression provides less compression and higher write throughput performance than Z-Standard compression. \n    For 'Copy tags to snapshots', enable the option to copy tags to the volume's snapshot.\n    For 'NFS exports', there is a default client configuration setting which you can modify or remove. \n    To provide additional client configurations:\n        a. In the 'Client addresses' field, specify which clients can access the volume. Enter an asterisk (*) for any client, a specific IP address, or a CIDR range of IP addresses.\n        b. In the 'NFS options' field, enter a comma-delimited set of exports options.\n        c. Choose 'Add client configuration'.\n        d. Repeat the procedure to add another client configuration.\n    For 'Record size', choose whether to use the default suggested record size of 128 KiB, or to set a custom suggested record size for the volume. \n    For 'User and group quotas', you can set a storage quota for a user or group:\n    a. For Quota type, choose USER or GROUP.\n        b. For User or group ID, choose a number that is the ID of the user or group.\n        c. For Usage quota, choose a number that is the storage quota of the user or group.\n        d. Choose Add quota.\n        e. Repeat the procedure to add a quota for another user or group.\n12. In 'Backup and maintenance - optional', you can set the following options:\n    For 'Daily automatic backup', choose 'Enabled' for automatic daily backups. This option is enabled by default.\n    For 'Daily automatic backup window', set the time of the day in Coordinated Universal Time (UTC) that you want the daily automatic backup window to start.\n    For 'Automatic backup retention period', set a period from 1\u201390 days that you want to retain automatic backups.\n    For 'Weekly maintenance window', you can set the time of the week that you want the maintenance window to start. \n13. For 'Tags - optional', you can enter a key and value to add tags to your file system. A tag is a case-sensitive key-value pair that helps you manage, filter, and search for your file system.\n14. Choose 'Next'.\nReview the file system configuration shown on the 'Create file system' page. For your reference, note which file system settings you can modify after the file system is created.\nChoose 'Create file system'.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-434":{"article":"Using the latest version of MQ broker, you adhere to AWS best practices and receive the newest Elasticsearch features, benefit from better performance and security and get the latest bug fixes.","impact":"Without keeping the MQ up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["BrokerArn"],"remediation":"1. Sign in to the AWS Management Console.\n2. Navigate to the Amazon MQ dashboard at https://console.aws.amazon.com/amazon-mq/.\n3. Click on the required broker.\n4. Click on the 'Edit'.\n5. Save changes.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-331":{"article":"WorkSpaces images require Operating system patches to be applied and updated and by confirming the creation date is not over 90 days old can help ensure that updates are being applied.","impact":"WorkSpaces images that have been created more than 90 days ago, pose a security vulnerability because they have not been actively maintained (analyzed, patched, updated).","report_fields":["ImageId"],"remediation":"To create a custom image: \n1. If you are still connected to the WorkSpace, disconnect by choosing Amazon Workspaces and Disconnect in the WorkSpaces client application.\n2. Open the WorkSpaces console at https://console.aws.amazon.com/workspaces/.\n3. In the navigation pane, choose \"WorkSpaces\".\n4. Select the WorkSpace and choose \"Actions\", \"Create Image\". If the status of the WorkSpace is STOPPED, you must start it first (choose \"Actions\", \"Start WorkSpaces\") before you can choose \"Actions\", \"Create Image\".\n5. A message displays, prompting you to reboot (restart) your WorkSpace before continuing. Rebooting your WorkSpace updates your Amazon WorkSpaces software to the latest version.\nReboot your WorkSpace and repeat Step 4 of this procedure, but this time choose Next when the reboot message appears. To create an image, the status of the WorkSpace must be AVAILABLE and its modification state must be None.\n6. Enter an image name and a description that will help you identify the image, and then choose \"Create Image\". While the image is being created, the status of the WorkSpace is SUSPENDED and the WorkSpace is unavailable.\n7. In the navigation pane, choose \"Images\". The image is complete when the status of the WorkSpace changes to \"Available\" (this can take up to 45 minutes).","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-487":{"article":"If you do not specify a Customer managed key when creating your environment, Amazon CodePipeline uses its AWS owned key for data encryption on your environment.\nWhen you create and use your own KMS CMK customer-managed keys to protect the data on your environment, you obtain full control over who can use the CMK keys and access the encrypted data. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs).\nIn addition, if you provide a Customer managed key, you must attach the policy statement for CloudWatch access. You must also create the Customer managed key-specific execution role.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption. If you are using the default S3 key, you cannot change or delete this AWS managed key. If you are using a customer managed key in AWS KMS to encrypt or decrypt artifacts in the S3 bucket, you can change or rotate this customer managed key as necessary.","report_fields":["name","roleArn"],"remediation":"Use AWS CLI to update encryption configuration:\naws codepipeline update-pipeline --cli-input-json file://test.json\nInside json file choose S3 as 'artifactStore'.\nInside 'encryptionKey' provide 'id' for KMS CMK key and in 'type' choose 'KMS'.","multiregional":false,"service":"AWS CodePipeline"},"ecc-aws-484":{"article":"You can configure a deployment group to automatically roll back when a deployment fails or when a monitoring threshold you specify is met. In this case, the last known good version of an application revision is deployed. \nYou can create a CloudWatch alarm that watches a single metric over a time period you specify and performs one or more actions based on the value of the metric relative to a given threshold over a number of time periods. For an Amazon EC2 deployment, you can create an alarm for an instance or Amazon EC2 Auto Scaling group that you are using in your CodeDeploy operations. For an AWS Lambda and an Amazon ECS deployment, you can create an alarm for errors in a Lambda function.\nYou can configure a deployment to stop when an Amazon CloudWatch alarm detects that a metric has fallen below or exceeded a defined threshold.","impact":"Without Automatic rollback option enabled, the failed deployment won't roll back to the most recent successfully deployed application revision or it won't roll back when alarm thresholds are met.","report_fields":["applicationName","deploymentGroupName","deploymentGroupId"],"remediation":"Before beginning to edit Deployment groups, you must have already created the alarm in CloudWatch before you can add it to a deployment group.\nTo enable Automatic rollbacks and CloudWatch alarms:\n1. Navigate to CodeDeploy Applications https://console.aws.amazon.com/codesuite/codedeploy/applications\n2. Select application to which you need to configure Automatic rollback and CloudWatch alarms.\n3. Select 'Deployment groups' tab.\n4. Select deployment group that you want to modify.\n5. Click 'Edit' at top right corner.\n6. Click 'Advanced - optional'.\n7. In 'Alarms' section click 'Add alarm'.\n8. In 'Create deployment alarm' window, select necessary alarms that you need to add. \n9. Click 'Add alarm'.\n10. Check that 'Ignore alarm configuration' is unchecked.\n11. (Optional) If you want deployments to proceed in the event that CodeDeploy is unable to retrieve alarm status from Amazon CloudWatch, choose 'Continue deployments even if alarm status is unavailable'.\n12. In 'Rollbacks' section, uncheck 'Disable rollbacks'.\n13. Choose one or both of the following:\n  a. 'Roll back when a deployment fails'. CodeDeploy will redeploy the last known good revision as a new deployment.\n  b. 'Roll back when alarm thresholds are met'. If you added an alarm to this application in the previous step, CodeDeploy will redeploy the last known good revision when one or more of the specified alarms is activated.\n14. Click 'Save changes'.","multiregional":false,"service":"AWS CodeDeploy"},"ecc-aws-389":{"article":"This policy identifies the Transit gateway attachment that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["TransitGatewayAttachmentId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/.\n2. Click on the 'Transit gateway attachments'.\n3. Click on the required Transit gateway attachment.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"AWS Transit Gateway"},"ecc-aws-071":{"article":"Authentication credentials should never be stored or transmitted in clear text or appear in the repository URL. Instead of personal access tokens or user name and password, you should use OAuth to grant authorization for accessing GitHub or Bitbucket repositories. Using personal access tokens or a user name and password could expose your credentials to unintended data exposure and unauthorized access.","impact":"Using personal access tokens or a user name and password could expose your credentials and lead to unauthorized access to your data.","report_fields":["arn"],"remediation":"1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Select your Build project that contains personal access tokens or a user name and password. \n3. From Edit, choose Source. \n4. Choose Disconnect from GitHub / Bitbucket. \n5. Choose Connect using OAuth and then choose Connect to GitHub / Bitbucket.\n6. In the message displayed by your source provider, authorize as appropriate.\n7. Reconfigure your Repository URL and additional configuration settings as needed.\n8. Choose Update source.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-410":{"article":"This policy identifies the ElasticSearch clusters that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon ElasticSearch at https://console.aws.amazon.com/esv3.\n2. Click on the required domain.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-047":{"article":"Password policies are, in part, used to enforce password complexity requirements. IAM password policies can be used to ensure passwords are comprised of different character sets. It is recommended that the password policy require at least one lowercase letter.","impact":"Not using a password policy with at least one lowercase letter reduces security and account resiliency against brute-force attacks.","report_fields":["account_id","account_name"],"remediation":"1. Login to the AWS Console (with appropriate permissions to View Identity Access Management Account Settings). \n2. Go to IAM Service on the AWS Console.\n3. Click on Account Settings in the Left Pane. \n4. Check 'Requires at least one lowercase letter'.\n5. Click on 'Apply password policy'.","multiregional":true,"service":"AWS Account"},"ecc-aws-532":{"article":"Ensure that certificates stored in AWS ACM are renewed before expiration.\nACM can automatically renew certificates that use DNS validation. For certificates that use email validation, you must respond to a domain validation email. ACM does not automatically renew certificates that you import. You must renew imported certificates manually.","impact":"Certificates that are not renewed prior to their expiration date become invalid, and the communication between a client and an AWS resource that implements the certificates is no longer secure. And it can break applications and reduce customer trust. Furthermore, manually accepting expired certificates as valid can mask other security risks which can lead to phishing, MITM and other identity spoofing attacks.","report_fields":["CertificateArn","DomainName"],"remediation":"For ACM certificates with DNS validation renewal is fully automated, you don't need to take any action.\n\nFor ACM certificates with email validation, you must respond to a domain validation email. \nTo be renewed, email-validated certificates require an action by the domain owner. ACM begins sending renewal notices 45 days before expiration, using the domain's WHOIS mailbox addresses and to five common administrator addresses. The notifications contain a link that the domain owner can click for easy renewal. Once all listed domains are validated, ACM issues a renewed certificate with the same ARN.\n\nFor ACM certificates that you import, ACM does not automatically renew certificates.\nIf you imported a certificate and associated it with other AWS services, you can reimport that certificate before it expires while preserving the AWS service associations of the original certificate. \nThe following conditions apply when you reimport a certificate:\n  - You can add or remove domain names.\n  - You cannot remove all of the domain names from a certificate.\n  - If 'Key Usage' extensions are present in the originally imported certificate, you can add new extension values, but you cannot remove existing values.\n  - If 'Extended Key Usage' extensions are present in the originally imported certificate, you can add new extension values, but you cannot remove existing values.\n  - The key type and size cannot be changed.\n  - You cannot apply resource tags when reimporting a certificate.\n\nThe following example shows how to reimport a certificate using the AWS Management Console.\n1. Open the ACM console at https://console.aws.amazon.com/acm/home.\n2. Select or expand the certificate to reimport.\n3. Open the details pane of the certificate and choose the 'Reimport certificate' button. If you selected the certificate by checking the box beside its name, choose 'Reimport certificate' on the 'Actions' menu.\n4. For 'Certificate body', paste the PEM-encoded end-entity certificate.\n5. For 'Certificate private key', paste the unencrypted PEM-encoded private key associated with the certificate's public key.\n6. (Optional) For 'Certificate chain', paste the PEM-encoded certificate chain. The certificate chain includes one or more certificates for all intermediate issuing certification authorities, and the root certificate. If the certificate to be imported is self-assigned, no certificate chain is necessary.\n7. Choose 'Review and import'.\n8. Review the information about your certificate. If there are no errors, choose 'Reimport'.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-192":{"article":"Elastic Beanstalk enhanced health reporting enables a more rapid response to changes in the health of the underlying infrastructure. These changes could result in a lack of availability of the application. \nElastic Beanstalk enhanced health reporting provides a status descriptor to gauge the severity of the identified issues and identify possible causes to investigate. The Elastic Beanstalk health agent, included in supported Amazon Machine Images (AMIs), evaluates logs and metrics of environment EC2 instances.","impact":"Elastic Beanstalk enhanced health reporting enables a more rapid response to changes in the health of the underlying infrastructure. These changes could result in a lack of the application availability.","report_fields":["EnvironmentArn"],"remediation":"1. Open the Elastic Beanstalk console at console.aws.amazon.com/elasticbeanstalk/ and in the Regions list, select your AWS Region. \n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list. \n3. In the navigation pane, choose Configuration. \n4. In the Monitoring configuration category, choose Edit. \n5. Under Health reporting, for System, choose Enhanced.\n6. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-220":{"article":"If a secret was not accessed within 90 days, this secret should be removed. Deleting unused secrets is as important as rotating secrets.\nUnused secrets can be abused by their former users, who no longer need access to these secrets. Also, as more users get access to a secret, someone might have mishandled and leaked it to an unauthorized entity, which increases the risk of abuse. Deleting unused secrets helps revoke secret access from users who no longer need it. It also helps to reduce the cost of using Secrets Manager. Therefore, it is essential to routinely delete unused secrets.","impact":"If secrets are unused for more than 90 days, it increases the likelihood of old secrets being used by their former users who no longer need access to these secrets.","report_fields":["Name","ARN"],"remediation":"1. Open the Secrets Manager console at https://console.aws.amazon.com/secretsmanager/. \n2. To locate the secret, enter the secret name in the search box. \n3. Choose the secret to delete. \n4. Under Secret details, from Actions, choose Delete secret. \n5. Under Schedule secret deletion, enter the number of days to wait before the secret is deleted. \n6. Choose Schedule deletion.","multiregional":false,"service":"AWS Secrets Manager"},"ecc-aws-141":{"article":"To enable HTTPS connections to your website or application in AWS, you need an SSL/TLS server certificate. You can use ACM or IAM to store and deploy server certificates. Use IAM as a certificate manager only when you must support HTTPS connections in a region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage.\nIAM supports deploying server certificates in all regions, but you must obtain your certificate from an external provider for using it with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.","impact":"Keeping expired SSL/TLS certificates that can be deployed accidentally to a resource such as ELB can damage the credibility of the application/website.","report_fields":["Arn"],"remediation":"Removing expired certificates via the AWS Management Console is not currently supported. To delete SSL/TLS certificates stored in IAM via the AWS API, use the Command Line Interface (CLI).\nFrom Command Line:\nTo delete the expired certificate, run the following command replacing <CERTIFICATE_NAME> with the name of the certificate to delete:\n'aws iam delete-server-certificate --server-certificate-name <CERTIFICATE_NAME>'\nWhen the preceding command is successful, it does not return any output.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-533":{"article":"This policy identifies the Key pairs that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["KeyPairId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EC2 at https://console.aws.amazon.com/ec2.\n2. Click on the 'Key pairs'.\n3. Click on the required key pair.\n4. Click 'Actions' and 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-018":{"article":"IAM users are granted access to services, functions, and data through IAM policies. \nThere are three ways to define policies for a user: \n1) edit the user policy directly (inline); \n2) attach the policy directly to a user; \n3) add the user to an IAM group that has the policy attached.Only the third implementation is recommended.","impact":"Assigning privileges at the IAM user level increases the complexity of access management as the number of users grows. Increased access management complexity might, in turn, increase the opportunity for a principal to inadvertently receive or retain excessive privileges. Also, assigning policies directly to a user can lead to duplication of information.","report_fields":["Arn"],"remediation":"Perform the following to create an IAM group and assign a policy to it:\n1. Sign in to the AWS Management Console and open the IAM console at  https://console.aws.amazon.com/iam/.\n2. In the navigation pane, click Groups and then click Create New Group. \n3. In the Group Name box, type the name of the group and then click Next Step. \n4. In the list of policies, select the check box for each policy that you want to apply to  all members of the group. Then click Next Step. \n5. Click Create Group \nPerform the following to add a user to a given group: \n1. Sign in to the AWS Management Console and open the IAM console at  https://console.aws.amazon.com/iam/. \n2. In the navigation pane, click Groups. \n3. Select the group to add a user to \n4. Click Add Users To Group.\n5. Select the users to be added to the group. \n6. Click Add Users. \nPerform the following to remove the direct association between a user and a policy: \n1. Sign in to the AWS Management Console and open the IAM console at https//console.aws.amazon.com/iam/.\n2. In the left navigation pane, click on Users. \n3. For each user: \n  - Select the user;\n  - Click on the Permissions tab;\n  - Expand Permissions policies;\n  - Click X for each policy; \n  - Then click Detach or Remove (depending on the policy type).","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-470":{"article":"The following types of API endpoints are supported:.\n  - Edge-optimized API endpoint: The default hostname of an API Gateway API that is deployed to the specified Region while using a CloudFront distribution to facilitate client access typically from across AWS Regions.\n  - Private API endpoint: An API endpoint that is exposed through interface VPC endpoints and allows a client to securely access private API resources inside a VPC. Private APIs are isolated from the public internet, and they can only be accessed using VPC endpoints for API Gateway that have been granted access.\n  - Regional API endpoint: The host name of an API that is deployed to the specified Region and intended to serve clients, such as EC2 instances, in the same AWS Region. API requests are targeted directly to the Region-specific API Gateway API without going through any CloudFront distribution.","impact":"Not choosing right endpoint type can lead to exposing API Gateway to the internet or slow connection time geographically diverse clients.","report_fields":["id","name"],"remediation":"1. Sign in to the API Gateway console and choose APIs in the primary navigation pane https://console.aws.amazon.com/apigateway.\n2. Under + Create API, choose the API settings (gear icon).\n3. Under Endpoint Configuration, change the 'Endpoint Type' option to required type.\n4. Choose Save to start the update.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-346":{"article":"Amazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources. After you create a health check, you can get the status of the health check, get notifications when the status changes, and configure DNS failover.","impact":"Without health check configured for Route53 resource record, you are missing out on the opportunity to improve monitoring and response time for any service issues.","report_fields":["c7n:parent-id","Name","Type","SetIdentifier"],"remediation":"The following procedure describes how to create health checks using the Route 53 console.\n1. Sign in to the AWS Management Console and open the Route 53 console at https://console.aws.amazon.com/route53/.\n2. In the navigation pane, choose 'Health Checks'.\n3. To create a health check, choose 'Create health check'. For more information about each setting, move the mouse pointer over a label to see its tooltip.\n4. Enter the applicable values. Note that some values can't be changed after you create a health check.\n5. Choose 'Create health check'.\n  Note: Route 53 considers a new health check to be healthy until there's enough data to determine the actual status, healthy or unhealthy. If you chose the option to invert the health check status, Route 53 considers a new health check to be unhealthy until there's enough data.\n6. Associate the health check with one or more Route 53 records.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-239":{"article":"The log_error_verbosity setting specifies the verbosity (amount of detail) of logged messages. Valid values are: TERSE, DEFAULT, VERBOSE, with each containing the fields of the level above it as well as additional fields. TERSE excludes the logging of DETAIL, HINT, QUERY, and CONTEXT error information. VERBOSE output includes the SQLSTATE, error code, and the source code file name, function name, and line number that generated the error. The appropriate value should be set based on your organization's logging policy.","impact":"If the 'log_error_verbosity' parameter is not set to the correct value, too many details or too few details may be logged.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type 'log_error_verbosity'.\n6. Choose value that must be set.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-060":{"article":"AWS CloudTrail is a web service that records AWS API calls for an account and makes those logs available to users and resources in accordance with IAM policies. AWS Key Management Service (KMS) is a managed service that helps create and control the encryption keys used to encrypt account data, and uses Hardware Security Modules (HSMs) to protect the security of encryption keys.\nCloudTrail logs can be configured to leverage server side encryption (SSE) and KMS customer created master keys (CMK) to further protect CloudTrail logs. \nIt is recommended that CloudTrail be configured to use SSE-KMS.","impact":"Without CloudTrail configured to use SSE-KMS, you do not have full control over who can read the log files in your organization. This can lead to unauthorized access to CloudTrail logs.","report_fields":["TrailARN"],"remediation":"1. Sign in to the AWS Management Console and open the CloudTrail console at https://console.aws.amazon.com/cloudtrail.\n2. In the left navigation pane, choose Trails. \n3. Click on a Trail.\n4. Under the S3 section, click on the edit button (pencil icon).\n5. Click on Advanced.\n6. Select an existing CMK from the KMS key ID drop-down menu.\nNote: Ensure the CMK is located in the same region as the S3 bucket.\nNote: You will need to apply a KMS Key policy on the selected CMK in order for CloudTrail as a service to encrypt and decrypt log files using the CMK provided. Steps are provided here for editing the selected CMK Key policy.\n7. Click on Save. \n8. You will see a notification message stating that you need to have decrypt permissions on the specified KMS key to decrypt log files. \n9. Click Yes.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-376":{"article":"To help debug issues related to request execution to API Gateway, you can enable Amazon CloudWatch Logs to log API calls. \nExecution logs contain information that you can use to identify and troubleshoot most API errors.","impact":"With disabled monitoring and logging of API Gateway, it may be difficult to troubleshoot any issues that might happen with APIs.","report_fields":["restApiId","StageName"],"remediation":"Create an IAM role for logging to CloudWatch:\n1. In the AWS Identity and Access Management (IAM) console, in the left navigation pane, choose Roles.\n2. On the Roles pane, choose Create role.\n3. On the Create role page, do the following:\n  3.1 For Trusted entity type, choose AWS Service.\n  3.2 For use case, choose API Gateway.\n  3.3 Choose the API Gateway radio button.\n  3.4 Choose Next.\n4. Under Permissions Policies, note that the AWS managed policy AmazonAPIGatewayPushToCloudWatchLogs is selected by default. The policy has all the required permissions.\n5. Choose Next.\n6. Under Name, review and create, do the following:\n  6.1 For Role name, enter a meaningful name for the role.\n  6.2 (Optional) For Role description, edit the description to your preferences.\n  6.3 (Optional) Add tags.\n  6.4 Choose Create role.\n7. On the Roles pane, in the search bar, enter the name of the role that you created. Then, choose the role from the search results.\n8. On the Summary pane, copy the Role ARN. You'll need this Amazon Resource Name (ARN) in the next section.\n\nAdd the IAM role in the API Gateway console:\nNote: If you're developing multiple APIs across different AWS Regions, complete these steps in each Region.\n1. In the API Gateway console, on the APIs pane, choose the name of an API that you created.\n2. In the left navigation pane, at the bottom, below the Client Certificates section, choose Settings.\n3. Under Settings, for CloudWatch log role ARN, paste the IAM role ARN that you copied.\n4. Choose Save.\nNote: The console doesn't confirm that the ARN is saved.\n\nTurn on logging for your API and stage:\n1. In the API Gateway console, find the Stage Editor for your API.\n2. On the Stage Editor pane, choose the 'Logs/Tracing' tab.\n3. On the 'Logs/Tracing' tab, under 'CloudWatch Settings', do the following to turn on execution logging:\n  3.1 Choose the 'Enable CloudWatch Logs' check box.\n  3.2 For 'Log level', choose INFO to generate execution logs for all requests. Or, choose ERROR to generate execution logs only for requests to your API that result in an error.\n4. Under 'Custom Access Logging', do the following to turn on access logging:\n  4.1 Choose the 'Enable Access Logging' check box.\n  4.2 For 'Access Log Destination ARN', enter the ARN of a CloudWatch log group or an Amazon Kinesis Data Firehose stream.\n  4.3 Enter a Log Format. For guidance, choose CLF, JSON, XML, or CSV to see an example in that format.\n5. Choose Save Changes.\nNote: The console doesn't confirm that settings are saved.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-232":{"article":"The log_rotation_size setting determines the maximum size of an individual log file. \nOnce the maximum size is reached, automatic log file rotation will occur. Verify that log_rotation_size is set in compliance with the organization's logging policy.","impact":"When this parameter is disabled. This will prevent automatic log file rotation when files become too large, which could put log data at increased risk of loss (unless age-based rotation is configured).","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_rotation_size\".\n6. Type for example 100000 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-009":{"article":"Ensure that SSL/TLS certificates stored in AWS IAM are renewed one week before expiry.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and an AWS resource that implements the certificates is no longer secure.","report_fields":["Arn"],"remediation":"From AWS CLI: \naws iam upload-server-certificate --server-certificate-name ExampleCertificate --certificate-body file://Certificate.pem --certificate-chain file://CertificateChain.pem --private-key file://PrivateKey.pem --tags '{\"Key\": \"ExampleKey\", \"Value\": \"ExampleValue\"}'","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-001":{"article":"Multi-Factor Authentication (MFA) adds an extra layer of authentication assurance beyond traditional credentials. With MFA enabled, when a user signs in to the AWS Console, they will be prompted for their user name and password as well as for an authentication code from their physical or virtual MFA token. It is recommended that MFA be enabled for all accounts that have a console password.","impact":"With MFA disabled for all IAM users who have a console password, the likelihood of a password being cracked and an account being compromised is much higher than with MFA enabled. By requiring MFA, you can reduce incidents of compromised accounts and keep sensitive data from being accessed by unauthorized users.","report_fields":["Arn"],"remediation":"1. Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. \n2. In the navigation pane, choose Users. \n3. Choose the name of the intended MFA user from the User Name list. \n4. Choose the Security Credentials tab and then choose Manage MFA Device. \n5. In the Manage MFA Device wizard, choose a virtual MFA device and then choose Continue. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the 'secret configuration key' that is available for manual entry on devices that do not support QR codes. \n6. Open your virtual MFA application. (For a list of apps that you can use for hosting virtual MFA devices, see a list of compatible applications: https://aws.amazon.com/iam/features/mfa/?audit=2019q1). If the virtual MFA application supports multiple accounts (multiple virtual MFA devices), choose the option to create a new account (a new virtual MFA device). \n7. Determine whether the MFA app supports QR codes, and then do one of the following: \n- Use the app to scan the QR code. For example, you might choose either a camera icon or an option similar to Scan code and then use the device's camera to scan the code. \n- In the Manage MFA Device wizard, choose Show secret key for manual configuration, and then type the secret configuration key into your MFA application. When finished, the virtual MFA device starts generating one-time passwords.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-379":{"article":"This policy identifies the EBS snapshots that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["SnapshotId","OwnerId"],"remediation":"1. Sign in to the AWS Management Console and open the EC2 console at https://console.aws.amazon.com/ec2/v2/.\n2. In the navigation pane under 'Elastic Block Store' choose 'Snapshots'.\n3. Choose required snapshot.\n4. Open 'Tags' and click on 'Manage tags'.\n5. Add a tags.\n6. Save.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-283":{"article":"Using the latest version of ElasticSearch engine, you adhere to AWS best practices and receive the newest ElasticSearch features, benefit from better performance and security and get the latest bug fixes.","impact":"Without keeping the OpenSearch up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Management Console and open the Amazon OpenSearch console at https://console.aws.amazon.com/esv3\n  1.1. (Optional) Take a manual snapshot of your domain. This snapshot serves as a backup that you can restore on a new domain if you want to return to using the prior OpenSearch version.\n2. In the navigation pane, under Domains, choose the domain you want to upgrade.\n3. Choose Actions and Upgrade.\n4. Choose the version to upgrade to. If you're upgrading to an OpenSearch version, the Enable compatibility mode option appears. If you enable this setting, OpenSearch reports its version as 7.10 to allow Elasticsearch OSS clients and plugins like Logstash to continue working with Amazon OpenSearch Service.\n5. Choose Upgrade.\n6. Check the Status on the domain dashboard to monitor the status of the upgrade.","multiregional":false,"service":"Amazon OpenSearch Service"},"ecc-aws-216":{"article":"Enabling automatic major version upgrades ensures that the latest major version updates to Amazon Redshift clusters are installed during the maintenance window. These updates might include security patches and bug fixes. Keeping up to date with patch installation is an important step in securing systems.","impact":"With an automatic update disabled, you are missing updates that may contain security patches and bug fixes.","report_fields":["ClusterIdentifier"],"remediation":"To remediate this issue from the AWS CLI, use the Amazon Redshift modify-cluster command to set the --allow-version-upgrade attribute:\naws redshift modify-cluster --cluster-identifier clustername --allow-version-upgrade","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-553":{"article":"Identifying unused Classic Load Balancers (CLBs) will reduce the costs of your AWS bill. A CLB is considered to be unused when it does not have registered instances.\nUnless there is a business need to retain unused CLBs, you should remove them to avoid incurring unexpected charges and to maintain an accurate inventory of system components.","impact":"Keeping unused Classic Load Balancers can result in escalating costs and cluttered AWS accounts.","report_fields":["LoadBalancerArn"],"remediation":"To delete Classic Load Balancer:\n  1. Sign in to the Amazon EC2 console and access the Amazon EC2 at https://console.aws.amazon.com/ec2/v2.\n  2. On the console, select the specific region. \n  3. Under the 'Load Balancing' click on the 'Load Balancers'.\n  4. Select a load balancer.\n  4. Click on the 'Actions' button and click on 'Delete load balancer'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-541":{"article":"AWS Glue provides real-time, continuous logging for AWS Glue jobs. You can view real-time Apache Spark job logs in Amazon CloudWatch, including driver logs, executor logs, and an Apache Spark job progress bar. Viewing real-time logs provides you with a better perspective on the running job.","impact":"With disabled logging of Glue Job, it may be difficult to troubleshoot any issues that might happen with jobs.","report_fields":["Name"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Glue at https://console.aws.amazon.com/glue.\n2. Under the 'AWS Glue Studio' click on the 'Jobs'.\n3. Click on the required job.\n4. Open 'Job details' and click on the 'Advanced properties'.\n5. Enable the 'Continuous logging'.\n6. Save changes.","multiregional":false,"service":"AWS Glue"},"ecc-aws-480":{"article":"Codebuild a fully managed build service that compiles source code, run unit tests and produces artifacts.","impact":"Disabled encryption allows unauthorized users to gain access to Codebuild artifacts. With encryption enabled, this data is protected.","report_fields":["arn"],"remediation":"1. Open the CodeBuild console at https://console.aws.amazon.com/codebuild/.\n2. Choose required project.\n3. Open 'Build details'. \n4. Under 'Artifacts' click 'Edit'.\n5. Uncheck 'Disable artifact encryption'.","multiregional":false,"service":"AWS CodeBuild"},"ecc-aws-237":{"article":"PostgreSQL does not maintain an internal record of attempted connections to the database for later auditing. It is only by enabling the logging of these attempts that one can determine if unexpected attempts are being made. Note that enabling this without also enabling log_disconnections provides little value. Generally, you would enable/disable the pair together.","impact":"Without the log_connections logs setting enabled, attempted connections will not be recorded. This makes harder to diagnose issues or detect different types of attacks, as well as retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_connections\".\n6. Choose 1 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-353":{"article":"Amazon Redshift logs information about connections and user activities in your database can help you to monitor the database for security and troubleshooting purposes.","impact":"If logs information will not record, there will be no opportunity to do any analysis, discover the cause of problems and troubleshoot them.","report_fields":["ClusterIdentifier"],"remediation":"To enable audit logging for a cluster:\n  1. Sign in to the AWS Management Console and open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n  2. On the navigation menu, choose Clusters, then choose the cluster that you want to update.\n  3. Choose the Properties tab. On the Database configurations panel, choose Edit, then Edit audit logging.\n  4. On the Edit audit logging page, choose Turn on and select S3 bucket or CloudWatch. Amazon recommends using CloudWatch because administration is easy and it has helpful features for data visualization.\n  5. Choose which logs to export.\n  6. To save your choices, choose Save changes.\n\nTo change default parameter group:\n  1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n  2. In the navigation panel, choose Provisioned clusters dashboard.\n  3. Choose the required Cluster.\n  4. Choose Properties.\n  5. Under Database configurations, choose Edit, Edit parameter group.\n  6. Choose existing parameter or create new parameter group.\n\nTo update parameter in existing non default parameter group:\n  1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n  2. In the navigation panel, choose Provisioned clusters dashboard.\n  3. Choose the required Cluster.            \n  4. Choose Properties.\n  5. Click on Parameter group name.\n  6. Click on Parameters.\n  7. Click Edit Parameters\n  8. Set 'enable_user_activity_logging' to true.\n  9. Choose Save.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-211":{"article":"IAM database authentication allows for password-free authentication to database instances. The authentication uses an authentication token. Network traffic to and from the database is encrypted using SSL.","impact":"Without IAM Database Authentication, you are missing the opportunity to use in-transit encryption. It may lead to Internet traffic exposure resulting in a breach of confidentiality and centralized access management to all databases.","report_fields":["DBClusterArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/. \n1. Choose Databases. \n2. Choose the DB cluster to modify. \n3. Choose Modify. \n4. Under Database options, select Enable IAM DB authentication. \n5. Choose Continue. \n6. Under Scheduling of modifications, choose when to apply modifications: Apply during the next scheduled maintenance window or Apply immediately. \n7. Choose Modify cluster.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-422":{"article":"This policy identifies the Lightsail instance that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["arn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Lightsail at https://lightsail.aws.amazon.com/ls/webapp/home/instances.\n2. Click on the required lightsail instance.\n3. Click on three dots and open Tags.\n4. Add key-value tag.","multiregional":false,"service":"Amazon Lightsail"},"ecc-aws-375":{"article":"WorkSpaces is integrated with the AWS Key Management Service (AWS KMS). This enables you to encrypt storage volumes of WorkSpaces using AWS KMS Key. When you launch a WorkSpace, you can encrypt the root volume (for Microsoft Windows, the C drive; for Linux, /) and the user volume (for Windows, the D drive; for Linux, /home). Doing so ensures that the data stored at rest, disk I/O to the volume, and snapshots created from the volumes are all encrypted. You need an AWS KMS Key before you can begin the encryption process. You can select a symmetric customer managed KMS Key that you created using AWS KMS. You can view, use, and manage this KMS Key, including setting its policies.","impact":"Not encrypting data at rest can lead to unauthorized access to sensitive data. Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["WorkspaceId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt existing AWS WorkSpaces data you must re-create the necessary WorkSpaces instances with the volumes encryption feature enabled as outlined above. Perform the following steps to encrypt WorkSpace volumes:\n1. Login to the WorkSpaces console at https://console.aws.amazon.com/workspaces/.\n2. Click Launch WorkSpaces and complete the first three steps.\n3. For the WorkSpaces Configuration step, do the following: \n  - Select the volumes to encrypt: Root Volume, User Volume, or both volumes. \n  - For Encryption Key, select an AWS KMS CMK. The CMK that you select must be symmetric.\n4. Click Next Step.\n5. Click Launch WorkSpaces.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-542":{"article":"Auto Scaling is available for your AWS Glue ETL and streaming jobs with AWS Glue version 3.0 or later.\nWith Auto Scaling enabled, you will get the following benefits:\n  - AWS Glue automatically adds and removes workers from the cluster depending on the parallelism at each stage or microbatch of the job run.\n  - It removes the need for you to experiment and decide on the number of workers to assign for your AWS Glue ETL jobs.\n  - If you choose the maximum number of workers, AWS Glue will choose the right size resources for the workload.\n  - You can see how the size of the cluster changes during the job run by looking at CloudWatch metrics on the job run details page in AWS Glue Studio.\nAuto Scaling helps with cost and utilization for your applications.","impact":"Disabling autoscaling in AWS Glue can result in inefficient resource usage, reduced performance, limited scalability, and additional operational overhead.","report_fields":["Name"],"remediation":"1. Log in to the AWS Console and navigate to the AWS Glue console.\n2. Click on the \"ETL Jobs\" tab to view your existing jobs.\n3. Select the job for which you want to enable autoscaling.\n4. Navigate to the \"Job details\" to edit the job configuration.\n5. Enable the \"Automatically scale the number of workers\" parameter.\n6. Set up the \"Maximum number of workers\" parameter.\n7. Click on the \"Save\" button to save the changes to your job configuration.\nOnce the changes are saved, AWS Glue will automatically scale up or down the number of workers based on your workload.","multiregional":false,"service":"AWS Glue"},"ecc-aws-163":{"article":"If you use a known port to deploy an RDS instance, an attacker can guess the information about the instance. The attacker can use this information together with other information to connect to the RDS instance or gain additional information about your application.\nWhen you change the port, you must also update the existing connection strings that were used to connect to the old port. You should also check the security group of the DB instance to ensure that it includes an ingress rule that allows connectivity to the new port.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Databases.\n3. Select the DB instance to modify.\n4. Choose Modify.\n5. Under Database options, change Database port to a non-default value.\n6. Choose Continue.\n7. Under Scheduling of modifications, choose when to apply modifications. You can choose either Apply during the next scheduled maintenance window or Apply immediately.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-349":{"article":"Amazon Route 53 log information about the public DNS queries that Route 53 receives, such as the following: \n  - Domain or subdomain that was requested; \n  - Date and time of the request; \n  - DNS record type (such as A or AAAA);\n  - Route 53 edge location that responded to the DNS query;\n  - DNS response code, such as NoError or ServFail.","impact":"With disabled logs for the Route53, it is harder to analyze statistics, diagnose issues, detect different types of attacks.","report_fields":["Id","Name"],"remediation":"To enable Route53 query logging:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/route53/v2/.\n2. In the navigation pane, choose 'Hosted zones'.\n3. Click on the Hosted zone that you want to modify.\n4. Click on the 'Configure query logging'.\n5. Under 'Permissions' tab and grant permissions to publish logs. \n6. Choose existing or create a new Log group.\n7. CHoose 'Create'.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-526":{"article":"A WAF global rule group can contain multiple rules. The rule's conditions allow for traffic inspection and take a defined action (allow, block, or count).","impact":"Without any rules, the traffic passes without inspection. A WAF global rule group with no rules, but with a name or tag suggesting allow, block, or count, could lead to the wrong assumption that one of those actions is occurring.","report_fields":["Name","RuleGroupId"],"remediation":"To delete an empty WAF rule group:\n1. Sign in to the AWS Management Console using the AWS Firewall Manager administrator account that you set up in the prerequisites, and then open the Firewall Manager console at https://console.aws.amazon.com/wafv2/fms.\n2. In the navigation pane, choose 'Switch to AWS WAF Classic'.\n3. In the 'AWS WAF Classic' navigation pane, choose 'Rule groups'.\n4. Choose an empty rule group that you want to delete.\n5. Click 'Delete'\n\nTo add rules to an empty WAF rule group:\n1. Sign in to the AWS Management Console using the AWS Firewall Manager administrator account that you set up in the prerequisites, and then open the Firewall Manager console at https://console.aws.amazon.com/wafv2/fms.\n2. In the navigation pane, choose 'Switch to AWS WAF Classic'.\n3. In the 'AWS WAF Classic' navigation pane, choose 'Rule groups'.\n4. Choose a rule group to which you want to add rules.\n5. Click 'Edit rule group'.\n6. From 'Rules' drop-down list select rules that you want to attach to this rule group. Click 'Add rule to rule group'\n7. Set the right order and 'Action'.\n8. Click 'Update'.","multiregional":true,"service":"AWS Web Application Firewall"},"ecc-aws-248":{"article":"You can use AWS WAF to protect your API Gateway API from common web exploits, such as SQL injection and cross-site scripting (XSS) attacks. These could affect API availability and performance, compromise security, or consume excessive resources. \nFor example, you can create rules to allow or block requests from specified IP address ranges, requests from CIDR blocks, requests that originate from a specific country or region, requests that contain malicious SQL code, or requests that contain malicious script.","impact":"Not enabling Api gateway WAF integration could affect API availability and performance, compromise security, or consume excessive resources.","report_fields":["stageName","restApiId"],"remediation":"1. Sign in to the API Gateway console at https://console.aws.amazon.com/apigateway.\n2. In the APIs navigation pane, choose the API, and then choose Stages.\n3. In the Stages pane, choose the name of the stage.\n4. In the Stage Editor pane, choose the Settings tab.\n5. To associate a Regional web ACL with the API stage:\n5.1. In the AWS WAF web ACL dropdown list, choose the Regional web ACL that you want to associate with this stage.\nNote. If the web ACL you need doesn't exist yet, choose Create WebACL. Then choose Go to AWS WAFto open the AWS WAF console in a new browser tab and create a Regional web ACL. Then return to the API Gateway console to associate the web ACL with the stage.\n6. Choose Save Changes.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-345":{"article":"To improve security, you should restrict the connections of unnecessary protocols and ports by properly configuring your Amazon VPC Security Group. For instance, to restrict access to most protocols while allowing access to OpenWire and the web console, you could allow access to only 61617 and 8162.","impact":"Allowed inbound traffic on all ports can increase opportunities for malicious activities such as unauthorized access, Man-In-The-Middle attacks (MITM), and brute-force attacks that raise the risk of resource compromising.","report_fields":["BrokerArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon MQ at https://console.aws.amazon.com/amazon-mq.\n2. Click on the required broker.\n3. Click 'Edit'.\n4. Under 'Security and network' choose existing security group restricted to only ports '8162' and '61617' or create a new one.\n5. Save changes.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-330":{"article":"When data changing statements are made (i.e., INSERT, UPDATE), MySQL can handle invalid or missing values differently depending on whether strict SQL mode is enabled.\nWhen strict SQL mode is enabled, data may not be truncated or otherwise \"adjusted\" to make the data changing statement work.","impact":"Without strict mode the server tries to proceed with the action when an error might have been a more secure choice. For example, by default MySQL will truncate data if it does not fit in a field, which can lead to unknown behavior, or be leveraged by an attacker to circumvent data validation.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type 'sql_mode'.\n6. Type 'STRICT_ALL_TABLES'\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-297":{"article":"AWS Elastic Beanstalk regularly releases platform updates to provide fixes, software updates, and new features. With managed platform updates, you can configure your environment to automatically upgrade to the latest version of a platform during a scheduled maintenance window. Your application remains in service during the update process with no reduction in capacity.","impact":"With an automatic update disabled, you are missing updates that may contain new software features, bug fixes, security patches and performance improvements.","report_fields":["EnvironmentArn"],"remediation":"1. Login to the AWS Management Console and open the Amazon Elastic Beanstalk console using https://console.aws.amazon.com/elasticbeanstalk/\n2. In the navigation pane, choose Environments, and then choose the name of your environment from the list.\n3. In the navigation pane, choose Configuration.\n4. In the Managed updates category, choose Edit.\n5. Disable or enable Managed updates.\n6. If managed updates are enabled, select a maintenance window, and then select an Update level.\n7. Choose Apply.","multiregional":false,"service":"AWS Elastic Beanstalk"},"ecc-aws-213":{"article":"RDS DB clusters should be configured for multiple Availability Zones to ensure availability of the data that is stored. Deployment to multiple Availability Zones allows for automated failover in the event of an Availability Zone availability issue and during regular RDS maintenance events.","impact":"Disabled multiple Availability Zones for RDS DB clusters threaten the availability of stored data.","report_fields":["DBClusterArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases, and then choose the DB instance to modify. \n3. Choose Modify. The Modify DB Instance page appears. \n4. Under Instance Specifications, set Multi-AZ deployment to Yes. \n5. Choose Continue and check the summary of modifications. \n6. (Optional) Choose Apply immediately to apply the changes immediately. Choosing this option can cause an outage in some cases.\n7. On the confirmation page, review your changes. If they are correct, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-327":{"article":"Elastic Compute Cloud (EC2) supports encryption at rest when using the Elastic Block Store (EBS) service.","impact":"Disabled encryption allows unauthorized or anonymous users to gain access to EBS containing sensitive data. With Elastic Block Store encryption enabled, the data stored on the snapshots is encrypted.","report_fields":["SnapshotId","OwnerId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt existing EBS Snapshot:\n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/.\n2. Under Elastic Block Store, click on Snapshots.\n3. Click on required snapshot.\n4. Click on the Actions.\n5. Click on the Copy snapshot.\n6. Under Encryption choose 'Encrypt this snapshot'.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-072":{"article":"This ensures that the group can determine an instance's health based on additional tests provided by the load balancer. Using Elastic Load Balancing health checks can help support the availability of applications that use EC2 Auto Scaling groups.","impact":"Not using an ELB health check can lead to a longer response time or no response to website availability issues.","report_fields":["AutoScalingGroupARN"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. On the navigation pane, under Auto Scaling, choose Auto Scaling Groups.\n3. To select the group from the list, choose the right box.\n4. From Actions, choose Edit.\n5. For Health Check Type, choose ELB. \n6. For Health Check Grace Period, enter 300.\n7. Choose Save.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-202":{"article":"Amazon RDS, Oracle database engine logs (Alert, Audit, Trace, Listener) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB instance that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Alert, Audit, Trace and Listener log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-010":{"article":"If you have an HTTPS listener, you deployed an SSL server certificate on your load balancer when you created the listener. Each certificate comes with a validity period. You must ensure that you renew or replace the certificate before its validity period ends.\nCertificates provided by AWS Certificate Manager and deployed on your load balancer can be renewed automatically. ACM attempts to renew certificates before they expire. If you imported a certificate into ACM, you must monitor the expiration date of the certificate and renew it before it expires. After a certificate that is deployed on a load balancer is renewed, new requests use the renewed certificate.","impact":"SSL/TLS certificates not renewed prior to their expiration date become invalid and the communication between a client and an AWS resource that implements the certificates (e.g. AWS ELB) is no longer secure.","report_fields":["LoadBalancerArn"],"remediation":"1. Login to the AWS EC2 console at console.aws.amazon.com/ec2/.\n2. Navigate to the 'Load Balancer' section, and then to the 'Listener' tab.  \n3. Select the listener (HTTPS) and select the 'View/edit certificates' tab.  \n4. Add the new certificate (from IAM) for each instance that failed the rule.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-299":{"article":"Field-level encryption adds an additional layer of security that lets you protect sensitive data, such as credit card numbers or personally identifiable information (PII) like social security numbers throughout system processing so that only certain applications can see it.\nWhen the HTTPS request with field-level encryption is forwarded to the origin, and the request is routed throughout your origin application or subsystem, the sensitive data is still encrypted, reducing the risk of a data breach or accidental data loss of the sensitive data. Components that need access to the sensitive data for business reasons, such as a payment processing system needing access to a credit number, can use the appropriate private key to decrypt and access the data.","impact":"Without field-level encryption, sensitive information could be exposed to unauthorized processes and pose a threat of data breach or accidental data loss.","report_fields":["ARN"],"remediation":"To enable field-level encryption for your Amazon CloudFront web distributions, perform the following actions: \n\nStep 1: Create an RSA key pair\n1. For example, if you\u2019re using OpenSSL, you can use the following command to generate a key pair with a length of 2048 bits and save it in the file private_key.pem: \n  'openssl genrsa -out private_key.pem 2048'\n2. The resulting file contains both the public and the private key. To extract the public key from that file, run the following command:\n  'openssl rsa -pubout -in private_key.pem -out public_key.pem'\n  The public key file (public_key.pem) contains the encoded key value that you paste in the following step.\n\nStep 2: Add your public key to CloudFront\nAfter you get your RSA key pair, add your public key to CloudFront.\nTo add your public key to CloudFront:\n1. Sign in to the AWS Management Console and open the CloudFront console at https://console.aws.amazon.com/cloudfront/v3/home.\n2. In the navigation pane, choose Public key.\n3. Choose Add public key.\n4. For Key name, type a unique name for the key. The name can't have spaces and can include only alphanumeric characters, underscores (_), and hyphens (-). The maximum number of characters is 128.\n5. For Key value, paste the encoded key value for your public key, including the -----BEGIN PUBLIC KEY----- and -----END PUBLIC KEY----- lines.\n6. For Comment, add an optional comment. For example, you could include the expiration date for the public key.\n7. Choose Add.\n\nStep 3: Create a profile for field-level encryption\nAfter you add at least one public key to CloudFront, create a profile that tells CloudFront which fields to encrypt.\nTo create a profile for field-level encryption:\n1. In the navigation pane, choose Field-level encryption.\n2. Choose Create profile.\n3. Fill in the fields.\n4. After you fill in the fields, choose Create profile.\n\nStep 4: Create a configuration\nTo create a configuration for field-level encryption:\n1. On the Field-level encryption page, choose Create configuration.\n2. Fill in the 'Default profile ID' field to specify the profile to use. \n3. If you want to change the CloudFront default behavior for the following options, select the appropriate check box.\n  a) Forward request to origin when request\u2019s content type is not configured. Select the check box if you want to allow the request to go to your origin if you have not specified a profile to use for the content type of the request.\n  b) Override the profile for a content type with a provided query argument. Select the check box if you want to allow a profile provided in a query argument to override the profile that you\u2019ve specified for a content type.\n4. If you select the check box to allow a query argument to override the default profile, you must complete the additional fields for the configuration. \n\nStep 5: Add a configuration to a cache behavior\n1. Go back to the navigation panel and choose Distributions. \n2. Select the distribution that you want to reconfigure.\n3. Choose the Behaviors tab and select the default behavior for the distribution.\n4. Click the Edit button. \n5. On the Edit Behavior page, select the ID of the field-level encryption configuration. Note that you can set the field-level encryption configuration only when Viewer Protocol Policy and Origin Protocol Policy settings are using HTTPS. \n6. Click 'Save changes'","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-525":{"article":"A WAF global rule can contain multiple conditions. A rule's conditions allow for traffic inspection and take a defined action (allow, block, or count).","impact":"Without any conditions, the traffic passes without inspection. A WAF global rule with no conditions, but with a name or tag suggesting allow, block, or count, could lead to the wrong assumption that one of those actions is occurring.","report_fields":["Name","RuleId"],"remediation":"To delete an empty WAF rule:\n1. Sign in to the AWS Management Console and open the AWS WAF console at https://console.aws.amazon.com/wafv2/.\n2. If you see 'Switch to AWS WAF Classic' in the navigation pane, select it.\n3. In the navigation pane, choose 'Rules'.\n4. Select an empty rule that you want to delete.\n5. Click 'Delete'.\n\nTo add conditions to an empty WAF rule:\n1. Sign in to the AWS Management Console and open the AWS WAF console at https://console.aws.amazon.com/wafv2/.\n2. If you see 'Switch to AWS WAF Classic' in the navigation pane, select it.\n3. In the navigation pane, choose 'Rules'.\n4. Select an empty rule to which you want to add condition.\n5. Click 'Edit rule'.\n6. To add an existing condition to the rule, specify the following values:\n    When a request does/does not\n    a) If you want AWS WAF Classic to allow or block requests based on the filters in a condition, choose 'does'. For example, if an IP match condition includes the IP address range 192.0.2.0/24 and you want AWS WAF Classic to allow or block requests that come from those IP addresses, choose does.\n    b) If you want AWS WAF Classic to allow or block requests based on the inverse of the filters in a condition, choose 'does not'. For example, if an IP match condition includes the IP address range 192.0.2.0/24 and you want AWS WAF Classic to allow or block requests that do not come from those IP addresses, choose does not.\n    \n    match/originate from\n    Choose the type of condition that you want to add to the rule:\n    a) Cross-site scripting match conditions - choose 'match at least one of the filters in the cross-site scripting match condition'\n    b) IP match conditions - choose 'originate from an IP address in'\n    c) Geo match conditions - choose 'originate from a geographic location in'\n    d) Size constraint conditions - choose 'match at least one of the filters in the size constraint condition'\n    e) SQL injection match conditions - choose 'match at least one of the filters in the SQL injection match condition'\n    f) String match conditions - choose 'match at least one of the filters in the string match condition'\n    g) Regular expression match conditions - choose 'match at least one of the filters in the regex match condition'\n7. Click 'Add condition'.\n8. When you're finished adding conditions, choose 'Update'.","multiregional":true,"service":"AWS Web Application Firewall"},"ecc-aws-178":{"article":"X-Ray active tracing enables a more rapid response to performance changes in the underlying infrastructure. Changes in performance could result in a lack of availability of the API. X-Ray active tracing provides real-time metrics of user requests that flow through your API Gateway REST API operations and connected services.","impact":"Disabled AWS X-Ray makes it harder to analyze user requests as they flow through the AWS API Gateway APIs to underlying services. It results in a slower response to performance changes in the underlying infrastructure.","report_fields":["stageName","restApiId"],"remediation":"1. Sign in to the API Gateway console at https://console.aws.amazon.com/apigateway.\n2. In the APIs pane, choose the API, and then choose Stages.\n3. In the Stages pane, choose the name of the stage.\n4. In the Stage Editor pane, choose the Logs/Tracing tab.\n5. To enable active X-Ray tracing, choose Enable X-Ray Tracing under X-Ray Tracing.\n6. If desired, choose Set X-Ray Sampling Rules and go to the X-Ray console to configure sampling rules.\n\nOnce you've enabled X-Ray for your API stage, you can use the X-Ray management console to view the traces and service map.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-315":{"article":"Enabling basic auditing features for the Oracle instance permits the collection of data to troubleshoot problems and provides valuable forensic logs in the case of a system breach.\nValues are: DB, OS, NONE, XML, EXTENDED.","impact":"If audit_trail is disabled than there will be no data to troubleshoot problems and provides valuable forensic logs in the case of a system breach.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"audit_trail\".\n6. Choose XML or other values according to your organization policy\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-369":{"article":"Use Cloudwatch to store/archive WorkSpaces login events for future reference, analysis, and action based on the patterns. Utilize the IP address collected to figure out where users are logged in from, and then build policies to allow access only to files or data from those WorkSpaces that meet company access criteria. With this information you can also use policy controls to block access from unauthorized IP addresses.","impact":"Not logging the login events for WorkSpaces makes it harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["account_id","account_name"],"remediation":"Perform the following steps to create a Rule for CloudWatch WorkSpaces Events From the Console:\n1. Login to the EventBridge console at https://console.aws.amazon.com/events/home.\n2. In the left pane click Rules.\n3. Click Create rule.\n4. For Rule definition, enter a name and description.\n5. For \"Rule type\" select \"Rule with an event pattern\", click \"Next\".\n6. For \"Event source\" select \"AWS events or EventBridge partner events\".\n7. In \"Event pattern\" section, for \"Event source\" select \"AWS services\", for \"AWS service\" select \"WorkSpaces\" and for \"Event type\" select \"WorkSpaces Access\".\n8. Click \"Next\".\n9. For \"Target types\", select \"AWS service\", for target select \"CloudWatch log group\".\n10. Select , click \"Next\".\n11. Configure tags if needed and click \"Next\".\n12. Review changes and click \"Create rule\".","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-472":{"article":"Instance metadata is used to configure or manage the running instance. The IMDS provides access to temporary, frequently rotated credentials. These credentials remove the need to hard code or distribute sensitive credentials to instances manually or programmatically.\nVersion 2 of the IMDS adds new protections for the following types of vulnerabilities. These vulnerabilities could be used to try to access the IMDS:\n  - Open website application firewalls;\n  - Open reverse proxies;\n  - Server-side request forgery (SSRF) vulnerabilities;\n  - Open Layer 3 firewalls and network address translation (NAT).","impact":"Instances that use IMDSv1 are exposed to the following vulnerabilities:\n- Open website application firewalls;\n- Open reverse proxies;Server-side request forgery (SSRF) vulnerabilities;\n- Open Layer 3 firewalls and network address translation (NAT).","report_fields":["LaunchConfigurationName"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo enable imdsv2 only, create new launch configuration:\n1. Open the Amazon EC2 Auto Scaling console at https://console.aws.amazon.com/ec2/.\n2. Under Auto Scaling, click on the Launch Configurations.\n3. Click on the Create launch configuration.\n4. Additional configuration, click on the Advanced details.\n5. Under \"Metadata accessible\" choose enabled.\n6. Under \"Metadata version\" choose \"V2 only\".\n7. Create launch configuration.","multiregional":false,"service":"Amazon EC2 Auto Scaling"},"ecc-aws-210":{"article":"IAM database authentication allows authentication to database instances with an authentication token instead of a password. Network traffic to and from the database is encrypted using SSL.","impact":"Without IAM Database Authentication, you are missing the opportunity to use in-transit encryption. It may lead to Internet traffic exposure resulting in a breach of confidentiality and centralized access management to all databases.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. Choose Databases. \n3. Select the DB instance to modify.\n4. Choose Modify. \n5. Under Database options, choose Enable IAM DB authentication. \n6. Choose Continue. \n7. Under Scheduling of modifications, choose when to apply modifications. The options are Apply during the next scheduled maintenance window or Apply immediately. \n8. For clusters, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-351":{"article":"When you create and use your own KMS CMK customer-managed keys to protect the contents of your RDS instances, you obtain full control over who can use the CMK keys and access the data encrypted within RDS. The AWS KMS service allows you to create, rotate, disable, enable, and audit your Customer Master Keys (CMKs) for Amazon RDS.","impact":"Without a KMS CMK customer-managed key, you do not have full and granular control over who can access key that is used for encryption.","report_fields":["DBInstanceArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nEncrypting Existing AWS RDS Database:\n1. Login to the AWS Management Console.\n2. Navigate to RDS dashboard at https://console.aws.amazon.com/rds/.\n3. In the navigation panel, under RDS Dashboard, click Instances.\n4. Select the RDS database instance that you want to encrypt.\n5. Click Instance Actions button from the dashboard top menu and select Take Snapshot.\n6. On the Take DB Snapshot page, enter a name for the instance snapshot in the Snapshot Name field and click Take Snapshot (the backup process may take few minutes and depends on your instance storage size).\n7. Select the new created snapshot and click the Copy Snapshot button from the dashboard top menu.\n8. On the Make Copy of DB Snapshot page, perform the following:\n  8.1 In the New DB Snapshot Identifier field, enter a name for the new snapshot (copy).\n  8.2 Check Copy Tags so the new snapshot can have the same tags as the source snapshot.\n  8.3 Select Yes from the Enable Encryption dropdown list to enable encryption.\n  8.4 Under AWS KMS Key choose required CMK (not default)\n9. Click Copy Snapshot to create an encrypted copy of the selected instance snapshot.\n10. Select the new snapshot copy (encrypted) and click Restore Snapshot button from the dashboard top menu. This will restore the encrypted snapshot to a new database instance.\n11. On the Restore DB Instance page, enter a unique name for the new database instance in the DB Instance Identifier field.\n12. Review the instance configuration details and click Restore DB Instance.\n13. As soon as the new instance provisioning process is completed (its status becomes available), you can update your application configuration to refer to the endpoint of the new (encrypted) database instance. Once the database endpoint is changed at your application level, you can remove the unencrypted instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-130":{"article":"Elastic Load Balancing uses a TLS negotiation configuration, known as a security policy, to negotiate TLS connections between a client and the Network load balancer. For TLS listeners, It's recommend using the ELBSecurityPolicy-TLS13-1-2-2021-06 security policy. This is the default policy for listeners created using the AWS Management Console. This security policy includes TLS 1.3, which is optimized for security and performance, and is backward compatible with TLS 1.2. For Forward Secrecy, you can use one of the ELBSecurityPolicy-FS policies or an ELBSecurityPolicy-TLS13 policy.","impact":"Using a deprecated security policy for TLS negotiation configuration within Network Load Balancers exposes the connection between the client and the load balancer to various TLS vulnerabilities. Deprecated security policies may have old versions of TLS or SSL protocols that are known to have fatal security flaws and do not provide protection for data in transit.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, under 'LOAD BALANCING', choose 'Load Balancers'.\n3. Select the load balancer and choose 'Listeners'.\n4. Select the check box for the TLS listener and choose 'Edit'.\n5. For 'Security policy', choose the security policy.\n6. Choose 'Update'.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-200":{"article":"Enabling cluster deletion protection is an additional layer of protection against accidental database deletion or deletion by an unauthorized entity. When deletion protection is enabled, an RDS cluster cannot be deleted. Before a deletion request can succeed, deletion protection must be disabled","impact":"Accidentally deleted or deleted by an unauthorized entity, the RDS cluster can result in data loss.","report_fields":["DBClusterArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases, then choose the DB cluster that you want to modify. \n3. Choose Modify. \n4. Under Deletion protection, choose Enable deletion protection. \n5. Choose Continue. \n6. Under Scheduling of modifications, choose when to apply modifications. The options are Apply during the next scheduled maintenance window or Apply immediately. \n7. Choose Modify Cluster.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-252":{"article":"With this feature enabled, you can encrypt AWS Glue Data Catalog objects such as databases, tables, partitions, connections and user-defined functions and also encrypt connection passwords that you provide when you create data connections.","impact":"Not encrypting data at rest can lead to unauthorized access to sensitive or private data and  may not meet compliance requirements defined within your organization for data-at-rest encryption.","report_fields":["CatalogId"],"remediation":"1. Sign in to the AWS Management Console and open the AWS Glue console at https://console.aws.amazon.com/glue/.\n2. Choose Settings in the navigation pane.\n3. On the Data catalog settings page, select Metadata encryption, and choose an AWS KMS key.","multiregional":false,"service":"AWS Glue"},"ecc-aws-111":{"article":"Ensure that all your public AWS ALBs are integrated with the Web Application Firewall (AWS WAF) service to protect against application-layer attacks","impact":"By not using a firewall, you deprive yourself of protection against common web-based attacks. This can lead to a loss of data confidentiality, integrity, and availability.","report_fields":["LoadBalancerArn"],"remediation":"1. Go to the AWS WAF service page and select Web ACLs. \n2. Create a new Web ACL or select the existing one. \n3. Select the 'Associated AWS resources' tab.\n4. Click on 'Add AWS resources'. \n5. Under 'Resource Type', select the resource type and then select the resource you want to associate with this web ACL.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-458":{"article":"CloudWatch Lambda Insights is a monitoring and troubleshooting solution for serverless applications running on AWS Lambda. The solution collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network. It also collects, aggregates, and summarizes diagnostic information such as cold starts and Lambda worker shutdowns to help you isolate issues with your Lambda functions and resolve them quickly.\nLambda Insights uses a new CloudWatch Lambda extension, which is provided as a Lambda layer. When you install this extension on a Lambda function, it collects system-level metrics and emits a single performance log event for every invocation of that Lambda function. CloudWatch uses embedded metric formatting to extract metrics from the log events.","impact":"With Enhanced Monitoring for Lambda functions disabled, you are missing out on the opportunity to use a monitoring and troubleshooting solution that helps you isolate issues with your Lambda functions and resolve them quickly.","report_fields":["FunctionArn"],"remediation":"When you enable Lambda Insights on a function in the Lambda console for a supported runtime, Lambda adds the Lambda Insights extension as a layer to your function, and verifies or attempts to attach the CloudWatchLambdaInsightsExecutionRolePolicy policy to your function\u2019s execution role.\nTo enable Lambda Insights in the Lambda console:\n1. Open the Functions page of the Lambda console https://console.aws.amazon.com/lambda/home#/functions.\n2. Choose your function.\n3. Choose the 'Configuration' tab.\n4. On the 'Monitoring and operations tools' pane, choose 'Edit'.\n5. Under 'CloudWatch Lambda Insights', turn on 'Enhanced monitoring'.\n6. Choose 'Save'.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-321":{"article":"The 'sec_return_server_release_banner' setting return information about the patch/update release number.","impact":"Allowing the database to return information about the patch/update release number could facilitate unauthorized users' attempts to gain access based upon known patch weaknesses.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"sec_return_server_release_banner\".\n6. Choose FALSE\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-123":{"article":"Enable encryption of your EFS file systems in order to protect your data and metadata from breaches or unauthorized access and fulfill compliance requirements for data-at-rest encryption within your organization.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in EFS.","report_fields":["FileSystemArn","OwnerId"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nIn order to encrypt the existing Amazon EFS with your own AWS KMS CMK, you need to create a new Amazon EFS and transfer all the data from the existing EFS to the new one with encryption enabled.\n1. Login to the AWS Management Console and Navigate to Elastic File System (EFS) dashboard.\n2. Select File Systems from the left navigation panel.\n3. Click 'Create File System' button from the dashboard top menu to start the file system setup process.\n4. On the 'Configure file system access' configuration page, perform the following actions.\n  4.1 Choose the right VPC from the VPC dropdown list.\n  4.2 Within 'Create mount targets' section, select the checkboxes for all of the Availability Zones (AZs) within the selected VPC. These will be your mount targets.\n  4.3 Click 'Next step' to continue.\n5. Perform the following on the 'Configure optional settings' page.\n  5.1 Create tags to describe your new file system.\n  5.2 Choose performance mode based on your requirements.\n  5.3 Check 'Enable encryption' checkbox and choose aws/elasticfilesystem from Select KMS master key dropdown list to enable encryption for the new file system using the default master key provided and managed by AWS KMS.\n  5.4 Click 'Next step' to continue.\n6. Review the file system configuration details on the review and create page and then click 'Create File System' to create your new AWS EFS file system.\n7. Copy the data from the old unencrypted EFS file system onto the newly create encrypted file system.\n8. Remove the unencrypted file system as soon as your data migration to the newly create encrypted file system is completed.","multiregional":false,"service":"Amazon Elastic File System"},"ecc-aws-517":{"article":"ACLs are legacy access control mechanisms that predate IAM. Instead of ACLs, it's recommend using IAM policies or S3 bucket policies to more easily manage access to your S3 buckets.\nACLs are suitable for specific scenarios. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using object ACL by the AWS account that owns the object.\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs, and it's recommend that you disable ACLs except in unusual circumstances where you need to control access for each object individually.","impact":"Granting permissions via access control lists (ACLs) has its limits and impairs the ability to control access to objects.\nBecause, by default, when another AWS account uploads an object to your S3 bucket, that account (the object writer) owns the object, has access to it, and can grant other users access to it through ACLs.","report_fields":["Name"],"remediation":"If your bucket ACL grants access outside of your AWS account, before you disable ACLs, you must migrate your bucket ACL permissions to your bucket policy and reset your bucket ACL to the default private ACL. If you don't migrate these bucket ACLs, your request to apply the bucket owner enforced setting to disable ACLs fails and returns the InvalidBucketAclWithObjectOwnership error code. \n\nTo review a bucket's ACL permissions:\n1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n2. In the 'Buckets list', choose the bucket name.\n3. Choose the 'Permissions' tab.\n4. Under 'Access control list (ACL)', review your bucket ACL permissions.\n\nTo review an object's ACL permissions:\n1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n2. In the 'Buckets list', choose the bucket name containing your object.\n3. In the 'Objects list', choose your object name.\n4. Choose the 'Permissions' tab.\n5. Under 'Access control list (ACL'), review your object ACL permissions.\n\nTo migrate ACL permissions and update your bucket ACL:\n1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n2. In the 'Buckets list', choose the bucket name.\n3. On the 'Permissions' tab, under 'Bucket policy', choose 'Edit'.\n4. In the Policy box, add or update your bucket policy.\n5. Choose 'Save changes'.\n6. Update your bucket ACL to remove ACL grants to other groups or AWS accounts.\n  6.1 Under 'Access control list (ACL)', choose 'Edit'.\n  6.2 Remove access from everyone, but 'Bucket owner (your AWS account)'.\n  6.3 Remove access to another AWS account, under 'Access for other AWS accounts', choose 'Remove'.\n  6.3 To save your changes, choose 'Save changes'.\n7. Click 'Edit' in the 'Object Ownership' section.\n8. Select 'ACLs disabled (recommended)' to apply the bucket owner enforced setting for Object Ownership.\n9. Choose 'Save changes'.","multiregional":true,"service":"Amazon S3"},"ecc-aws-126":{"article":"AWS Redshift instances should be encrypted at rest to help protecting sensitive data from breaches.","impact":"Disabled encryption allows a user to get unauthorized access to sensitive data in Redshift clusters.","report_fields":["ClusterIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\nTo resolve this issue, create a new instance with encryption enabled, migrate the file data from the reported Redshift instances to this new instance, and then delete the original instances. \n1. Sign in to the AWS Admin Console and Access the Redshift service. \n2. Click on the identified Redshift cluster and take a snapshot of it. \n3. Create a new Redshift cluster (now with 'Encryption' set to 'Yes' during creation time) and use the snapshot to populate (restore) the new cluster. \n4. Once the new cluster is populated, delete the older cluster (without encryption).","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-466":{"article":"Amazon FSx for NetApp ONTAP has two deployment types, Single-AZ and Multi-AZ, that offer you different levels of availability and durability. In a Multi-AZ deployment, the standby file server is deployed in a different Availability Zone from the active file server in the same AWS Region. Any changes written to your file system are synchronously replicated across Availability Zones to the standby.\nMulti-AZ file systems are designed for use cases such as business-critical production workloads that require high availability to shared ONTAP file data and need storage with built-in replication across Availability Zones. Single-AZ file systems are designed for use cases that do not require the data resiliency model of a Multi-AZ file system. They provide a cost-optimized solution for use cases such as development and test environments, or storing secondary copies of data that is already stored on premises or in other AWS Regions, by only replicating data within an Availability Zone.","impact":"When an FSx file system is not configured to use multiple Availability Zones and Availability Zone in which file system deploy becomes unavailable, your data also can become unavailable. Using Single-AZ does not provide data resiliency and is not recommended for use cases such as business-critical production workloads that require high availability to shared ONTAP file data.","report_fields":["ResourceARN"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create FSx system with enabled Multi-AZ, perform the following actions: \n1. Open the Amazon FSx console at https://console.aws.amazon.com/fsx/.\n2. On the dashboard, choose 'Create file system 'to start the file system creation wizard.\n3. On the 'Select file system type' page, choose 'Amazon FSx for NetApp ONTAP', and then choose 'Next'. The 'Create file system' page appears. For 'Creation method', choose 'Standard create'.\n4. For 'File system name - optional', enter a name for your file system. \n5. For Deployment type choose 'Multi-AZ'.\n6. For 'SSD storage capacity', specify the storage capacity of your file system, in gibibytes (GiBs).\n7. For 'Virtual Private Cloud (VPC)'', choose the Amazon VPC that you want to associate with your file system.\n8. For 'Storage efficiency', choose 'Enabled' to turn on the ONTAP storage efficiency features (compression, deduplication, and compaction) or Disabled to turn them off.\n9. Set up all other necessary configurations.\n10. Choose 'Next'.\n11. Review the file system configuration shown on the Create ONTAP file system page. For your reference, note which file system settings you can modify after the file system is created.\n12. Choose 'Create file system'.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-396":{"article":"This policy identifies the cloudformation stacks that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["StackId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Cloudfront at https://console.aws.amazon.com/cloudformation/.\n2. Click on 'Stacks'.\n3. CLick on the required stack.\n4. Click 'Update'.\n5. On step 3 click 'Add new tag'.\n6. Add new tag and save.","multiregional":false,"service":"AWS CloudFormation"},"ecc-aws-381":{"article":"This policy identifies the ENI that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["NetworkInterfaceId"],"remediation":"1. Sign in to the AWS Management Console and open the EC2 console at https://console.aws.amazon.com/ec2/v2/.\n2. In the navigation pane under 'Network & Security' choose 'Network interfaces'..\n3. Choose required eni.\n4. Open 'Tags' and click on 'Manage tags'.\n5. Add a tags.\n6. Save.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-241":{"article":"The log_statement setting specifies the types of SQL statements that are logged. \nValues are: none (off), ddl, mod, all (all statements). It is recommended to set on 'all' for better security.","impact":"If log_statement is disabled than sql statements will not be logged.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_statement\".\n6. Choose ddl or other values according to your organization policy\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-357":{"article":"The domain registries for all generic TLDs and many geographic TLDs let you lock a domain to prevent someone from transferring the domain to another registrar without your permission.","impact":"While this feature disabled, unauthorized transfer may occur.","report_fields":["DomainName"],"remediation":"If locking is supported and you want to lock your domain, perform the following procedure:\n1. Sign in to the AWS Management Console and open the Route53 console at https://console.aws.amazon.com/route53/.\n2. In the navigation pane, choose 'Registered Domains'.\n3. Choose the name of the domain that you want to update.\n4. Choose 'Enable' for 'Transfer lock'.\n5. If you encounter issues while enabling automatic renewal, you can contact AWS Support for free. \n  For more information, see 'Contacting AWS Support about domain registration issues': https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-contact-support.html.","multiregional":true,"service":"Amazon Route 53"},"ecc-aws-088":{"article":"Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts.","impact":"Users can accidentally delete S3 bucket sensitive data.","report_fields":["Name"],"remediation":"1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/. \n2. Choose the S3 bucket that does not have cross-region replication enabled. \n3. Choose Management, then choose Replication. \n4. Choose Add rule. If versioning is not already enabled, you are prompted to enable it. \n5. Choose your source bucket - Entire bucket. \n6. Choose your destination bucket. If versioning is not already enabled on the destination bucket for your account, you are prompted to enable it. \n7. Choose an IAM role. For more information on setting up permissions for replication, see the Amazon Simple Storage Service Developer Guide. \n8. Enter a rule name, choose Enabled for the status, then choose Next. \n9. Choose Save.","multiregional":true,"service":"Amazon S3"},"ecc-aws-242":{"article":"Including csvlog in the log_destination list provides a convenient way to import log files into a database table. This option emits log lines in comma-separated-values (CSV) format, with these columns: time stamp with milliseconds, user name, database name, process ID, client host:port number, session ID, per-session line number, command tag, session start time, virtual transaction ID, regular transaction ID, error severity, SQLSTATE code, error message, error message detail, hint, internal query that led to the error (if any), character count of the error position therein, error context, user query that led to the error (if any and enabled by log_min_error_statement), character count of the error position therein, location of the error in the PostgreSQL source code (if log_error_verbosity is set to verbose), application name, and backend type.","impact":"If log destination is not set to csv it would be harder to analyze reports.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_destination\".\n6. Choose csvlog.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-212":{"article":"Backups help you to recover more quickly from a security incident. They also strengthens the resilience of your systems. Aurora backtracking reduces the time to recover a database to a point in time. It does not require a database restore to do so.","impact":"Without enabling backtracking, it is impossible to quickly recover data in the event of failure. This may have a big impact on the resilience of your systems.","report_fields":["DBClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nYou cannot enable backtracking on an existing cluster. Instead, you can create a clone that has backtracking enabled. Open the Amazon RDS console at https://console.aws.amazon.com/rds/. \n1. In the left navigation panel, under Amazon RDS, click Databases.  \n2. Select your database.\n3. Choose your Aurora DB cluster from the list, and for Actions, choose Create clone.\n4. On the Create Clone page, perform the following actions:\n4.1 In the Settings section, within DB instance identifier box, provide a name for the primary instance of the clone database cluster. \n4.2 In the Additional configuration section, select Enable Backtrack to activate the feature, then specify the amount of time that you want to be able to backtrack, within the Target Backtrack window box.\n4.3 If required, configure any other settings for the clone database cluster.\n4.4 Choose Create clone to launch the Aurora clone of your chosen Aurora DB cluster. \n5. Once the cluster is created, replace the required endpoints within your application code to switch the source cluster with the new database cluster. \n6. You can now connect to your cloned database to verify that your database is identical to the one you cloned: check the databases, tables, users, and records you created to see they are included in the cloned database.\n7. Now you can remove the source Aurora database cluster from your AWS account in order to avoid further charges. To delete the necessary cluster, perform the following: \n7.1 Select the primary instance for the database cluster that you want to terminate. \n7.2 Click the Instance Actions button from the dashboard top menu and select Delete.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-077":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms. It is recommended that a metric filter and alarm be established for console logins that are not protected by multi-factor authentication (MFA). Monitoring of single-factor console logins will increase visibility into accounts that are not protected by MFA.","impact":"Lack of monitoring and logging of sign-ins without MFA can lead to insufficient response time to threats from an attacker. This, in turn, may result in data loss and degradation of the service. Monitoring single-factor console logins will increase the visibility of accounts not protected by MFA.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for unauthorized API calls and the <cloudtrail_log_group_name> taken from step 1.\naws logs put-metric-filter --log-group-name <cloudtrail_log_group_name> --filter-name `<unauthorized_api_calls_metric>` --metric-transformations metricName=`<unauthorized_api_calls_metric>`,metricNamespace=<namespace_name>,metricValue='1' --filter-pattern '{{($.eventName = \"ConsoleLogin\") && ($.additionalEventData.MFAUsed != \"Yes\") && ($.userIdentity.type = \"IAMUser\") && ($.responseElements.ConsoleLogin = \"Success\")}}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<unauthorized_api_calls_alarm>` --metric-name `<unauthorized_api_calls_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace  <namespace_name> --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-339":{"article":"The MQ service releases engine version upgrades regularly to introduce new software features, bug fixes, security patches and performance improvements.\nAuto minor version upgrade option allows to have minor engine upgrades applied automatically to the replication instance during the maintenance window or immediately if you choose the Apply changes immediately option.","impact":"With an automatic update disabled, you are missing updates that may contain new software features, bug fixes, security patches and performance improvements.","report_fields":["BrokerArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/amazon-mq/\n2. In the navigation pane, choose Brokers.\n3. Choose the required brokers.\n4. Choose Edit.\n5. Under Maintenance, click on the 'Enable automatic minor version upgrades'.\n6. Choose Save.","multiregional":false,"service":"Amazon MQ"},"ecc-aws-385":{"article":"This policy identifies the Route table that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["RouteTableId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/.\n2. Click on the 'Route tables'.\n3. Click on the required route table.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-515":{"article":"Security Hub collects security data from across AWS accounts, services, and supported third-party partner products and helps you analyze your security trends and identify the highest priority security issues.\nWhen Security Hub is enabled, it starts to collect, organize, and prioritize security findings from other security-oriented AWS cloud services such as intrusion detection findings from Amazon GuardDuty, vulnerability findings from Amazon Inspector, and sensitive data identification findings from Amazon Macie, or from third-party partner security tools.","impact":"With a disabled SecurityHub, it will be harder to organize and analyze findings.","report_fields":["account_id","account_name"],"remediation":"1. Open the Security Hub console at https://console.aws.amazon.com/securityhub/\n2. Click on the 'Go to Security Hub'.\n3. Click 'Enable Security Hub'.","multiregional":false,"service":"AWS Security Hub"},"ecc-aws-173":{"article":"By default, Elasticsearch exposes TCP port 9200 for REST API access and TCP port 9300 for internal cluster communication. Consider adding rules to allow connecting to TCP port 9200 from desired subnets, typically private subnets, and TCP port 9300 from the subnets where Elasticsearch nodes live.\nUnrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:  \n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-110":{"article":"Ensure that AWS ECS clusters are encrypted. Data encryption at rest prevents unauthorized users from accessing sensitive data on your AWS ECS clusters and associated cache storage systems.","impact":"Not encrypting data at rest can lead to unauthorized access to sensitive data.","report_fields":["clusterArn"],"remediation":"ECS remediation steps to encrypt new EBS volumes: \n1. From the AWS Management Console, select EC2. \n2. Under 'Elastic Block Store', select 'Volumes' \n3. Select 'Create Volume' \n4. Enter the required configuration for your Volume. \n5. Select the checkbox for 'Encrypt this volume' \n6. Select the KMS Customer Master Key (CMK) to be used under 'Master Key' \n7. Select 'Create Volume' There is no option to encrypt existing EBS volumes. \nTo encrypt existing EBS volumes, use the following steps to create a snapshot and encrypt the resulting new volume or snapshot using your default CMK:\n1. Select your unencrypted volume \n2. Select 'Actions' - 'Create Snapshot'\n3. When the snapshot is complete, select 'Snapshots' under 'Elastic Block Store'. Select your newly created snapshot \n4. Select 'Actions' - 'Copy' \n5. Check the box for 'Encryption' \n6. Select the CMK for KMS to use as required \n7. Click on 'Copy'\n8. Select the newly created snapshot \n9. Select 'Actions' - 'Create Volume' \n10. You will notice that the normal 'Encryption' option is set to 'True'. Because the snapshot is itself encrypted, this cannot be modified. The volume currently created from this snapshot will be encrypted","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-397":{"article":"This policy identifies the CLoudfront distributions that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EFS at https://console.aws.amazon.com/cloudfront/v3.\n2. Click on the required distribution.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-065":{"article":"Enforce HTTPS-only traffic between a CloudFront distribution and the origin. It is recommended to use HTTPS for secure communications between your CloudFront distribution and end users to guarantee encryption of traffic and prevent malicious actors from intercepting your traffic. \nNote: This rule runs on all the origins except S3 Buckets.","impact":"The unencrypted traffic between the edge servers and the custom origin can be disclosed by malicious users in case they are able to capture packets sent across Cloudfront Content Distribution Network (CDN).","report_fields":["ARN"],"remediation":"Perform the following for each distribution that failed the rule: \nOn the AWS CloudFront console, check the details for the distribution and ensure that the Viewer Protocol Policy is HTTPS Only. \nConfigure CloudFront to require HTTPS between viewers and CloudFront. \n1. Navigate to the the AWS console CloudFront dashboard.\n2. Select your distribution ID.\n3. Select the 'Behaviors' tab.\n4. Check the behavior you want to modify, then select Edit.\n5. Select 'HTTPS Only' or 'Redirect HTTP to HTTPS' for Viewer Protocol Policy.\n6. Select 'Yes', then select Edit.","multiregional":true,"service":"Amazon CloudFront"},"ecc-aws-508":{"article":"Using the latest version of Apache Airflow, you adhere to AWS best practices and receive the newest features, benefit from better performance and security and get the latest bug fixes.","impact":"Without keeping the MWAA up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["Arn"],"remediation":"1. Navigate to https://console.aws.amazon.com/mwaa/home.\n2. Open required environment.\n3. Click 'Edit'. \n4. Change 'Airflow version'\n5. Save changes","multiregional":false,"service":"Amazon Managed Workflows for Apache Airflow"},"ecc-aws-140":{"article":"Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign programmatic requests to the AWS CLI or AWS API (directly or using the AWS SDK)","impact":"Keeping more than one AWS IAM access key increases the risk of unauthorized access to your AWS resources and components.","report_fields":["Arn"],"remediation":"1. Sign in to the AWS Management Console and navigate to IAM dashboard at https://console.aws.amazon.com/iam/.\n2. In the left navigation panel, choose Users.\n3. Click on the IAM user name that you want to examine.\n4. On the IAM user configuration page, select Security Credentials tab.\n5. In Access Keys section, choose one access key that is less than 90 days old. This should be the only active key used by this IAM user to access AWS resources programmatically. Test your application(s) to make sure that the chosen access key is working.\n6. In the same Access Keys section, identify your non-operational access keys (other than the chosen one) and deactivate it by clicking the Make Inactive link.\n7. If you receive the Change Key Status confirmation box, click Deactivate to switch off the selected key.\n8. Repeat steps no. 3 - 7 for each IAM user in your AWS account","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-206":{"article":"Amazon SQL Server database engine logs (Error, Agent) should be enabled and sent to CloudWatch.\nDatabase logging provides detailed records of requests made to RDS. Database logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the database, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBInstanceArn"],"remediation":"To publish SQL Server DB logs to CloudWatch Logs from the AWS Management Console:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB instance that you want to modify.\n4. Choose Modify.\n5. Under Log exports, choose Error and Agent log types to start publishing to CloudWatch Logs.\n6. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n7. Choose Continue. Then on the summary page, choose Modify DB Instance.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-416":{"article":"This policy identifies the IAM Roles that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["Arn"],"remediation":"1. Login to the AWS Console.\n2. Find the IAM service.\n3. Choose Roles without tags.\n4. Click on the \"Tags\".\n5. Click on the \"Manage tags\".\n6. Add tag.","multiregional":true,"service":"AWS Identity and Access Management"},"ecc-aws-463":{"article":"Ensure that your S3 buckets are using DNS-compliant bucket names in order to adhere to AWS best practices and to benefit from new S3 features such as S3 Transfer Acceleration, to benefit from operational improvements and to receive support for virtual-host style access to buckets. A DNS-compliant name is an S3 bucket name that doesn't contain periods (\".\").","impact":"To use virtual hosted\u2013style buckets with SSL or enable S3 Transfer Acceleration feature, the names of these buckets cannot contain periods (\".\").","report_fields":["Name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Login to the AWS S3 Console at https://s3.console.aws.amazon.com/s3/.\n2. Delete non compliant bucket or create a new one without periods (\".\").","multiregional":true,"service":"Amazon S3"},"ecc-aws-384":{"article":"This policy identifies the NetworkACL that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["NetworkAclId","OwnerId"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon VPC at https://console.aws.amazon.com/vpc/.\n2. Open 'Network ACLs'.\n3. Click on the required NACL.\n4. Open 'Tags' and click on the 'Manage tags'.\n5. Add new tag and save.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-503":{"article":"When creating a RDS cluster, you should change the default admin username 'admin' or 'postgres' to a unique value. Default usernames are public knowledge and should be changed upon configuration.","impact":"Well-known database Admin username could lead to unintended access.","report_fields":["DBClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose Create database.\n4. In Choose a database creation method, choose Standard create.\n5. In Engine options, choose Amazon Aurora.\n6. Under the 'Credentials Settings' change 'Master username'.\n7. Create database","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-516":{"article":"By enabling Event Notifications, you receive alerts on your Amazon S3 buckets when specific events occur. For example, you can be notified of object creation, object removal, and object restoration. These notifications can alert relevant teams to accidental or intentional modifications that may lead to unauthorized data access.","impact":"Not enabling S3 bucket event notifications can lead to slow or no response when specific events occur in your bucket.","report_fields":["Name"],"remediation":"To enable and configure event notifications for an S3 bucket\n  1. Sign in to the AWS Management Console and open the 'Amazon S3 console' at https://console.aws.amazon.com/s3/.\n  2. In the 'Buckets list', choose the name of the bucket that you want to enable events for.\n  3. Choose 'Properties'.\n  4. Navigate to the 'Event Notifications' section and choose 'Create event notification'.\n  5. In the 'General configuration' section, specify descriptive event name for your event notification. \n    Optionally, you can also specify a prefix and a suffix to limit the notifications to objects with keys ending in the specified characters.\n    a. Enter a description for the 'Event name'.\n    b. (Optional) To filter event notifications by prefix, enter a 'Prefix'.\n    c. (Optional) To filter event notifications by suffix, enter a 'Suffix'.\n  6. In the 'Event types' section, select one or more event types that you want to receive notifications for.\n  7. In the 'Destination' section, choose the event notification destination.\n  Note: Before you can publish event notifications, you must grant the Amazon S3 principal the necessary permissions to call the relevant API. This is so that it can publish notifications to a Lambda function, SNS topic, or SQS queue.\n    a. Select the destination type: Lambda Function, SNS Topic, or SQS Queue.\n    b. After you choose your destination type, choose a function, topic, or queue from the list.\n    c. Or, if you prefer to specify an Amazon Resource Name (ARN), select 'Enter ARN' and enter the ARN.\n  8. Choose 'Save changes', and Amazon S3 sends a test message to the event notification destination.","multiregional":true,"service":"Amazon S3"},"ecc-aws-161":{"article":"RDS event notifications use Amazon SNS to make you aware of changes in the availability or configuration of your RDS resources. These notifications allow for rapid response.\nDBParameterGroup:['configuration change'] Notification should be enabled for All DBParameterGroups.","impact":"Not enabling RDS parameter group event notifications can lead to slow or no response to a parameter group configuration change.","report_fields":["account_id","account_name"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Event subscriptions.\n3. Under Event subscriptions, choose Create event subscription.\n4. In the Create event subscription dialog, do the following\n4.1. For Name, enter a name for the event notification subscription.\n4.2. For Send notifications to, choose an existing Amazon SNS ARN for an SNS topic.\n     To use a new topic, choose create topic to enter the name of a topic and a list of recipients.\n4.3. For Source type, choose Parameter groups.\n4.4. Under Instances to include, select All parameter groups.\n4.5. Under Event categories to include, select Specific event categories. \n     The control also passes if you select All event categories.\n4.5.1 Select configuration change.\n4.6. Choose Create.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-359":{"article":"To help debug issues related to client access to API Gateway REST API, you can enable Amazon CloudWatch Access Logs to log API calls. \nAccess logs contain details about who accessed your API and how they accessed it, which can also be used for troubleshooting API errors.","impact":"With disabled logging of API Gateway, it may be difficult to troubleshoot any issues that might happen with APIs or find who accessed API and how they accessed it.","report_fields":["stageName","restApiId"],"remediation":"Create an IAM role for logging to CloudWatch:\n1. In the AWS Identity and Access Management (IAM) console, in the left navigation pane, choose Roles.\n2. On the Roles pane, choose Create role.\n3. On the Create role page, do the following:\n  3.1 For Trusted entity type, choose AWS Service.\n  3.2 For use case, choose API Gateway.\n  3.3 Choose the API Gateway radio button.\n  3.4 Choose Next.\n4. Under Permissions Policies, note that the AWS managed policy AmazonAPIGatewayPushToCloudWatchLogs is selected by default. The policy has all the required permissions.\n5. Choose Next.\n6. Under Name, review and create, do the following:\n  6.1 For Role name, enter a meaningful name for the role.\n  6.2 (Optional) For Role description, edit the description to your preferences.\n  6.3 (Optional) Add tags.\n  6.4 Choose Create role.\n7.    On the Roles pane, in the search bar, enter the name of the role that you created. Then, choose the role from the search results.\n8.    On the Summary pane, copy the Role ARN. You'll need this Amazon Resource Name (ARN) in the next section.\n\nAdd the IAM role in the API Gateway console:\nNote: If you're developing multiple APIs across different AWS Regions, complete these steps in each Region.\n1. In the API Gateway console, on the APIs pane, choose the name of an API that you created.\n2. In the left navigation pane, at the bottom, below the Client Certificates section, choose Settings.\n3. Under Settings, for CloudWatch log role ARN, paste the IAM role ARN that you copied.\n4. Choose Save.\nNote: The console doesn't confirm that the ARN is saved.\n\nTurn on logging for your API and stage:\n1. In the API Gateway console, find the Stage Editor for your API.\n2. On the Stage Editor pane, choose the Logs/Tracing tab.\n3. Under Custom Access Logging, do the following to turn on access logging:\n  3.1 Choose the Enable Access Logging check box.\n  3.2 For Access Log Destination ARN, enter the ARN of a CloudWatch log group or an Amazon Kinesis Data Firehose stream.\n  3.3 Enter a Log Format. For guidance, choose CLF, JSON, XML, or CSV to see an example in that format.\n4. Choose Save Changes.\nNote: The console doesn't confirm that settings are saved.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-373":{"article":"The authentication protocol between the Microsoft AD DCs and the RADIUS server supported are PAP, CHAP, MS-CHAPv1, and MS-CHAPv2. MS-CHAPv2 provides the strongest security of the options supported.","impact":"Insecure protocols can make the connection vulnerable to attackers and lead to a breach of confidentiality.","report_fields":["DirectoryId"],"remediation":"Perform the steps below to set the protocol to MS-CHAPv2 for multi-factor authentication:\nFor AWS Managed AD based environments:\n1. Log in to the Directory Service console at https://console.aws.amazon.com/directoryservicev2.\n2. In the left pane select Directories.\n3. On the Directory details page, select the Networking & security tab.\n4. In the Multi-factor authentication section, choose Actions, and then choose Edit.\n5. On the Enable multi-factor authentication (MFA) page change the value for 'Protocol' to MS-CHAPv2.\n6. Click Save.\n\nFor directory connector / self-managed AD environments:\n1. Log in to the AWS Workspaces console at https://console.aws.amazon.com/workspaces.\n2. In the left pane select Directories.\n3. Select the directory ID link for your AWS Managed Microsoft AD directory.\n4. On the Directories page, select the Actions > Update Details.\n5. In the Multi-factor authentication section modify the protocol using the dropdown menu to be MS-CHAPv2 from the currently selected option.\n6. Click Update and exit once settings are as desired.","multiregional":false,"service":"Amazon WorkSpaces Family"},"ecc-aws-509":{"article":"DAX encryption in transit increases confidentiality, ensuring that all requests and responses between the application and the cluster are encrypted by transport level security (TLS).","impact":"If encryption in transit is disabled traffic between your application and your DAX cluster is defended only with standard tools like security groups, subnet segmentation with Network ACLs, and VPC flow tracing.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nYou cannot enable or disable encryption in transit after a cluster is created. To create a cluster with enabled encryption in transit, perform the following actions:  \n1. Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n2. In the navigation panel on the left side of the console, under DAX, choose Clusters.\n3. Choose Create cluster. \n4. For Cluster name, enter a short name for your cluster. Choose the node type for all of the nodes in the cluster, and for the cluster size, use 3 nodes. \n5. In Encryption, make sure that 'Enable encryption in transit' is selected.\n6. After choosing the IAM role, subnet group, security groups, and cluster settings, choose Launch cluster.","multiregional":false,"service":"Amazon DynamoDB Accelerator"},"ecc-aws-575":{"article":"EBS volumes that are attached to instances continue to retain information and accrue charges, even when an instance is stopped.\nAmazon EBS snapshots are billed at a lower rate than active EBS volumes. You can minimize your Amazon EBS charges but still retain the information that's stored in Amazon EBS for later use. To do this, create a snapshot of the volume as a backup, and then delete the active volume. Later, when you need the information from the snapshot, use the snapshot to replace the EBS volume for use with your infrastructure.\nTo stop Amazon EBS-related charges, delete EBS volumes and snapshots that you don't need.","impact":"EBS volumes that are attached to instances continue to accrue charges, even when an instance is stopped. Keeping unused EBS volumes can result in escalating costs and cluttered AWS accounts.","report_fields":["VolumeId"],"remediation":"To retain the information that's stored in Amazon EBS, you can create a snapshot:\n  1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n  2. In the navigation pane, choose 'Snapshots', 'Create snapshot'.\n  3. For 'Resource type', choose 'Volume'.\n  4. For 'Volume ID', select the volume from which to create the snapshot.\n  5. The 'Encryption' field indicates the selected volume's encryption status. If the selected volume is encrypted, the snapshot is automatically encrypted using the same KMS key. If the selected volume is unencrypted, the snapshot is not encrypted.\n  6. (Optional) For 'Description', enter a brief description for the snapshot.\n  7. (Optional) To assign custom tags to the snapshot, in the 'Tags' section, choose 'Add tag', and then enter the key-value pair. You can add up to 50 tags.\n  8. Choose 'Create snapshot'.\n\nTo delete an unused EBS volume:\n  1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n  2. In the navigation pane, choose 'Volumes'.\n  3. Select the volume to delete and choose 'Actions', 'Delete volume'.\n  Note: If 'Delete volume' is greyed out, the volume is attached to an instance. You must detach the volume from the instance before it can be deleted.\n  4. In the confirmation dialog box, choose 'Delete'.","multiregional":false,"service":"Amazon Elastic Block Store"},"ecc-aws-086":{"article":"Determine the specific permissions needed by your Lambda Functions, and then craft IAM policies for these permissions only, instead of full administrative privileges. There should not be any policies that grant blanket permissions ('*') to resources. \nIt is recommended and considered a standard security best practice to grant least privileges, that is granting only the permissions required to perform a task.","impact":"Providing full administrative privileges instead of restricted ones to the minimum set of permissions can expose your AWS resources to potentially unwanted actions.","report_fields":["FunctionArn"],"remediation":"1. For each Lambda Function that failed this rule, navigate to Policies on the IAM console.\n2. Search for the policy that failed the rule.\n3. Rework the permissions in the policy to grant positive permissions to specific AWS services or actions instead of blanket permissions using '*'.","multiregional":false,"service":"AWS Lambda"},"ecc-aws-329":{"article":"Removing unused Key Pairs will help you to adhere best security practices and protect against unapproved SSH access.","impact":"Not removing unused Key Pairs increase the risk of unauthorized access to AWS EC2 instances as these keys can be reassociated at any time, providing access to the wrong users.","report_fields":["KeyPairId"],"remediation":"To delete Key Pair: \n1. Login to the AWS Management Console and open the Amazon EC2 console using https://console.aws.amazon.com/ec2/ \n2. Under 'Network & Security', click on 'Key Pairs'.\n3. Click on the required key pair.\n4. Under the Actions, click on Delete.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-073":{"article":"If an EIP is not attached to an Amazon EC2 instance, this is an indication that it is no longer in use. Unless there is a business need to retain them, you should remove unused resources to maintain an accurate inventory of system components.","impact":"Keeping unused EIPs will inflict unnecessary monthly costs and eventually can prevent from creating new EIPs because of the limit.","report_fields":["AllocationId"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Elastic IPs. \n3. Choose the Elastic IP address, choose Actions, and then choose Release addresses. \n4. When prompted, choose Release.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-523":{"article":"When a KMS key is scheduled for deletion, a mandatory waiting period is enforced to allow time to reverse the deletion, if it was scheduled in error. The default waiting period is 30 days, but it can be reduced to as short as 7 days when the KMS key is scheduled for deletion. During the waiting period, the scheduled deletion can be canceled and the KMS key will not be deleted.","impact":"KMS keys cannot be recovered once deleted. Data encrypted under a KMS key is also permanently unrecoverable if the KMS key is deleted.","report_fields":["KeyArn","AWSAccountId"],"remediation":"To cancel unintentional key deletion:\n1. Sign in to the AWS Management Console and open the KMS console at https://console.aws.amazon.com/iam. \n2. Choose required key. \n3. Click on the 'Key actions'.\n4. Click on the 'Cancel key deletion'.","multiregional":false,"service":"AWS Key Management Service"},"ecc-aws-261":{"article":"It is recommended to delete unused internet gateways or associate them (use them).","impact":"Keeping unused internet gateways eventually can prevent creating new internet gateways because of the limit.","report_fields":["InternetGatewayId","OwnerId"],"remediation":"1. Sign in to the AWS Management Console and open the VPC console at https://console.aws.amazon.com/vpc.\n2. In the navigation pane, choose Internet gateways. \n3. Choose the name of the intended Internet gateway. \n4. Choose the Actions and then choose either \"a\" or \"b\":\n  a) Attach to VPC, and select intended VPC.\n  b) Delete internet gateway.","multiregional":false,"service":"Amazon Virtual Private Cloud"},"ecc-aws-274":{"article":"Amazon Aurora database engine logs (Audit, Error, General, SlowQuery) should be enabled and sent to CloudWatch.\nRDS cluster logging provides detailed records of requests made to RDS databases. RDS cluster logs can assist with security and access audits and can help to diagnose availability issues.","impact":"With disabled logs for the RDS DB cluster, it is harder to analyze statistics, diagnose issues, detect different types of attacks, and retain data for regulatory or legal purposes.","report_fields":["DBClusterArn"],"remediation":"Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n1. In the navigation pane, choose Databases.\n2. Choose the DB cluster that you want to modify.\n3. Choose Modify.\n4. Under Log exports, choose Audit, Error, General, SlowQuery log types to start publishing to CloudWatch Logs.\n5. Log exports is available only for database engine versions that support publishing to CloudWatch Logs.\n6. Choose Continue. Then on the summary page, choose Modify.\n\nTo enable and publish Aurora logs to CloudWatch Logs from the AWS Management Console, set the following parameters in a custom Cluster Parameter Group:\n- general_log=1\n- slow_query_log=1\n- log_output = FILE\n\nTo create a custom cluster parameter group:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Parameter groups.\n3. Choose Create parameter group. The Create parameter group window appears.\n4. In the Parameter group family list, choose a DB parameter group family.\n5. In the Type list, choose DB Cluster Parameter Group.\n6. In Group name, enter the name of the new DB cluster parameter group.\n7. In Description, enter a description for the new DB parameter group.\n8. Choose Create.\n\nTo apply a new DB cluster parameter group to an RDS DB cluster:\n1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n2. In the navigation pane, choose Databases.\n3. Choose the DB cluster that you want to modify.\n4. Choose Modify. The Modify DB cluster page appears.\n5. Under Database options, change the DB cluster parameter group.\n6. When you finish you changes, choose Continue. Check the summary of modifications.\n7. Choose Modify CLuster to save your changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-184":{"article":"Encrypting data at rest reduces the risk of data stored on disk being accessed by a user not authenticated to AWS. The encryption adds another set of access controls to limit the ability of unauthorized users to access to the data. For example, API permissions are required to decrypt the data before it can be read.","impact":"Encrypting data at rest reduces the risk of data stored on a disk being accessed by a user not authenticated to AWS. The encryption adds another set of access controls to limit the ability of unauthorized users to access the data.","report_fields":["ClusterArn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nYou cannot enable or disable encryption at rest after a cluster is created. To create a cluster with enabled encryption at rest, perform the following actions:  \n1. Sign in to the AWS Management Console and open the DynamoDB console at https://console.aws.amazon.com/dynamodb/.\n2. In the navigation pane on the left side of the console, under DAX, choose Clusters.\n3. Choose Create cluster. \n4. For Cluster name, enter a short name for your cluster. Choose the node type for all of the nodes in the cluster, and for the cluster size, use 3 nodes. \n5. In Encryption, make sure that Enable encryption is selected.\n6. After choosing the IAM role, subnet group, security groups, and cluster settings, choose Launch cluster.","multiregional":false,"service":"Amazon DynamoDB Accelerator"},"ecc-aws-491":{"article":"Always review cross-account attachment requests to your Transit gateway and approve them only if you trust the source.","impact":"Turning on AutoAcceptSharedAttachments configures a Transit Gateway to automatically accept any cross-account VPC attachment requests without verifying the request or the account the attachment is originating from which can lead to unauthorized access.","report_fields":["TransitGatewayArn"],"remediation":"1. Sign in to the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n2. Choose Transit Gateways.\n3. Choose relevant gateway and click Actions and select Modify.\n4. Uncheck 'Auto accept shared attachments'.\n5. Save changes.","multiregional":false,"service":"AWS Transit Gateway"},"ecc-aws-393":{"article":"This policy identifies the ACM that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["CertificateArn","DomainName"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon ACM at https://console.aws.amazon.com/acm.\n2. Click on the required acm certificate.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"AWS Certificate Manager"},"ecc-aws-411":{"article":"This policy identifies the FSX that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ResourceARN"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon EFS at https://console.aws.amazon.com/sns/v3.\n2. Click on the required sns topic.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon FSx"},"ecc-aws-027":{"article":"Ensure access is restricted from 0-65535 and from all other protocols and ports.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and  loss of data. \nA wide range of open ports inside EC2 security groups is a bad practice. It allows attackers to use port scanners and other probing techniques to identify services used on your instances and exploit their vulnerabilities.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"1. Login to the AWS Console \n2. Go to Security Group \n3. Go to the SG rules\n4. Click on the reported Firewall rule \n5. Click on Edit \n6. Modify the rule \n7. Click on Save","multiregional":false,"service":"Amazon EC2"},"ecc-aws-374":{"article":"Data events provide visibility into the resource operations performed on or within a resource. These are also known as data plane operations. Data events are often high-volume activities.\nUse the following data event resource types with basic event selectors: Amazon S3 object-level API activity, AWS Lambda function execution activity, Amazon DynamoDB object-level API activity on tables.\nBy default, trails do not log data events. Additional charges apply for data events.","impact":"If security critical information is not recorded, there will be no trail for forensic analysis, and discovering the cause of problems or the source of attacks may become more difficult or impossible.","report_fields":["TrailARN"],"remediation":"To enable Data events for all CloudTrail trails existing in your AWS account, perform the following: \n  1. Open the Amazon CloudTrail console at https://console.aws.amazon.com/cloudtrail/. \n  2. In the navigation pane, choose Trails.\n  3. Choose the trail that you want to reconfigure.\n  4. Click the Edit button, next to the Data events section.\n  5. Select data event type to log and configure according to your requirements.\n  5. To add another data type on which to log data events, choose Add data event type.\n  6. Choose Update trail.","multiregional":false,"service":"AWS CloudTrail"},"ecc-aws-256":{"article":"Ensure that encryption at rest is enabled within your Amazon Glue security configurations to meet regulatory requirements and prevent unauthorized users from getting access to the data written to Amazon S3. A security configuration is a set of security properties that can be used by AWS Glue to configure encryption for processes and resources associated with the security configuration such as jobs, crawlers and development endpoints. \nWith S3 encryption enabled, when you run crawlers, execute ETL jobs or start development endpoints, AWS Key Management Service (KMS) keys are used to encrypt your data at rest.","impact":"When encryption of data at rest is disabled, it can lead to unauthorized access to data.","report_fields":["Name"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo create and configure a new AWS Glue security configuration, perform the following actions: \n1. Sign in to AWS Management Console.\n2. Navigate to Glue service dashboard at https://console.aws.amazon.com/glue/.\n3. In the left navigation panel, under Security, choose Security configurations.\n4. Click Add security configuration to create new security configuration.\n5. On Add security configuration page, perform the following actions:\n  5.1 Provide a unique name for your new configuration in the Security configuration name box.\n  5.2 Select S3 encryption checkbox to enable at-rest encryption when writing data to Amazon S3, then choose the ARN of the AWS KMS key that you want to use for encryption.\n6. Reconfigure any existing Amazon Glue ETL jobs, crawlers, and development endpoints to make use of the new security configuration created at the previous step.","multiregional":false,"service":"AWS Glue"},"ecc-aws-291":{"article":"Ensure that a compliant lifecycle configuration is enabled for your Amazon Backup plans in order to meet compliance requirements when it comes to security and cost optimization. \nThe lifecycle defines when a backup is transitioned to cold storage and when it expires. AWS Backup transitions and expires backups automatically according to the lifecycle that you define. \nBackups that are transitioned to cold storage must be stored in cold storage for a minimum of 90 days. Therefore, on the console, the \u201cexpire after days\u201d setting must be 90 days longer than the \u201ctransition to cold after days\u201d setting. \nWhen backups reach the end of their lifecycle and are marked for deletion as part of your lifecycle policy, AWS Backup deletes the backups at a randomly chosen point over the following 8 hours.","impact":"If an AWS Backup lifecycle configuration is not used or has non-compliant values, it could violate security best practices and not meet regulatory compliance within your organization. It could also be economically ineffective.","report_fields":["BackupPlanName","BackupPlanId"],"remediation":"To implement compliant lifecycle configurations for your existing Amazon Backup plans, perform the following actions:\n1. Open the Amazon Backup console at https://console.aws.amazon.com/backup/.\n2. In the left navigation panel, select Backup plans.\n3. Select the backup plan that you want to configure.\n4. Under 'Backup rules' section, select the backup rule that you want to update, then click Edit.\n5. Under 'Transition to cold storage' select 'Days' from dropdown list and enter the number of days for the MoveToColdStorageAfterDays configuration parameter. You can't change this value after a copy has transitioned to cold storage. \n6. Repeat the step 5 for 'Retention period'. Note that Retention period setting must be at least 90 days after your Transition to cold storage setting. \n7. Click Save.","multiregional":false,"service":"AWS Backup"},"ecc-aws-494":{"article":"AWS Fargate platform versions refer to a specific runtime environment for Fargate task infrastructure, which is a combination of kernel and container runtime versions. New platform versions are released as the runtime environment evolves. For example, a new version may be released for kernel or operating system updates, new features, bug fixes, or security updates. Security updates and patches are deployed automatically for your Fargate tasks. If a security issue is found that affects a platform version, AWS patches the platform version.","impact":"With an automatic update disabled, you are missing updates that may contain new software features, bug fixes, security patches and performance improvements.","report_fields":["serviceArn"],"remediation":"To update ECS platform version: \n1. Open the Amazon ECS console at https://console.aws.amazon.com/ecs/\n2. Open required cluster.\n3. Click on the 'service'.\n4. Click on the 'Update'.\n5. Change 'Platform version' to 'LATEST'.\n6. Update service.","multiregional":false,"service":"Amazon Elastic Container Service"},"ecc-aws-497":{"article":"The Kubernetes project is rapidly evolving, introducing new features, design updates, and bug fixes. The community releases new Kubernetes minor versions (1.XX), as generally available approximately every three months. Each minor version is supported for approximately nine months after it is first released. As new Kubernetes versions become available for Amazon EKS, we recommend that you proactively update your clusters to use the latest available version.","impact":"Without keeping the Kubernetes container-orchestration system up-to-date, it is possible to miss out on new software features, bug fixes, security patches, and performance improvements.","report_fields":["arn"],"remediation":"1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n2. Choose the name of the Amazon EKS cluster to update and choose Update cluster version. \n3. For Kubernetes version, select the version to update your cluster to and choose Update. \n4. For Cluster name, type the name of your cluster and choose Confirm. The update takes several minutes to complete.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-064":{"article":"A VPC comes with a default security group the initial settings of which deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances assigned to the security group. If you do not specify the security group when launching an instance, the instance is automatically assigned to this default security group.\nSecurity groups provide stateful filtering of ingress/egress network traffic to AWS resources. It is recommended that the default security group restrict all traffic.","impact":"Not blocking outbound traffic in default security group can increase opportunities for malicious activity such as compromising other resources in the VPC.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"Perform the following to implement the prescribed state:\n1. Identify AWS resources that exist within the default security group.\n2. Create a set of least privilege security groups for those resources.\n3. Place the resources in those security groups.\n4. Remove the resources noted in #1 from the default security group.\nSecurity Group State \n1. Login to the AWS Management Console at https://console.aws.amazon.com/vpc/home.\n2. Repeat the next steps for all VPCs, including the default VPC in each AWS region:\n3. In the left pane, click Security Groups.\n4. For each default security group, perform the following:\n5. Select the default security group.\n6. Click on the Inbound Rules tab.\n7. Remove any inbound rules.\n8. Click on the Outbound Rules tab.\n9. Remove any inbound rules.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-320":{"article":"'sec_protocol_error_trace_action' specifies the kind of logging the database server does when bad packets are received from a possibly malicious client, apart from the client receiving the error.","impact":"If 'sec_protocol_error_trace_action' is not set to LOG, there will be no trail for forensic analysis and discovering of the bad packets that were received from the client which could result in a denial-of-service condition.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in the search bar, type \"sec_protocol_error_trace_action\".\n6. Choose LOG\n7. Choose Save changes.\n8. Restart instance","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-449":{"article":"By using relocation in Amazon Redshift, you enable Amazon Redshift to move a cluster to another Availability Zone (AZ) without any loss of data or changes to your applications. With relocation, you can continue operations when there is an interruption of service on your cluster with minimal impact.","impact":"Disabled relocation in Amazon Redshift threaten the availability of stored data.","report_fields":["ClusterIdentifier"],"remediation":"To enable relocation for a new cluster:\n1. Sign in to the AWS Management Console and open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. On the navigation menu, choose Clusters.\n3. Choose Create cluster to create a new cluster.\n4. Under Backup, for Cluster relocation, choose Enable. Relocation is disabled by default.\n5. Under Network and security, for Publicly accessible, accept the default Disable. If you choose Enable, Amazon Redshift returns an error.\n5. Choose Create cluster.\n\nModifying relocation for an existing cluster:\n1. Sign in to the AWS Management Console and open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. On the navigation menu, choose Clusters.\n3. Choose the name of the cluster that you want to modify from the list. The cluster details page appears.\n4. Choose the Maintenance tab, then in the Backup details section choose Edit.\n5. Under Backup, choose Enable. Relocation is disabled by default.\n6. Choose the Properties tab, then in the Network and security section make sure to choose Disable for the Publicly accessible option.\n7. In the Network and security section, make sure to choose Disable for the Publicly accessible option.\n8. Choose Modify cluster.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-172":{"article":"By default, Elastic Beanstalk configures the proxy to forward requests to your application on port 8080. \nUnrestricted access (0.0.0.0/0) increases opportunities for malicious activity, such as unauthorized access, denial-of-service attacks, and loss of data.","impact":"Unrestricted access increases the opportunity for malicious activity such as unauthorized access, denial-of-service attacks, and loss of data.","report_fields":["GroupId","VpcId","OwnerId"],"remediation":"From Console:  \n1. Login to AWS Management Console and open the EC2 console using https://console.aws.amazon.com/ec2/.\n2. In the navigation pane, choose Security Groups.\n3. Select the security group to update, choose Actions, and then choose Edit inbound rules to remove an inbound rule or Edit outbound rules to remove an outbound rule.\n4. Choose the Delete button to the right of the rule to delete.\n5. Choose Preview changes, Confirm.","multiregional":false,"service":"Amazon EC2"},"ecc-aws-231":{"article":"When logging_collector is enabled, the log_rotation_age parameter determines the  maximum lifetime of an individual log file (depending on the value of log_filename). After  this many minutes have elapsed, a new log file will be created via automatic log file  rotation. Current best practices advise log rotation at least daily, but your organization's  logging policy should dictate your rotation schedule.","impact":"For instance, if the rotation age is 1 day or any long period of time. Then there is a single log file for the entire period, and it will be difficult to look for specific queries when required.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group.\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_rotation_age\".\n6. Type for example 60 in value field.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-535":{"article":"If the Classic Load Balancer uses HTTPS/SSL listeners it should use a certificate provided by AWS Certificate Manager.\nFor certificates in a Region supported by AWS Certificate Manager (ACM), it is recommend that you use ACM to provision, manage, and deploy your server certificates. In unsupported Regions, you must use IAM as a certificate manager. \nACM is the preferred tool to provision, manage, and deploy your server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew.\nUse IAM as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. Additionally, you cannot manage your certificates from the IAM Console.","impact":"Managing server certificates in AWS IAM Console is not supported.","report_fields":["LoadBalancerArn"],"remediation":"1. Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. \n2. On the navigation pane, under 'Load Balancing', choose 'Load Balancers'. \n3. Select the reported ELB. \n4. On the 'Listeners' tab, choose 'Edit'.\n5. For 'SSL Certificate', choose 'Change'.\n6. Select 'Choose an existing certificate from AWS Certificate Manager (ACM)', select the certificate from 'Certificate', and then choose 'Save'.\n7. Choose 'Save' to add the listeners you just configured.","multiregional":false,"service":"Amazon Elastic Load Balancing"},"ecc-aws-240":{"article":"Enabling the log_hostname setting causes the hostname of the connecting host to be logged in addition to the host's IP address for connection log messages. Disabling the setting causes only the connecting host's IP address to be logged, and not the hostname. Unless your organization's logging policy requires hostname logging, it is best to disable this setting so as not to incur the overhead of DNS resolution for each statement that is logged.","impact":"When log_hostname is enabled it can incur the overhead of DNS resolution for each statement that is logged.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_hostname\".\n6. Choose 0.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-249":{"article":"Content encoding allows API clients to request content to be compressed before being sent back in the response to an API request. \nThis reduces the amount of data that is sent from API Gateway to API clients and decreases the time it takes to transfer the data.","impact":"Without compression, API performance and bandwidth utilization will be worse than with the compression enabled.","report_fields":["id","name"],"remediation":"1. Sign in to the API Gateway console.\n2. Choose an existing API.\n3. In the primary navigation pane, choose Settings under the API you chose.\n4. Under the Content Encoding section in the Settings pane, select the Content Encoding enabled option to enable payload compression. Enter a number for the minimum compression size (in bytes) next to Minimum body size required for compression. To disable the compression, clear the Content Encoding enabled option.\n5. Choose Save Changes.","multiregional":false,"service":"Amazon API Gateway"},"ecc-aws-078":{"article":"Real-time monitoring of API calls can be achieved by directing CloudTrail Logs to CloudWatch Logs and establishing corresponding metric filters and alarms.\nIt is recommended that a metric filter and alarm be established for root login attempts. Monitoring for root account logins will provide visibility into the use of a fully privileged account and an opportunity to reduce the use of it.","impact":"Lack of monitoring and logging of root usage can lead to insufficient response or no response to any changes made by root user.","report_fields":["account_id","account_name"],"remediation":"Perform the following to setup the metric filter, alarm, SNS topic, subscription and trail using the AWS CLI: \n1. Create a log group.\naws logs create-log-group --log-group-name <cloudtrail_log_group_name>\n2. Create a log stream.\naws logs create-log-stream --log-group-name <cloudtrail_log_group_name> --log-stream-name <log-stream-name>\n3. Create a metric filter based on provided filter pattern which checks for Root account usage and the <cloudtrail_log_group_name> taken from step 1.\naaws logs put-metric-filter --log-group-name `<cloudtrail_log_group_name>` --filter-name `<root_usage_metric>` --metric-transformations metricName=`<root_usage_metric>` ,metricNamespace='CISBenchmark',metricValue=1 --filterpattern '{{ $.userIdentity.type = \"Root\" && $.userIdentity.invokedBy NOT EXISTS && $.eventType != \"AwsServiceEvent\" }}'\n4. Create a topic to which notifications will be published.\naws sns create-topic --name <sns_topic_name>\n5. Subscribe an endpoint to an Amazon SNS topic. If the endpoint type is HTTP/S or email, or if the endpoint and the topic are not in the same Amazon Web Services account, the endpoint owner must run the ConfirmSubscription action to confirm the subscription.\naws sns subscribe --topic-arn <sns_topic_arn>  --protocol email --notification-endpoint <sns_subscription_endpoints> \n6. Amazon SNS will send a subscription confirmation message to the endpoint. Confirm subscription to topic, by visiting the link in an email that you specified as notification endpoint.\n7. Create an alarm that is associated with the CloudWatch Logs Metric Filter created in step 3 and an SNS topic created in step 4.  \naws cloudwatch put-metric-alarm --alarm-name `<root_usage_alarm>` --metricname `<root_usage_metric>` --statistic Sum --period 300 --threshold 1 --comparison-operator GreaterThanOrEqualToThreshold --evaluation-periods 1 --namespace 'CISBenchmark' --alarm-actions <sns_topic_arn>\n8. Create an S3 bucket to deliver log files to:\naws s3api create-bucket --bucket <bucket_name> \n9. To deliver log files to an S3 bucket, CloudTrail must have the required permissions. The following policy allows CloudTrail to write log files to the bucket from supported regions. Replace myBucketName, [optionalPrefix]/, myAccountID, region, and trailName with the appropriate values for your configuration. \nCreate a file 'policy.json' with the following policy.\n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n      {\n          \"Sid\": \"AWSCloudTrailAclCheck20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:GetBucketAcl\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>\"\n      },\n      {\n          \"Sid\": \"AWSCloudTrailWrite20150319\",\n          \"Effect\": \"Allow\",\n          \"Principal\": {\"Service\": \"cloudtrail.amazonaws.com\"},\n          \"Action\": \"s3:PutObject\",\n          \"Resource\": \"arn:aws:s3:::<myBucketName>/[optionalPrefix]/AWSLogs/<myAccountID>/*\",\n          \"Condition\": {\n              \"StringEquals\": {\n                  \"s3:x-amz-acl\": \"bucket-owner-full-control\",\n                  \"aws:SourceArn\": \"arn:aws:cloudtrail:<region>:<myAccountID>:trail/<trailName>\"\n              }\n          }\n      }\n  ]\n}}\n10. Apply the Amazon S3 bucket policy to the Amazon S3 bucket created in step 8.\naws s3api put-bucket-policy --bucket <bucket_name> --policy file://<path to policy>.json\n11. Create a role for CloudTrail that enables it to send events to the CloudWatch Logs log group. To create the JSON file that will contain the policy document, open a text editor and save the following policy contents in a file called 'assume_role_policy_document.json'. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}}\n12. Create a role.\naws iam create-role --role-name <role_name> --assume-role-policy-document file://<path to assume_role_policy_document>.json\n13. Create the following role policy document for CloudTrail. This document grants CloudTrail the permissions required to create a CloudWatch Logs log stream in the log group you specify and to deliver CloudTrail events to that log stream. Save the policy document in a file called role-policy-document.json. Replace region, accountID, log_group_name, with the appropriate values for your configuration. \n{{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n\n      \"Sid\": \"AWSCloudTrailCreateLogStream2014110\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:CreateLogStream\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n\n    },\n    {\n      \"Sid\": \"AWSCloudTrailPutLogEvents20141101\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": [\n        \"arn:aws:logs:<region>:<accountID>:log-group:<log_group_name>:log-stream:<accountID>_CloudTrail_<region>*\"\n      ]\n    }\n  ]\n}}\n14. Run the following command to apply the policy to the role.\naws iam put-role-policy --role-name <role_name>  --policy-name cloudtrail-policy --policy-document file://<path to role-policy-document>.json\n15. Create a trail that specifies the settings for delivery of log data to an Amazon S3 bucket at is associated with the S3 bucket created in step 8, CloudWatch log group created in step 1 and IAM role created in step 12. \naws cloudtrail create-trail --include-global-service-events --is-multi-region-trail  --name <cloudtrail_trail_name>  --s3-bucket-name <bucket_name> --cloud-watch-logs-log-group-arn <cloudtrail_log_group_arn> --cloud-watch-logs-role-arn <role_name>\n16. Start the recording of Amazon Web Services API calls and log file delivery for a trail. For a trail that is enabled in all regions, this operation must be called from the region in which the trail was created.\naws cloudtrail start-logging --name <cloudtrail_trail_name>","multiregional":false,"service":"AWS Account"},"ecc-aws-457":{"article":"Apache Spark web UI makes it possible to monitor and debug AWS Glue ETL jobs running on the AWS Glue job system, and also Spark applications running on AWS Glue development endpoints. The Spark UI enables you to check the following for each job:\n  - The event timeline of each Spark stage;\n  - A directed acyclic graph (DAG) of the job;\n  - Physical and logical plans for SparkSQL queries;\n  - The underlying Spark environmental variables for each job.","impact":"With disabled Spark UI of Glue Job, it may be difficult to monitor and debug AWS Glue ETL jobs and Spark applications.","report_fields":["Name"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon Glue at https://console.aws.amazon.com/glue.\n2. Under the 'AWS Glue Studio' click on the 'Jobs'.\n3. Click on the required job.\n4. Open 'Job details' and click on the 'Advanced properties'.\n5. Enable the 'Spark UI' and choose 'Spark UI logs path' bucket.\n6. Save changes.","multiregional":false,"service":"AWS Glue"},"ecc-aws-404":{"article":"This policy identifies the EKS that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["arn"],"remediation":"1. Open the Amazon EKS console at https://console.aws.amazon.com/eks/home#/clusters.\n2. Click on the required eks cluster.\n3. Open 'Tags' and click on the 'Manage tags'.\n4. Add new tag and save.","multiregional":false,"service":"Amazon Elastic Kubernetes Service"},"ecc-aws-243":{"article":"Causes checkpoints and restart points to be logged in the server log. Some statistics are included in the log messages, including the number of buffers written and the time spent writing them.","impact":"In most cases, checkpoints are disrupting your RDS PostgreSQL database performance and can cause connections to stall for up to a few seconds while they occur. By disabling the \"log_checkpoints\" flag, you cannot get verbose logging of the checkpoint process for your PostgreSQL database instances to identify and troubleshoot sub-optimal PostgreSQL database performance.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_checkpoints\".\n6. Choose 1.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-046":{"article":"The root user account is the most privileged user in an AWS account. AWS Access Keys provide programmatic access to a given AWS account. It is recommended that all access keys associated with the root user account be removed.","impact":"Existing access keys can be used to compromise the account and gain unrestricted access to all AWS resources. Anyone who has your root access keys can gain unrestricted access to all the services within your AWS environment, including billing information.","report_fields":["account_id","account_name"],"remediation":"1. Sign in to the AWS Management Console as Root and open the IAM console at  https://console.aws.amazon.com/iam/. \n2. Click on <Root_Account_Name> at the top right and select My Security Credentials from the drop-down list.\n3. On the pop out screen, click on Continue to Security Credentials.\n4. Click on Access Keys (Access Key ID and Secret Access Key).\n5. Under the Status column, if there are any Keys which are Active: \n- Click on Make Inactive - (Disable the key temporarily - it may be needed again);\n- Click on Delete - (Deleted keys cannot be recovered).","multiregional":true,"service":"AWS Account"},"ecc-aws-505":{"article":"When creating a Redshift cluster, you should change the default admin username 'awsuser' to a unique value. Default usernames are public knowledge and should be changed upon configuration.","impact":"Well-known database Admin username could lead to unintended access.","report_fields":["ClusterIdentifier"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n1. Open the Amazon Redshift console at https://console.aws.amazon.com/redshift/.\n2. Click on the 'Create cluster'.\n3. Under 'Database configuration' change 'Admin user name'.\n4. Create Cluster.","multiregional":false,"service":"Amazon Redshift"},"ecc-aws-247":{"article":"Each Transit Gateway attachment comes with routes that can be installed in one or more transit gateway route tables. When an attachment is propagated to a transit gateway route table, these routes are installed in the route table. \nTo manage the VPC environment and the transit gateway, it is preferable to manually establish the table propagation for the transit gateway.","impact":"With the Default Route Table Propagation enabled, you have less control over which routes of Transit Gateway attachments will be associated with the Transit Gateway Route Table, and with this option enabled it will be harder to implement granular custom routing rules and routing requirements.","report_fields":["TransitGatewayArn"],"remediation":"Perform the following steps in order to set 'Default route table propagation' to disable:\n1. Sign in to the Amazon VPC console at https://console.aws.amazon.com/vpc/.\n2. Choose Transit Gateways.\n3. Choose relevant gateway and click Actions and select Modify.\n4. Uncheck 'Default propagation route table'.\n5. Update route table with the necessary routes.","multiregional":false,"service":"AWS Transit Gateway"},"ecc-aws-306":{"article":"The PostgreSQL executor is responsible for executing the plan handed over by the PostgreSQL planner. The executor processes the plan recursively to extract the required set of rows. The 'log_executor_stats' flag controls the inclusion of PostgreSQL executor performance statistics in the PostgreSQL logs for each query.","impact":"The 'log_executor_stats' flag enables a crude profiling method for logging PostgreSQL executor performance statistics which even though can be useful for troubleshooting, it may increase the amount of logs significantly and have performance overhead.","report_fields":["DBInstanceArn"],"remediation":"1. Open the Amazon RDS console at https://console.aws.amazon.com/rds/\n2. In the navigation pane, choose Parameter groups.\n3. Choose the required parameter group\n4. Choose Edit parameters.\n5. Under Parameters, in search bar type \"log_executor_stats\".\n6. Choose 0.\n7. Choose Save changes.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-aws-402":{"article":"This policy identifies the DMS instances that do not have any Tags. Tags can be used for easy identification and search.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["ReplicationInstanceIdentifier","ReplicationInstanceArn"],"remediation":"1. Sign in to the AWS Admin Console and access the Amazon DMS at https://console.aws.amazon.com/dms/v2.\n2. Click on the required instance.\n3. Open 'Tags' and click on the 'Add'.\n4. Add new tag and save.","multiregional":false,"service":"AWS Database Migration Service"},"ecc-aws-518":{"article":"An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.\nIt is recommended to configure lifecycle rules on your Amazon S3 bucket as these rules help you define actions that you want Amazon S3 to take during an object's lifetime. Configure their Amazon S3 Lifecycle, to manage your objects so that they are stored cost effectively throughout their lifecycle.","impact":"Without the S3 lifecycle configuration when versioning enabled, you are missing out on the opportunity to manage S3 objects so that they are stored cost-effectively throughout their lifecycle by moving data to more economical storage classes over time or expiring data based on the object age.","report_fields":["Name"],"remediation":"To create a lifecycle rule:\n  1. Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n  2. In the 'Buckets' list, choose the name of the bucket that you want to create a lifecycle rule for.\n  3. Choose the 'Management' tab, and choose 'Create lifecycle rule'.\n  4. In 'Lifecycle rule name', enter a name for your rule. The name must be unique within the bucket.\n  5. Choose the scope of the lifecycle rule:\n    - To apply this lifecycle rule to all objects with a specific prefix or tag, choose 'Limit the scope to specific prefixes or tags'.\n      - To limit the scope by prefix, in 'Prefix', enter the prefix.\n      - To limit the scope by tag, choose 'Add tag', and enter the tag key and value.\n    - To apply this lifecycle rule to all objects in the bucket, choose 'This rule applies to all objects in the bucket', and choose 'I acknowledge that this rule applies to all objects in the bucket'.\n  6. To filter a rule by object size, you can check 'Specify minimum object size, Specify maximum object size', or both options.\n    - When you're specifying a 'minimum object' size or 'maximum object size', the value must be larger than 0 bytes and up to 5TB. You can specify this value in bytes, KB, MB, or GB.\n    - When you're specifying both, the maximum object size must be larger than the minimum object size.\n  7. Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:\n    - Transition current versions of objects between storage classes\n    - Transition previous versions of objects between storage classes\n    - Expire current versions of objects\n    - Permanently delete previous versions of objects\n    - Delete expired delete markers or incomplete multipart uploads\n    Depending on the actions that you choose, different options appear.\n  8. To transition current versions of objects between storage classes, under 'Transition current versions of objects between storage classes':\n    a. In 'Storage class transitions', choose the storage class to transition to:\n      - Standard-IA\n      - Intelligent-Tiering\n      - One Zone-IA\n      - S3 Glacier Flexible Retrieval\n      - Glacier Deep Archive\n    b. In 'Days after object creation', enter the number of days after creation to transition the object.\n    ! Important:\n      When you choose the S3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, your objects remain in Amazon S3. You cannot access them directly through the separate Amazon S3 Glacier service.\n  9. To transition non-current versions of objects between storage classes, under 'Transition non-current versions of objects between storage classes':\n    a. In 'Storage class transitions', choose the storage class to transition to:\n      - Standard-IA\n      - Intelligent-Tiering\n      - One Zone-IA\n      - S3 Glacier Flexible Retrieval\n      - Glacier Deep Archive\n    b. In Days after object becomes non-current, enter the number of days after creation to transition the object.\n  10. To expire current versions of objects, under 'Expire previous versions of objects', in 'Number of days after object creation', enter the number of days.\n  ! Important:\n    In a non-versioned bucket the expiration action results in Amazon S3 permanently removing the object.\n  11. To permanently delete previous versions of objects, under 'Permanently delete noncurrent versions of objects', in 'Days after objects become noncurrent', enter the number of days. You can optionally specify the number of newer versions to retain by entering a value under 'Number of newer versions to retain'.\n  12. Under 'Delete expired delete markers or incomplete multipart uploads', choose' Delete expired object delete markers' and 'Delete incomplete multipart uploads'. Then, enter the number of days after the multipart upload initiation that you want to end and clean up incomplete multipart uploads.\n  13. Choose 'Create rule'.\n      If the rule does not contain any errors, Amazon S3 enables it, and you can see it on the 'Management' tab under 'Lifecycle rules'.","multiregional":true,"service":"Amazon S3"},"ecc-aws-442":{"article":"Encryption in transit - is when requests between AWS AppSync, the cache, and data sources (except insecure HTTP data sources) are encrypted at the network level. You can enable the encryption in transit only when first enabling caching for your AWS AppSync API. \nBecause there is some processing needed to encrypt and decrypt the data at the endpoints, in-transit encryption can impact performance.","impact":"Without enabled encryption in transit, AppSync accepts a connection whether it uses SSL or not. This can lead to malicious activity such as man-in-the-middle attacks (MITM), intercepting, or manipulating network traffic.","report_fields":["name","arn"],"remediation":"Note: Affected resource must be redeployed to mitigate the issue.\n\nTo encrypt, cache you should delete the existing cache and recreate it.\n1. Navigate to the https://console.aws.amazon.com/appsync/home\n2. Select the API that for which you want enable encryption.\n3. At the left panel, choose 'Caching'.\n4. Click 'Delete cache'.\n5. Submit deletion.\n6. Select caching behavior:\n  - Full request caching: All requests are fully cached.\n  - Per-resolver caching: Individual resolvers that you specify are cached.\n7. In 'Cache settings' section, make sure you have selected 'Encryption in transit' option.\n8. Click 'Create cache'.","multiregional":false,"service":"AWS AppSync"},"ecc-aws-041":{"article":"This policy identifies the AWS RDS instance that does not have any Tags. Tags can be used for easy identification and searches.","impact":"Without a tagging schema, it is harder to organize resources available within your AWS environment. As your AWS environment is becoming more and more complex, it requires better management strategies.","report_fields":["DBInstanceArn"],"remediation":"1. Login to the AWS Console. \n2. Choose RDS Service.\n3. Select your RDS DB without tags and click on its name.\n4. Click on tab 'Tags' and add a new one.\n5. Click on Save.","multiregional":false,"service":"Amazon Relational Database Service"},"ecc-k8s-016":{"article":"Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.\nBy default, profiling is enabled.","impact":"Profiling generates a significant amount of program data that could potentially be exploited to uncover system and program details, increasing  the potential attack surface.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter:\n--profiling=false","multiregional":true,"service":"Pod"},"ecc-k8s-067":{"article":"Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks.","impact":"Containers with additional capabilities beyond the default set can be used to perform actions that they would otherwise be prevented from doing. This can allow an attacker to gain unauthorized access to the host system, bypass security controls, or compromise other containers running on the same host.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  If you have containers:\n    spec:\n      containers:\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n  If you have initContainers:\n    spec:\n      initContainers:\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n  If you have ephemeralContainers:\n    spec:\n      ephemeralContainers:\n        securityContext:\n          capabilities:\n            drop:\n            - ALL","multiregional":true,"service":"Pod"},"ecc-k8s-081":{"article":"Capabilities permit certain named root actions without giving full root access and are considered a fine-grained permissions model.\nIt's recommended all capabilities should be dropped from a pod, with only those required added back. There are a large number of capabilities, with SYS_ADMIN bounding most. SYS_ADMIN is a highly privileged access level equivalent to root access and should generally be avoided.","impact":"Using the highly privileged access level of SYS_ADMIN can pose a significant security risk, potentially granting root access to unauthorized users and compromising the entire system.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and remove SYS_ADMIN capability from containers.","multiregional":true,"service":"Pod"},"ecc-k8s-062":{"article":"A container running in the host's IPC namespace can use IPC to interact with processes outside the container.\nIf you need to run containers which require hostIPC, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.","impact":"When the 'host_ipc' flag is set to true, it allows containers to share the host's IPC (inter-process communication) namespace. This means that the containers can use IPC to communicate with processes outside of their own container, including other containers on the same host. It increases the risk of potential security vulnerabilities, as containers can potentially access sensitive data or execute privileged commands on the host.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  spec:\n     host_ipc: false","multiregional":true,"service":"Pod"},"ecc-k8s-044":{"article":"Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.\nWhen peer client authentication is enabled, etcd peers require clients to present a valid client certificate during TLS negotiation to establish a secure connection. This ensures that only authorized clients with valid client certificates signed by a trusted Certificate Authority (CA) can access the etcd cluster, improving the security of the etcd deployment.","impact":"When peer client authentication is not enabled then etcd peers do not require clients to present a valid client certificate during TLS negotiation, and any client can connect to the etcd cluster without authentication. This can create significant security vulnerabilities, as it allows unauthorized clients to access and modify data stored in etcd. Attackers can potentially compromise the etcd cluster, steal sensitive data, and execute malicious actions, compromising the confidentiality, integrity, and availability of Kubernetes deployments.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the Control Plane node and set the '--peer-client-cert-auth' parameter to 'true'.","multiregional":true,"service":"Pod"},"ecc-k8s-049":{"article":"Seccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto a node to your Pods and containers.\nSeccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.\nThe default seccomp profile provides a sane default for running containers with seccomp and disables around 44 system calls out of 300+. It is moderately protective while providing wide application compatibility. If the docker/default seccomp profile is too restrictive for you, you would have to create/manage your own seccomp profiles.\nBy default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled.","impact":"If the seccomp profile is not set to the recommended value, it could allow attackers to bypass security controls and execute malicious code within the container.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Use security context to enable the docker/default seccomp profile in your pod definitions.\nFollow the Kubernetes documentation and set the seccomp profile in a configuration file: https://kubernetes.io/docs/tutorials/security/seccomp/\nEdit the pod specification file and set the below parameters:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault","multiregional":true,"service":"Pod"},"ecc-k8s-024":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key.","impact":"Broken transport security between apiserver and etcd services (non-HTTPS).","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. \nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate and key file parameters:\n- --etcd-certfile=</path/to/ca-file>\n- --etcd-keyfile=</path/to/key-file>\nReferences:\nhttps://kubernetes.io/docs/setup/best-practices/certificates/","multiregional":true,"service":"Pod"},"ecc-k8s-032":{"article":"Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.","impact":"Profiling can generate a significant amount of data that may contain sensitive information about the system and program. This data could potentially be exploited by attackers to gain insights into the system and to launch attacks.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the 'profiling' parameter to 'false'.","multiregional":true,"service":"Pod"},"ecc-k8s-038":{"article":"Profiling allows for the identification of specific performance bottlenecks via web interface 'host:port/debug/pprof/'. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.\nCaution: This parameter is ignored if a config file is specified in '--config'. If '--config' set, this rule may return incorrect result, if config file has this parameter 'enableProfiling' set to true.","impact":"Profiling generates a significant amount of program data that could potentially be exploited to uncover system and program details.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml file on the Control Plane node and set the below parameter:\n--profiling=false","multiregional":true,"service":"Pod"},"ecc-k8s-002":{"article":"The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication.","impact":"The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the documentation and configure alternate mechanisms for authentication: https://kubernetes.io/docs/reference/access-authn-authz/authentication/.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and remove the --token-auth-file=<filename> parameter.","multiregional":true,"service":"Pod"},"ecc-k8s-013":{"article":"When you create a pod, if you do not specify a service account, it is automatically assigned the default service account in the same namespace. You should create your own service account and let the API server manage its security tokens.\nBy default, ServiceAccount is set.","impact":"If the ServiceAccount admission control plugin is disabled, your new pod is not automatically assigned a default service, and the pod will not have access to the Kubernetes API server.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the documentation and create ServiceAccount objects as per your environment: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and ensure that the --disable-admission-pluginsparameter is set to a value that does not include ServiceAccount.","multiregional":true,"service":"Pod"},"ecc-k8s-070":{"article":"Containers may run as any Linux user. Containers which run as the root user, whilst constrained by Container Runtime security features still have a escalated likelihood of container breakout. Ideally, all containers should run as a defined non-UID 0 user.","impact":"Allowing containers to run as the root user poses security risks such as increased potential for container breakout, unauthorized access to host systems or other containers, and unrestricted privileges that could lead to malicious actions.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\nFor pod:\n  spec:\n    securityContext:\n      runAsNonRoot: true\n      runAsUser: <specific user UID other than 0>\nFor containers:\n  spec:\n    containers:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: <specific user UID other than 0>\nFor initContainers:\n  spec:\n    initContainers:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: <specific user UID other than 0>\nFor ephemeralContainers:\n  spec:\n    ephemeralContainers:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: <specific user UID other than 0>","multiregional":true,"service":"Pod"},"ecc-k8s-092":{"article":"Basic authentication uses plaintext credentials for authentication. Currently, the basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server. The basic authentication is currently supported for convenience. Hence, basic authentication should not be used.\nBy default, --basic-auth-file argument is not set and OAuth authentication is configured.\nOpenShift provides it's own fully integrated authentication and authorization mechanism. The APIserver is protected by either requiring an OAuth token issued by the platform's integrated OAuth server or signed certificates.","impact":"The basic authentication credentials last indefinitely, and the password cannot be changed without restarting the API server.","report_fields":["metadata.name","metadata.namespace"],"remediation":"None required. --basic-auth-file cannot be configured on OpenShift.","multiregional":true,"service":"ConfigMap"},"ecc-k8s-012":{"article":"The SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where  PodSecurityPolicy is not in place within the cluster.\nSecurityContextDeny can be used to provide a layer of security for clusters which do not have PodSecurityPolicy enabled.\nBy default, SecurityContextDeny is not set.","impact":"Without the SecurityContextDeny admission control plugin, pods that use some SecurityContext fields can lead to privilege escalation in the cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include SecurityContextDeny, unless PodSecurityPolicy is already in place.\n--enable-admission-plugins=...,SecurityContextDeny,...","multiregional":true,"service":"Pod"},"ecc-k8s-061":{"article":"A Windows container making use of the 'hostProcess' flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides \"privileged access\" to the Windows node.\nThese containers operate as normal processes but have access to the host network namespace, storage, and devices when given the appropriate user privileges. HostProcess containers can be used to deploy network plugins, storage configurations, device plugins, kube-proxy, and other components to Windows nodes without the need for dedicated proxies or the direct installation of host services.","impact":"Enabling HostProcess in Windows containers pose a high security risk, and can potentially lead to privilege escalation, malware infections, and increased potential for unauthorized access and data breaches.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Recreate a pod that violates this rule, and before applying it make sure that the setting 'security_context.windows_options.host_process' is removed from the pod definition or is not set to True.","multiregional":true,"service":"Pod"},"ecc-k8s-008":{"article":"RBAC is a powerful and flexible authorization mechanism that allows cluster administrators to define granular access policies that control who can access and perform actions on Kubernetes resources.","impact":"Without RBAC, any authenticated user or service account in the cluster will have unrestricted access to all resources and actions available in the API server. This means that an attacker who gains access to a user's or service account's credentials can potentially access and modify any resource in the cluster, including sensitive data such as secrets and configuration files. It is also difficult to enforce the principle of least privilege, which is a fundamental security concept that requires that users and applications are granted only the minimum privileges necessary to perform their tasks. This can lead to overprivileged accounts and applications that can access and modify resources that they should not be able to access, increasing the attack surface of the cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the etcd pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the '--authorization-mode' parameter to 'RBAC'.","multiregional":true,"service":"Pod"},"ecc-k8s-030":{"article":"TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.","impact":"The use of weak TLS ciphers in Kubernetes can compromise the security of the system, leaving it vulnerable to a range of attacks. It's essential to use strong TLS ciphersuits to prevent potential data breaches and unauthorized access.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the '--tls-cipher-suites' parameter to 'TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384'.","multiregional":true,"service":"Pod"},"ecc-k8s-005":{"article":"The connections from the API server to the kubelet are used for:\n  - Fetching logs for pods.\n  - Attaching (usually through kubectl) to running pods.\n  - Providing the kubelet's port-forwarding functionality.\nThese connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks.","impact":"By default, the API server does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets: https://v1-25.docs.kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --kubelet-certificate-authority parameter to the path to the cert file for the certificate authority.\n--kubelet-certificate-authority=<ca-string>","multiregional":true,"service":"Pod"},"ecc-k8s-072":{"article":"Using a read-only root file system for Kubernetes pods provides several benefits. It prevents any process from writing to the file system, which can help mitigate certain types of security attacks.","impact":"If a read-only root file system is not used it can make the system more vulnerable to security attacks, as attackers may be able to modify the file system to gain escalated privileges or execute malicious code. Additionally, a writable file system can make the system less stable, as changes to the file system can introduce instability and increase the likelihood of failures. Finally, a writable file system can make the system less predictable, as unexpected changes can occur that can lead to errors and failures.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  If you have containers:\n    spec:\n      containers:\n        securityContext:\n          readOnlyRootFilesystem: true\n  If you have initContainers:\n    spec:\n      initContainers:\n        securityContext:\n          readOnlyRootFilesystem: true\n  If you have ephemeralContainers:\n    spec:\n      ephemeralContainers:\n        securityContext:\n          readOnlyRootFilesystem: true","multiregional":true,"service":"Pod"},"ecc-k8s-007":{"article":"The Node authorization mode only allows kubelets to read Secret, ConfigMap, PersistentVolume, and PersistentVolumeClaim objects associated with their nodes.","impact":"The Node authorization mode provides a minimum necessary permission on a need-to-know basis for kubelet to do its work. \nWithout enabling this mode, kubelet may use a more permissive mode that grants excessive access to cluster resources.\nThis lack of restricted permissions can result in several security risks, including privilege escalation, unauthorized access to sensitive data, unauthorized modification or deletion of Kubernetes resources, and potential compromise of the entire cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to a value that includes Node. One such example could be as below:\n--authorization-mode=Node,RBAC","multiregional":true,"service":"Pod"},"ecc-k8s-035":{"article":"Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks.\nProviding the root certificate for the API server's serving certificate to the controller manager with the --root-ca-file argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server.\nBy default, --root-ca-file is not set.","impact":"Without proper certificate verification, there is a risk of falling victim to man-in-the-middle attacks, leading to severe security breaches and unauthorized access to sensitive data.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the --root-ca-file parameter to the certificate bundle file`.","multiregional":true,"service":"Pod"},"ecc-k8s-064":{"article":"A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.\nIf you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.","impact":"When the hostNetwork flag is set to true, a container can access the local loopback device and network traffic to and from other pods. This increases the risk of security vulnerabilities and compromises the isolation between containers.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  spec:\n     host_network: false","multiregional":true,"service":"Pod"},"ecc-k8s-052":{"article":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult. Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.","impact":"Placing objects in default namespace makes application of RBAC and other security controls more difficult.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.\nTo remediate this issue, you should recreate existing deployment resource and attach it to non-default namespace.\n1. If you do not have namespace, create one using command: 'kubectl create namespace <namespace_name>'.\n2. Before creating a new resource in different namespace, you should delete it from default namespace. Because you cannot move Kubernetes resources between namespaces. They must be recreated.\nUse the following command to delete a desired deployment from default namespace: 'kubectl delete -f <deployment_config>.yaml'.\nWhere '<deployment_config>.yaml' is a file that describes the deployment you want to recreate.\n3. Then in a file <deployment_config>.yaml, change parameter 'metadata.namespace' to non-default namespace. For example, to the namespace name that was created at step 1.\n4. Apply changes with command: 'kubectl apply -f <deployment_config>.yaml'.","multiregional":true,"service":"Deployment"},"ecc-k8s-066":{"article":"Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers. Ideally, all containers should drop this capability.","impact":"Allowing containers with the NET_RAW capability enabled can pose significant security risks, allowing for potential exploitation of networking exploits within the cluster, leading to data breaches, unauthorized access, and other security incidents.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\nFor containers\n  spec:\n    containers:\n      securityContext:\n        capabilities:\n          drop:\n            - NET_RAW\nfor initContainers:\n  spec:\n    initContainers:\n      securityContext:\n        capabilities:\n          drop:\n            - NET_RAW\nfor ephemeralContainers:\n  spec:\n    ephemeralContainers:\n      securityContext:\n        capabilities:\n          drop:\n            - NET_RAW","multiregional":true,"service":"Pod"},"ecc-k8s-031":{"article":"Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.","impact":"Failing to configure appropriate garbage collection thresholds may result in an unresponsive system or long periods of unusability.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Contoller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the ' --terminated-pod-gc-threshold' parameter to '10' or an appropriate value.","multiregional":true,"service":"Pod"},"ecc-k8s-026":{"article":"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If --client-ca-file argument is set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.","impact":"Without proper client certificate validation, an attacker may be able to connect to the Kubernetes API server and access resources or perform unauthorized actions. If an attacker is able to intercept traffic between the client and the Kubernetes API server, he may be able to inject his own certificate or spoof the client's certificate to gain unauthorized access.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set up the TLS connection on the apiserver.\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the client certificate authority file:\n--client-ca-file=<path/to/client-ca-file>\nReferences:\nhttps://kubernetes.io/docs/setup/best-practices/certificates/","multiregional":true,"service":"Pod"},"ecc-k8s-017":{"article":"Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path.\nBy default, auditing is not enabled.","impact":"Not setting the audit log path and enabling auditing can leave organizations vulnerable to security threats and hinder their ability to detect and respond to security incidents.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --audit-log-path parameter to a suitable path and file where you would like audit logs to be written, for example:\n--audit-log-path=/var/log/apiserver/audit.log\nFollow the Kubernetes documentation and configure Auditing: https://v1-25.docs.kubernetes.io/docs/tasks/debug/debug-cluster/audit/.","multiregional":true,"service":"Pod"},"ecc-k8s-071":{"article":"Do not generally permit containers with capabilities.\nContainers run with a default set of capabilities as assigned by the Container Runtime. Capabilities are parts of the rights generally granted on a Linux system to the root user. In many cases applications running in containers do not require any capabilities to operate, so from the perspective of the principal of least privilege use of capabilities should be minimized.\nBy default, there are no restrictions on the creation of containers with additional capabilities.","impact":"If containers with capabilities are generally permitted in Kubernetes Pods, it may introduce unnecessary security risks, as containers could potentially have excessive privileges that could be exploited by attackers, leading to potential privilege escalation attacks and increased vulnerability to unauthorized actions.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Review the use of capabilities in applications running on your cluster. Where applications do not require any Linux capabilities to operate consider editing the required pod YAML definition file and set the below parameters:\nFor containers:\n  spec:\n    containers:\n      securityContext:\n        capabilities:\n          drop:\n            - ALL\nFor initContainers:\n  spec:\n    initContainers:\n      securityContext:\n        capabilities:\n          drop:\n            - ALL\nFor ephemeralContainers:\n  spec:\n    ephemeralContainers:\n      securityContext:\n        capabilities:\n          drop:\n            - ALL","multiregional":true,"service":"Pod"},"ecc-k8s-087":{"article":"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation. Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.","impact":"Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Where possible, remove get, list and watch access to secret objects in the cluster.","multiregional":true,"service":"Role"},"ecc-k8s-059":{"article":"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue.","impact":"If 'automountServiceAccountToken' is true, it's increases the attack surface of the cluster. If an attacker gains control of the pod, they can access the service account token and use it to escalate their privileges, potentially compromising other resources in the cluster. This can lead to serious security breaches and data loss.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  spec:\n     automountServiceAccountToken: false","multiregional":true,"service":"Pod"},"ecc-k8s-023":{"article":"By default, if no --service-account-key-file is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with --service-account-key-file.","impact":"Service account tokens may have permissions that are beyond what the pod itself is authorized to perform. Without proper signing of service account tokens, an attacker may be able to use compromised pods to escalate privileges and gain access to sensitive resources.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver. yaml on the Control Plane node and set the --service-account-key-file parameter to the public key file for service accounts:\n- --service-account-key-file=/path/to/<filename>\nReferences:\nhttps://github.com/kubernetes/kubernetes/issues/24167","multiregional":true,"service":"Pod"},"ecc-k8s-060":{"article":"A container which mounts a hostPath volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of hostPath volumes may allow containers access to privileged areas of the node filesystem.\nHostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly. If restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts MUST be required to use readOnly mounts for the policy to be effective.","impact":"The use of hostPath volumes may allow containers access to privileged areas of the node filesystem.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Recreate a pod that violates this rule, and before applying it make sure that the setting spec.volumes[*].hostPath is removed from the pod definition.","multiregional":true,"service":"Pod"},"ecc-k8s-079":{"article":"Memory resources can be defined using values from bytes to petabytes, it is common to use mebibytes. If you configure a memory request that is larger than the amount of memory on your nodes, the pod will never be scheduled. When specifying a memory request for a container, include the resources:requests field in the container`s resource manifest. To specify a memory limit, include resources:limits.\nSetting memory requests enforces a memory limit for a container. A container is guaranteed to have as much memory as it requests, but is not allowed to use more memory than the limit set. This configuration may save resources and prevent an attack on an exploited container.","impact":"Not setting memory requests for Kubernetes containers can result in unpredictable application behavior, as the scheduler and kubelet use this information to allocate resources and enforce resource limits. This may lead to either excessive resource usage or reduced performance, depending on the application's memory requirements.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Add resource memory request to the pod or container specifications to prevent them from using more resources than necessary:\nFor containers:\n  spec:\n    containers:\n      resources:\n        requests:\n          memory: <memory request>\nFor initContainers:\n  spec:\n    initContainers:\n      resources:\n        requests:\n          memory: <memory request>","multiregional":true,"service":"Pod"},"ecc-k8s-053":{"article":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult. Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.","impact":"Placing objects in default namespace makes application of RBAC and other security controls more difficult.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.\nTo remediate this issue, you should recreate existing role resource and attach it to non-default namespace.\n1. If you do not have namespace, create one using command: 'kubectl create namespace <namespace_name>'.\n2. Before creating a new resource in different namespace, you should delete it from default namespace. Because you cannot move Kubernetes resources between namespaces. They must be recreated.\nUse the following command to delete a desired role from default namespace: 'kubectl delete -f <role_config>.yaml'.\nWhere '<role_config>.yaml' is a file that describes the role you want to recreate.\n3. Then in a file <role_config>.yaml, change parameter 'metadata.namespace' to non-default namespace. For example, to the namespace name that was created at step 1.\n4. Apply changes with command: 'kubectl apply -f <role_config>.yaml'.","multiregional":true,"service":"Role"},"ecc-k8s-074":{"article":"Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.\nIt is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs. Benefits for storing secrets as files include: setting file permissions, projects of secret keys to specific paths, consuming secret values from volumes, and secret values can be updated without restarting the pod.","impact":"If you don't use secrets as files in Kubernetes, sensitive information stored in the secrets can be exposed as plain text if secrets are used as environment variables, which can pose a security risk. Additionally, environment variables are often visible to all processes on the system, so there is a risk of accidental exposure or deliberate misuse of the information.","report_fields":["metadata.name","metadata.namespace"],"remediation":"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.\nReferences: \nhttps://v1-25.docs.kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod","multiregional":true,"service":"Pod"},"ecc-k8s-033":{"article":"The controller manager creates a service account per controller in the kube-system namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the --use-serviceaccount-credentials to true runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.","impact":"Not using service account credentials in Kubernetes can have significant security implications. Without service account credentials, controller loops would need to use other forms of authentication to access Kubernetes resources, such as usernames and passwords or API tokens. This could potentially result in security vulnerabilities if these credentials are compromised or not properly secured. In addition, without RBAC, control loops may have more permissions than necessary, which could lead to unintended consequences and increase the risk of unauthorized access to Kubernetes resources. Therefore, not using service account credentials could significantly increase the potential impact of security breaches in Kubernetes environments.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the '--use-service-account-credentials' parameter to 'true'.","multiregional":true,"service":"Pod"},"ecc-k8s-043":{"article":"Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.\netcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters.","impact":"Broken transport security in peer communications to etcd service (non-HTTPS).","report_fields":["metadata.name","metadata.namespace"],"remediation":"Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable\nFollow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters:\n- --peer-client-file=</path/to/peer-cert-file>\n- --peer-key-file=</path/to/peer-key-file>\nReferences:\nhttps://etcd.io/docs/v3.5/op-guide/security/","multiregional":true,"service":"Pod"},"ecc-k8s-027":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file.","impact":"Without proper validation of the etcd server's certificate, an attacker may be able to intercept and read or modify sensitive data being transmitted between the Kubernetes API server and etcd.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. \nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the etcd certificate authority file parameter:\n- --etcd-cafile=<path/to/ca-file>\nReferences:\nhttps://kubernetes.io/docs/setup/best-practices/certificates/","multiregional":true,"service":"Pod"},"ecc-k8s-047":{"article":"Kubernetes Roles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set it to be the wildcard \"*\" which matches all items.\nThe principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.","impact":"Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Where possible replace any use of wildcards in roles with specific objects or actions.\nThe following command can be used to edit roles: 'kubectl edit role/<role_name>'\nOr you can use a 'kubectl apply -f <config_file>.yaml' where <config_file>.yaml contains the yaml definition of the role that needs to be modified with your change included.","multiregional":true,"service":"Role"},"ecc-k8s-041":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.","impact":"If --client-cert-auth is not enabled, any client can access the etcd service without any authentication, including unauthorized users and malicious actors. This can lead to unauthorized access to sensitive data stored in the etcd service, such as secrets and configuration data. Without client authentication, the etcd service is vulnerable to attacks such as man-in-the-middle attacks, where an attacker can intercept and modify the communication between the client and the etcd service, potentially leading to data loss, data manipulation, or even complete compromise of the Kubernetes cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the Control Plane node and set the '--client-cert-auth' parameter to 'true'.","multiregional":true,"service":"Pod"},"ecc-k8s-075":{"article":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult. Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.","impact":"Placing objects in default namespace makes application of RBAC and other security controls more difficult.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.\nTo remediate this issue, you should recreate existing secret resource and attach it to non-default namespace.\n1. If you do not have namespace, create one using command: 'kubectl create namespace <namespace_name>'.\n2. Before creating a new resource in different namespace, you should delete it from default namespace. Because you cannot move Kubernetes resources between namespaces. They must be recreated. \nUse the following command to delete a desired secret from default namespace: 'kubectl delete -f <secret_config>.yaml'.\nWhere '<secret_config>.yaml' is a file that describes the secret you want to recreate.\n3. Then in a file <secret_config>.yaml, change parameter 'metadata.namespace' to non-default namespace. For example, to the namespace name that was created at step 1.\n4. Apply changes with command: 'kubectl apply -f <secret_config>.yaml'.","multiregional":true,"service":"Secret"},"ecc-k8s-014":{"article":"Setting admission control policy to NamespaceLifecycle ensures that objects cannot be created in non-existent namespaces, and that namespaces undergoing termination are not used for creating the new objects. This is recommended to enforce the integrity of the namespace termination process and also for the availability of the newer objects.\nBy default, NamespaceLifecycle is set.","impact":"If the NamespaceLifecycle admission control plugin is disabled, the integrity of the namespace termination process and the availability of newer objects cannot be ensured.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --disable-admission-plugins parameter to ensure it does not include NamespaceLifecycle.","multiregional":true,"service":"Pod"},"ecc-k8s-080":{"article":"The scheduler uses resource request information for containers in a pod to decide which node to place the pod on. The kubelet enforces the resource limits set, so that the running container is not allowed to use more resource than the limit set.\nIf a process in the container tries to consume more than the allowed amount of memory, the system kernel terminates the process that attempted the allocation, with an out of memory (OOM) error. With no limit set, kubectl allocates more and more memory to the container until it runs out.","impact":"Not setting resource limits for Kubernetes containers can lead to inefficient resource usage and potential performance issues. Without this information, Kubernetes may not be able to make informed decisions about resource allocation, potentially resulting in resource overuse or the termination of processes due to out-of-memory errors.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Add resource memory limits to the pod or container specifications to prevent them from using more resources than necessary:\nFor containers:\n  spec:\n    containers:\n      resources:\n        limits:\n          memory: <memory limits>\nFor initContainers:\n  spec:\n    initContainers:\n      resources:\n        limits:\n          memory: <memory limits>","multiregional":true,"service":"Pod"},"ecc-k8s-037":{"article":"Do not bind the Controller Manager service to non-loopback insecure addresses.\nThe Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface.\nBy default, the --bind-address parameter is set to 0.0.0.0.","impact":"If --bind-address parameter is not bound to a localhost interface, unauthorized user can access health and metrics information of a Controller Manager.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter","multiregional":true,"service":"Pod"},"ecc-k8s-028":{"article":"etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures.","impact":"If an attacker gains unauthorized access to the etcd datastore or intercepts network traffic to the Kubernetes API server, he may be able to access sensitive data in a plaintext.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and configure a EncryptionConfig file. \nThen, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --encryption-provider-config parameter to the path of that file:\n- --encryption-provider-config=</path/to/EncryptionConfig/File>\nReferences:\nhttps://kubernetes.io/docs/setup/best-practices/certificates/","multiregional":true,"service":"Pod"},"ecc-k8s-069":{"article":"Readiness Probe is a Kubernetes capability that enables teams to make their applications more reliable and robust. This probe regulates under what circumstances the pod should be taken out of the list of service endpoints so that it no longer responds to requests. In defined circumstances the probe can remove the pod from the list of available service endpoints.","impact":"Lack of the readiness probe can make the application less reliable and make it more difficult to prevent failure and apply recovery from unexpected errors.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Configure a readiness probe for all containers (where applicable).","multiregional":true,"service":"Pod"},"ecc-k8s-054":{"article":"Host ports connect containers directly to the host's network. \nDon't specify a 'hostPort' settings in either the 'container' or 'initContainer' sections for a pod unless it is absolutely necessary. When you bind a pod to a 'hostPort', it limits the number of places the pod can be scheduled, because each <hostIP, hostPort, protocol> combination must be unique. If you don't specify the hostIP and protocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as the default protocol.\nIf you only need access to the port for debugging purposes, you can use the API Server proxy or 'kubectl port-forward'.\nIf you explicitly need to expose a pod's port on the node, consider using a NodePort Service before resorting to hostPort.","impact":"Host ports connect containers directly to the host's network. This can bypass controls such as network policy. Also, it limits the number of places the Pod can be scheduled.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Recreate a pod that violates this rule, and before applying it make sure that the setting 'hostPort' is removed from pod definition in the following paths:\n  - spec.containers[*].ports[*].hostPort\n  - spec.initContainers[*].ports[*].hostPort","multiregional":true,"service":"Pod"},"ecc-k8s-088":{"article":"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation. Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.","impact":"Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Where possible, remove get, list and watch access to secret objects in the cluster.","multiregional":true,"service":"ClusterRole"},"ecc-k8s-006":{"article":"Any request that is successfully authenticated (including an anonymous request) is then authorized. This means that every authenticated request to the Kubernetes API will be successfully authorized if authorization mode is set to 'AlwaysAllow'. This mode should not be used on any production cluster.","impact":"The 'AlwaysAllow' authorization mode explicitly allow unauthorized requests that allows anonymous users to gain access to your resources.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --authorization-mode parameter to values other than AlwaysAllow. One such example could be as below:\n--authorization-mode=RBAC","multiregional":true,"service":"Pod"},"ecc-k8s-003":{"article":"This admission controller rejects all net-new usage of the 'Service' field 'externalIPs' and mitigates a known security vulnerability CVE-2020-8554. This feature is very powerful (allows network traffic interception) and not well controlled by policy. When enabled, users of the cluster may not create new Services which use 'externalIPs' and may not add new values to 'externalIPs' on existing 'Service' objects. Existing uses of 'externalIPs' are not affected, and users may remove values from 'externalIPs' on existing 'Service' objects.\nMost users do not need the ability to set the 'externalIPs' field for a 'Service' at all, and cluster admins should consider disabling this functionality by enabling the 'DenyServiceExternalIPs' admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage.","impact":"Kubernetes API server allow an attacker who is able to create a ClusterIP service and set the spec.externalIPs field, to intercept traffic to that IP address. Additionally, an attacker who is able to patch the status (which is considered a privileged operation and should not typically be granted to users) of a LoadBalancer service can set the status.loadBalancer.ingress.ip to similar effect.","report_fields":["metadata.name","metadata.namespace"],"remediation":"The best approach for mitigation is to restrict the use of ExternalIPs in a cluster, for this install an admission controller to prevent the use of ExternalIPs.\nEdit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and add 'DenyServiceExternalIPs' to the `--enable-admission-plugins' parameter. Check that 'DenyServiceExternalIPs' not set in '--disable-admission-plugins' parameter.","multiregional":true,"service":"Pod"},"ecc-k8s-042":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.","impact":"When the --auto-tls argument is set to true, automatically generated self-signed certificates are used for TLS encryption, which could compromise the security of sensitive objects stored in the etcd key-value store. This misconfiguration could allow unauthenticated clients to access these objects, potentially leading to unauthorized data access or tampering. To avoid this, the --auto-tls argument should be set to false and valid certificates should be used for client authentication to secure the etcd service.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the Control Plane node and set the '--auto-tls' parameter to 'false'.","multiregional":true,"service":"Pod"},"ecc-k8s-077":{"article":"Roles with the impersonate, bind or escalate permissions should not be granted unless strictly required.","impact":"Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators. The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster-admin level.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Where possible, remove the impersonate, bind and escalate rights from subjects.","multiregional":true,"service":"Role"},"ecc-k8s-040":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit.","impact":"Broken transport security in client-to-server communications to etcd service (non-HTTPS).","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the etcd service documentation and configure TLS encryption.\nThen, edit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the master node and set the below parameters:\n- --cert-file=</path/to/ca-file>\n- --key-file=</path/to/key-file>\nReferences:\nhttps://etcd.io/docs/v3.5/op-guide/security/","multiregional":true,"service":"Pod"},"ecc-k8s-021":{"article":"Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed.","impact":"Incorrectly specifying the value for the global request timeout limit can have serious consequences. If the timeout is set too low, it can lead to inaccessible cluster resources for users with slower connection speeds, resulting in a poor user experience. On the other hand, setting the timeout limit too high can exhaust the API server's resources, making it vulnerable to Denial-of-Service attacks.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --request-timeout parameter to 300 or as an appropriate value.","multiregional":true,"service":"Pod"},"ecc-k8s-068":{"article":"The kubelet uses liveness probes to know when to schedule restarts for containers. Restarting a container in a deadlock state can help to make the application more available, despite bugs.","impact":"Lack of the liveness probe can lead to a performance degradation and possibly allow to perform denial of service (DoS) attack.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Configure a liveness probe for all containers (where applicable).","multiregional":true,"service":"Pod"},"ecc-k8s-010":{"article":"Setting admission control plugin AlwaysAdmit allows all requests and do not filter any requests. The AlwaysAdmit admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers.","impact":"Setting admission control plugin AlwaysAdmit allows all requests without filtering, which can lead to unwanted or malicious activity in the cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and either remove the --enable-admission-plugins parameter, or set it to a value that does not include AlwaysAdmit.","multiregional":true,"service":"Pod"},"ecc-k8s-051":{"article":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult. Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.","impact":"Placing objects in default namespace makes application of RBAC and other security controls more difficult.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.\nTo remediate this issue, you should recreate existing ConfigMap resource and attach it to non-default namespace.\n1. If you do not have namespace, create one using command: 'kubectl create namespace <namespace_name>'.\n2. Before creating a new resource in different namespace, you should delete it from default namespace. Because you cannot move Kubernetes resources between namespaces. They must be recreated.\nUse the following command to delete desired ConfigMap from default namespace: 'kubectl delete -f <configmap>.yaml'.\nWhere '<configmap>.yaml' is a file that describes the ConfigMap you want to recreate.\n3. Then in a file <configmap>.yaml, change parameter 'metadata.namespace' to non-default namespace. For example, to the namespace name that was created at step 1.\n4. Apply changes with command: 'kubectl apply -f <configmap>.yaml'.","multiregional":true,"service":"ConfigMap"},"ecc-k8s-063":{"article":"Do not generally permit containers to be run with the allowPrivilegeEscalation flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with. It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.\nA container running with the allowPrivilegeEscalation flag set to true may have processes that can gain more privileges than their parent.","impact":"Enabling the allowPrivilegeEscalation flag in container runtime settings can pose a security risk as it may allow containers to gain more privileges than their parent processes, potentially leading to privilege escalation attacks.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  If you have containers:\n    spec:\n      containers:\n        securityContext:\n          allowPrivilegeEscalation: false\n  If you have initContainers:\n    spec:\n      initContainers:\n        securityContext:\n          allowPrivilegeEscalation: false\n  If you have ephemeralContainers:\n    spec:\n      ephemeralContainers:\n        securityContext:\n          allowPrivilegeEscalation: false","multiregional":true,"service":"Pod"},"ecc-k8s-019":{"article":"Retaining old log files ensures that one would have sufficient log data available for carrying out any investigation or correlation.","impact":"If old log files are not retained, there may not be sufficient log data available for carrying out any investigation or correlation, which can hinder the ability to identify and troubleshoot issues.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --audit-log-maxbackup parameter to 10 or as an appropriate value.","multiregional":true,"service":"Pod"},"ecc-k8s-022":{"article":"If Service account lookup is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted.","impact":"Not enabling this option can lead to a security vulnerability known as a time of check to time of use issue, where a service account token can be used even after the corresponding service account has been deleted. This can potentially result in unauthorized access to resources and data, as the token may have been issued to an account with specific permissions and privileges.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the '--service-account-lookup' parameter to 'true'.","multiregional":true,"service":"Pod"},"ecc-k8s-025":{"article":"API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic.","impact":"Broken transport security to kube-apiserver service (non-HTTPS).","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. \nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the master node and set the TLS certificate and private key file parameters:\n- --tls-cert-file=</path/to/ca-file>\n- --tls-private-key-file=</path/to/key-file>\nReferences:\nhttps://kubernetes.io/docs/setup/best-practices/certificates/","multiregional":true,"service":"Pod"},"ecc-k8s-058":{"article":"Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster. Avoiding mounting these tokens removes this attack avenue.\nBy default, 'automountServiceAccountToken' field in a Kubernetes service account is 'true'.","impact":"Enabling 'automountServiceAccountToken' in Kubernetes increases the cluster's attack surface, allowing potential unauthorized access to the service account token and escalating privileges.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required service accounts and set the below parameters:\n  automountServiceAccountToken: false","multiregional":true,"service":"ServiceAccount"},"ecc-k8s-001":{"article":"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests. \nIf you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.\nBy default, anonymous access is enabled, so if you don't see it in the list of parameters passed to the server anonymous access will be enabled.\nOn it's own that won't actually give attackers a lot of access to the cluster, as it only covers one of the three gates a request passes through before it's processed. As shown in the Kubernetes documentation 'Controlling Access to the Kubernetes API', the request then has to pass authorization and admission control. However it does mean that in many configurations the only thing stopping attackers compromising your cluster is RBAC rules, so a single mistake could cause significant issues, especially if your cluster is exposed to the internet.","impact":"When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated. This may expose certain information, and capabilities to an attacker with access to the cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml on the Control Plane node and set the below parameter:\n--anonymous-auth=false","multiregional":true,"service":"Pod"},"ecc-k8s-065":{"article":"A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.\nIf you need to run containers which require 'hostPID', this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.","impact":"When the 'hostPID' flag is set to true, a container can inspect processes running outside of the container, potentially leading to the escalation of privileges outside of the container if the container has access to ptrace capabilities.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit required pod YAML definition file and set the below parameters:\n  spec:\n     host_pid: false","multiregional":true,"service":"Pod"},"ecc-k8s-039":{"article":"Do not bind the scheduler service to non-loopback insecure addresses.\nThe Scheduler API service which runs on port 10259/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface.\nBy default, the --bind-address parameter is set to 0.0.0.0 .","impact":"If --bind-address parameter is not bound to a localhost interface, unauthorized user can access health and metrics information of a scheduler.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Scheduler pod specification file /etc/kubernetes/manifests/kube-scheduler.yaml on the Control Plane node and ensure the correct value for the --bind-address parameter.","multiregional":true,"service":"Pod"},"ecc-k8s-036":{"article":"RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.\nNote: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.\nBy default, RotateKubeletServerCertificate is set to \"true\" this recommendation verifies that it has not been disabled.","impact":"If RotateKubeletServerCertificate is set to \"false\" and kubelets get their certificates from the API server, it may impact the availability of the Kubernetes cluster. Expired certificates may also create security vulnerabilities, as they can be exploited by malicious actors to gain unauthorized access or perform other malicious activities.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the --feature-gates parameter to include RotateKubeletServerCertificate=true:\n--feature-gates=...,RotateKubeletServerCertificate=true,...","multiregional":true,"service":"Pod"},"ecc-k8s-048":{"article":"Kubernetes ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set it to be the wildcard \"*\" which matches all items.\nThe principle of least privilege recommends that users are provided only the access required for their role and nothing more. The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.","impact":"Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Caution: API servers create a set of default ClusterRole objects. Many of these are 'system:' prefixed, which indicates that the resource is directly managed by the cluster control plane. Take care when modifying ClusterRoles with names that have a 'system:' prefix. Modifications to these resources can result in non-functional clusters.\n\nWhere possible replace any use of wildcards in ClusterRoles with specific objects or actions.\nThe following command can be used to edit ClusterRoles: 'kubectl edit clusterrole/<clusterrole_name>'\nOr you can use a 'kubectl apply -f <config_file>.yaml' where <config_file>.yaml contains the yaml definition of the ClusterRole that needs to be modified with your change included.","multiregional":true,"service":"ClusterRole"},"ecc-k8s-015":{"article":"Using the NodeRestriction plug-in ensures that the kubelet is restricted to the Node and Pod objects that it could modify as defined. Such kubelets will only be allowed to modify their own Node API object, and only modify Pod API objects that are bound to their node.\nBy default, NodeRestriction is not set.","impact":"Kubelets will be allowed to modify API objects of all pods and Nodes. If an attacker gives access to a Node, it can lead to privilege escalation and it will give access not only to it, but also to other Nodes.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and configure NodeRestriction plug-in on kubelets: https://v1-25.docs.kubernetes.io/docs/reference/access-authn-authz/node/. Then, edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the master node and set the --enable-admission-plugins parameter to a value that includes NodeRestriction.\n--enable-admission-plugins=...,NodeRestriction,...","multiregional":true,"service":"Pod"},"ecc-k8s-057":{"article":"To control pod security Kubernetes provided Pod Security Standards, they specify a set of security settings that Pods must meet before they can be created or updated in a cluster.\nThe Pod Security Standards define three different policies to broadly cover the security spectrum. These standards let you define how you want to restrict the behavior of pods in a clear, consistent fashion. Policies are cumulative and range from highly-permissive to highly-restrictive: privileged, baseline, and restricted.\nKubernetes defines a set of labels that you can set to define which of the predefined Pod Security Standard levels you want to use for a namespace. The label you select defines what action the control plane takes if a potential violation is detected: enforce, audit, warn.\nNote: If you already have a cluster-wide policies configured in AdmissionConfiguration, you can ignore this finding.","impact":"Securely adopting Kubernetes includes preventing unwanted changes to clusters. Unwanted changes can disrupt cluster operations, workload behaviors, and even compromise the whole environment integrity. Introducing pods that lack correct security configurations is an example of an unwanted cluster change.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Policies are applied to a namespace via labels. These labels are as follows:\nREQUIRED: pod-security.kubernetes.io/<MODE>: <LEVEL>\nOPTIONAL: pod-security.kubernetes.io/<MODE>-version: <VERSION> (defaults to latest)\n\nIt is helpful to apply the --dry-run flag when initially evaluating security profile changes for namespaces. The Pod Security Standard checks will still be run in dry run mode, giving you information about how the new policy would treat existing pods, without actually updating a policy.\nkubectl label --dry-run=server --overwrite ns --all pod-security.kubernetes.io/enforce=baseline\n\nTo enforce baseline pod security level use the following command:\nkubectl label --overwrite ns <namespace_name> pod-security.kubernetes.io/enforce=baseline \n\nReference: https://kubernetes.io/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/","multiregional":true,"service":"Namespace"},"ecc-k8s-086":{"article":"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc.) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.\nBy default, no security contexts are automatically applied to pods.","impact":"Without setting security context for pods, containers, the default security parameters are used, which may not provide adequate protection for sensitive data and services. This can result in potential security vulnerabilities and increased risk of data breaches or unauthorized access to resources.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and apply security contexts to your pods and containers: https://v1-25.docs.kubernetes.io/docs/tasks/configure-pod-container/security-context/\nFor a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers.","multiregional":true,"service":"Pod"},"ecc-k8s-020":{"article":"The --audit-log-maxsize flag is a configuration option for the Kubernetes API server that specifies the maximum size in megabytes of each audit log file before it gets rotated. When the audit log file reaches the specified maximum size, it is renamed with a timestamp and a new file is created.","impact":"If the --audit-log-maxsize flag is not specified or is set to an excessively high value, audit log files can grow too large and consume all available disk space. This can cause the Kubernetes API server to crash or become unresponsive, leading to service disruptions and potential security vulnerabilities. In addition, it can be difficult to analyze and extract useful information from very large audit log files, which can hinder the ability to identify and troubleshoot security incidents.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --audit-log-maxsize parameter to 100 or as an appropriate value.","multiregional":true,"service":"Pod"},"ecc-k8s-078":{"article":"Kubernetes allows administrators to set CPU quotas in namespaces, as hard limits for resource usage. Containers cannot use more CPU than the configured limit. Provided the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests.\nCPU quotas are used to ensure adequate utilization of shared resources. A system without managed quotas could eventually collapse due to inadequate resources for the tasks it bares.","impact":"Not setting CPU limits in Kubernetes can result in performance issues and resource waste, as containers may consume more resources than necessary, potentially affecting other applications running on the same node. This can lead to a higher risk of system failure and reduced application performance.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Add resource cpu limits to the pod or container specifications to prevent them from using more resources than necessary:\nFor containers:\n  spec:\n    containers:\n      resources:\n        limits:\n          cpu: <cpu limits>\nFor initContainers:\n  spec:\n    initContainers:\n      resources:\n        limits:\n          cpu: <cpu limits>","multiregional":true,"service":"Pod"},"ecc-k8s-034":{"article":"To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with --service-account-private-key-file as appropriate.","impact":"Without this parameter service account signing/verification key may not get rotated according to the organization policies which may lead to cluster compromise.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the Controller Manager pod specification file /etc/kubernetes/manifests/kube-controller-manager.yaml on the Control Plane node and set the --service-account-private-key-file parameter to the private key file for service accounts.\n--service-account-private-key-file=<filename>","multiregional":true,"service":"Pod"},"ecc-k8s-056":{"article":"Do not generally permit containers to be run with the securityContext.privileged flag set to true. There should be at least one admission control policy defined which does not permit privileged containers.If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.","impact":"Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Make sure that the securityContext.privileged field is omitted or set to false for each pod.","multiregional":true,"service":"Pod"},"ecc-k8s-045":{"article":"etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use automatically generated certificates for TLS connections between peers.\nInstead, you should enable peer client cert authentication that ensures all communication between members in the cluster encrypted and authenticated using the client certificates.\nNote: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","impact":"Automatically generated certificates do not provide authentication between members of etcd cluster. Without peer authentication, attackers can potentially gain access to sensitive data, including Kubernetes configuration information, API objects, and secrets. This could lead to significant security breaches, impacting the confidentiality, integrity, and availability of the Kubernetes deployment.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Note: This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.\nEdit the etcd pod specification file /etc/kubernetes/manifests/etcd.yaml on the and either remove the --peer-auto-tls parameter or set it to false:\n--peer-auto-tls=false\nTo enable peer client cert authentication, refer this documentation https://etcd.io/docs/v3.5/op-guide/security/.","multiregional":true,"service":"Pod"},"ecc-k8s-076":{"article":"When specifying the resource request for containers in a pod, the scheduler uses this information to decide which node to place the pod on. When setting resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.\nIf a container is created in a namespace that has a default CPU limit, and the container does not specify its own CPU limit, then the container is assigned the default CPU limit.","impact":"Not setting CPU requests for Kubernetes pods and containers can lead to unanticipated application behavior since the scheduler and kubelet rely on this information to allocate resources and enforce resource utilization limits. Furthermore, if a default CPU request is assigned, it may not accurately represent the actual resource demands of the application, resulting in either unnecessary resource consumption or degraded performance.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Add resource cpu request to the pod or container specifications to prevent them from using more resources than necessary:\nFor containers:\n  spec:\n    containers:\n      resources:\n        requests:\n          cpu: <cpu request>\nFor initContainers:\n  spec:\n    initContainers:\n      resources:\n        requests:\n          cpu: <cpu request>","multiregional":true,"service":"Pod"},"ecc-k8s-004":{"article":"The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate based-kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests.","impact":"A kubelet's HTTPS endpoint exposes APIs which give access to data of varying sensitivity, and allow you to perform operations with varying levels of power on the node and within containers. Attackers may attempt to use anonymous accounts to gain initial access to the cluster or to avoid attribution of their activities within the cluster.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets: https://v1-25.docs.kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/\nThen, edit API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the kubelet client certificate and key parameters as below:\n--kubelet-client-certificate=<path/to/client-certificate-file>\n--kubelet-client-key=<path/to/client-key-file>","multiregional":true,"service":"Pod"},"ecc-k8s-082":{"article":"Cluster roles with the impersonate, bind or escalate permissions should not be granted unless strictly required.","impact":"Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators. The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level. Each of these permissions has the potential to allow for privilege escalation to cluster-admin level.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Where possible, remove the impersonate, bind and escalate rights from subjects.","multiregional":true,"service":"ClusterRole"},"ecc-k8s-018":{"article":"Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events.","impact":"Failure to retain logs for at least 30 days can make it difficult or impossible to investigate or correlate past events. As a result, identifying the cause of problems or security incidents becomes challenging, and this could potentially lead to prolonged downtime, data breaches, or other critical issues. Furthermore, not retaining logs could result in regulatory noncompliance and potential legal consequences.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --audit-log-maxage parameter to 30 or as an appropriate number of days.","multiregional":true,"service":"Pod"},"ecc-k8s-011":{"article":"Setting admission control policy to AlwaysPullImages forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image`s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required.\nBy default, AlwaysPullImages is not set.","impact":"Without this access control policy, an image pulled to on a node can be used without any authorization check for image ownership.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the --enable-admission-plugins parameter to include AlwaysPullImages:\n--enable-admission-plugins=...,AlwaysPullImages,...","multiregional":true,"service":"Pod"},"ecc-k8s-050":{"article":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult. Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.","impact":"Placing objects in default namespace makes application of RBAC and other security controls more difficult.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.\nTo remediate this issue, you should recreate existing pod resource and attach it to non-default namespace.\n1. If you do not have namespace, create one using command: 'kubectl create namespace <namespace_name>'.\n2. Before creating a new resource in different namespace, you should delete it from default namespace. Because you cannot move Kubernetes resources between namespaces. They must be recreated.\nUse the following command to delete a desired pod from default namespace: 'kubectl delete -f <pod_config>.yaml'.\nWhere '<pod_config>.yaml' is a file that describes the pod you want to recreate.\n3. Then in a file <pod_config>.yaml, change parameter 'metadata.namespace' to non-default namespace. For example, to the namespace name that was created at step 1.\n4. Apply changes with command: 'kubectl apply -f <pod_config>.yaml'.","multiregional":true,"service":"Pod"},"ecc-k8s-009":{"article":"Using 'EventRateLimit' admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept.\nYou need to carefully tune in limits as per your environment.","impact":"A misbehaving workload could overwhelm and DoS the API Server, making it unavailable.","report_fields":["metadata.name","metadata.namespace"],"remediation":"Follow the Kubernetes documentation and set the desired limits in a configuration file: https://v1-25.docs.kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#eventratelimit\nThen, edit the API server pod specification file /etc/kubernetes/manifests/kubeapiserver.yaml and set the below parameters:\n--enable-admission-plugins=...,EventRateLimit,...\n--admission-control-config-file=<path/to/configuration/file>","multiregional":true,"service":"Pod"}}